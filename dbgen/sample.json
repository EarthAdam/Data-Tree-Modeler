{"files":{},"directories":{"TargetInfo":{"files":{"NVPTXTargetInfo.cpp":{"path":"TargetInfo/NVPTXTargetInfo.cpp","size":808,"lines":23,"functions":{"LLVMInitializeNVPTXTargetInfo":{"range":[18,24],"code":"extern \"C\" void LLVMInitializeNVPTXTargetInfo() {\n  RegisterTarget<Triple::nvptx> X(TheNVPTXTarget32, \"nvptx\",\n                                  \"NVIDIA PTX 32-bit\");\n  RegisterTarget<Triple::nvptx64> Y(TheNVPTXTarget64, \"nvptx64\",\n                                    \"NVIDIA PTX 64-bit\");\n}\n"}}}},"directories":{}},"MCTargetDesc":{"files":{"NVPTXMCAsmInfo.cpp":{"path":"MCTargetDesc/NVPTXMCAsmInfo.cpp","size":1909,"lines":59,"functions":{"NVPTXMCAsmInfo":{"range":[28,60],"code":"NVPTXMCAsmInfo::NVPTXMCAsmInfo(const Triple &TheTriple) {\n  if (TheTriple.getArch() == Triple::nvptx64) {\n    PointerSize = CalleeSaveStackSlotSize = 8;\n  }\n\n  CommentString = \"//\";\n\n  HasSingleParameterDotFile = false;\n\n  InlineAsmStart = \" begin inline asm\";\n  InlineAsmEnd = \" end inline asm\";\n\n  SupportsDebugInformation = CompileForDebugging;\n  // PTX does not allow .align on functions.\n  HasFunctionAlignment = false;\n  HasDotTypeDotSizeDirective = false;\n  // PTX does not allow .hidden or .protected\n  HiddenDeclarationVisibilityAttr = HiddenVisibilityAttr = MCSA_Invalid;\n  ProtectedVisibilityAttr = MCSA_Invalid;\n\n  Data8bitsDirective = \" .b8 \";\n  Data16bitsDirective = \" .b16 \";\n  Data32bitsDirective = \" .b32 \";\n  Data64bitsDirective = \" .b64 \";\n  ZeroDirective = \" .b8\";\n  AsciiDirective = \" .b8\";\n  AscizDirective = \" .b8\";\n\n  // @TODO: Can we just disable this?\n  WeakDirective = \"\\t// .weak\\t\";\n  GlobalDirective = \"\\t// .globl\\t\";\n}\n"},"anchor":{"range":[26,27],"code":"void NVPTXMCAsmInfo::anchor() {}\n\n"}}},"NVPTXMCTargetDesc.cpp":{"path":"MCTargetDesc/NVPTXMCTargetDesc.cpp","size":2573,"lines":79,"functions":{"createNVPTXMCSubtargetInfo":{"range":[47,50],"code":"createNVPTXMCSubtargetInfo(const Triple &TT, StringRef CPU, StringRef FS) {\n  return createNVPTXMCSubtargetInfoImpl(TT, CPU, FS);\n}\n\n"},"createNVPTXMCRegisterInfo":{"range":[39,46],"code":"static MCRegisterInfo *createNVPTXMCRegisterInfo(const Triple &TT) {\n  MCRegisterInfo *X = new MCRegisterInfo();\n  // PTX does not have a return address register.\n  InitNVPTXMCRegisterInfo(X, 0);\n  return X;\n}\n\nstatic MCSubtargetInfo *\n"},"createNVPTXMCInstPrinter":{"range":[51,61],"code":"static MCInstPrinter *createNVPTXMCInstPrinter(const Triple &T,\n                                               unsigned SyntaxVariant,\n                                               const MCAsmInfo &MAI,\n                                               const MCInstrInfo &MII,\n                                               const MCRegisterInfo &MRI) {\n  if (SyntaxVariant == 0)\n    return new NVPTXInstPrinter(MAI, MII, MRI);\n  return nullptr;\n}\n\n// Force static initialization.\n"},"LLVMInitializeNVPTXTargetMC":{"range":[62,80],"code":"extern \"C\" void LLVMInitializeNVPTXTargetMC() {\n  for (Target *T : {&TheNVPTXTarget32, &TheNVPTXTarget64}) {\n    // Register the MC asm info.\n    RegisterMCAsmInfo<NVPTXMCAsmInfo> X(*T);\n\n    // Register the MC instruction info.\n    TargetRegistry::RegisterMCInstrInfo(*T, createNVPTXMCInstrInfo);\n\n    // Register the MC register info.\n    TargetRegistry::RegisterMCRegInfo(*T, createNVPTXMCRegisterInfo);\n\n    // Register the MC subtarget info.\n    TargetRegistry::RegisterMCSubtargetInfo(*T, createNVPTXMCSubtargetInfo);\n\n    // Register the MCInstPrinter.\n    TargetRegistry::RegisterMCInstPrinter(*T, createNVPTXMCInstPrinter);\n  }\n}\n"},"createNVPTXMCInstrInfo":{"range":[33,38],"code":"static MCInstrInfo *createNVPTXMCInstrInfo() {\n  MCInstrInfo *X = new MCInstrInfo();\n  InitNVPTXMCInstrInfo(X);\n  return X;\n}\n\n"}}}},"directories":{}},"InstPrinter":{"files":{"NVPTXInstPrinter.cpp":{"path":"InstPrinter/NVPTXInstPrinter.cpp","size":7754,"lines":286,"functions":{"printInst":{"range":[70,77],"code":"void NVPTXInstPrinter::printInst(const MCInst *MI, raw_ostream &OS,\n                                 StringRef Annot, const MCSubtargetInfo &STI) {\n  printInstruction(MI, OS);\n\n  // Next always print the annotation.\n  printAnnotation(OS, Annot);\n}\n\n"},"printOperand":{"range":[78,91],"code":"void NVPTXInstPrinter::printOperand(const MCInst *MI, unsigned OpNo,\n                                    raw_ostream &O) {\n  const MCOperand &Op = MI->getOperand(OpNo);\n  if (Op.isReg()) {\n    unsigned Reg = Op.getReg();\n    printRegName(O, Reg);\n  } else if (Op.isImm()) {\n    O << markup(\"<imm:\") << formatImm(Op.getImm()) << markup(\">\");\n  } else {\n    assert(Op.isExpr() && \"Unknown operand kind in printOperand\");\n    Op.getExpr()->print(O, &MAI);\n  }\n}\n\n"},"printMemOperand":{"range":[263,278],"code":"void NVPTXInstPrinter::printMemOperand(const MCInst *MI, int OpNum,\n                                       raw_ostream &O, const char *Modifier) {\n  printOperand(MI, OpNum, O);\n\n  if (Modifier && !strcmp(Modifier, \"add\")) {\n    O << \", \";\n    printOperand(MI, OpNum + 1, O);\n  } else {\n    if (MI->getOperand(OpNum + 1).isImm() &&\n        MI->getOperand(OpNum + 1).getImm() == 0)\n      return; // don't print ',0' or '+0'\n    O << \"+\";\n    printOperand(MI, OpNum + 1, O);\n  }\n}\n\n"},"printRegName":{"range":[35,69],"code":"void NVPTXInstPrinter::printRegName(raw_ostream &OS, unsigned RegNo) const {\n  // Decode the virtual register\n  // Must be kept in sync with NVPTXAsmPrinter::encodeVirtualRegister\n  unsigned RCId = (RegNo >> 28);\n  switch (RCId) {\n  default: report_fatal_error(\"Bad virtual register encoding\");\n  case 0:\n    // This is actually a physical register, so defer to the autogenerated\n    // register printer\n    OS << getRegisterName(RegNo);\n    return;\n  case 1:\n    OS << \"%p\";\n    break;\n  case 2:\n    OS << \"%rs\";\n    break;\n  case 3:\n    OS << \"%r\";\n    break;\n  case 4:\n    OS << \"%rd\";\n    break;\n  case 5:\n    OS << \"%f\";\n    break;\n  case 6:\n    OS << \"%fd\";\n    break;\n  }\n\n  unsigned VReg = RegNo & 0x0FFFFFFF;\n  OS << VReg;\n}\n\n"},"printCvtMode":{"range":[92,141],"code":"void NVPTXInstPrinter::printCvtMode(const MCInst *MI, int OpNum, raw_ostream &O,\n                                    const char *Modifier) {\n  const MCOperand &MO = MI->getOperand(OpNum);\n  int64_t Imm = MO.getImm();\n\n  if (strcmp(Modifier, \"ftz\") == 0) {\n    // FTZ flag\n    if (Imm & NVPTX::PTXCvtMode::FTZ_FLAG)\n      O << \".ftz\";\n  } else if (strcmp(Modifier, \"sat\") == 0) {\n    // SAT flag\n    if (Imm & NVPTX::PTXCvtMode::SAT_FLAG)\n      O << \".sat\";\n  } else if (strcmp(Modifier, \"base\") == 0) {\n    // Default operand\n    switch (Imm & NVPTX::PTXCvtMode::BASE_MASK) {\n    default:\n      return;\n    case NVPTX::PTXCvtMode::NONE:\n      break;\n    case NVPTX::PTXCvtMode::RNI:\n      O << \".rni\";\n      break;\n    case NVPTX::PTXCvtMode::RZI:\n      O << \".rzi\";\n      break;\n    case NVPTX::PTXCvtMode::RMI:\n      O << \".rmi\";\n      break;\n    case NVPTX::PTXCvtMode::RPI:\n      O << \".rpi\";\n      break;\n    case NVPTX::PTXCvtMode::RN:\n      O << \".rn\";\n      break;\n    case NVPTX::PTXCvtMode::RZ:\n      O << \".rz\";\n      break;\n    case NVPTX::PTXCvtMode::RM:\n      O << \".rm\";\n      break;\n    case NVPTX::PTXCvtMode::RP:\n      O << \".rp\";\n      break;\n    }\n  } else {\n    llvm_unreachable(\"Invalid conversion modifier\");\n  }\n}\n\n"},"printLdStCode":{"range":[215,262],"code":"void NVPTXInstPrinter::printLdStCode(const MCInst *MI, int OpNum,\n                                     raw_ostream &O, const char *Modifier) {\n  if (Modifier) {\n    const MCOperand &MO = MI->getOperand(OpNum);\n    int Imm = (int) MO.getImm();\n    if (!strcmp(Modifier, \"volatile\")) {\n      if (Imm)\n        O << \".volatile\";\n    } else if (!strcmp(Modifier, \"addsp\")) {\n      switch (Imm) {\n      case NVPTX::PTXLdStInstCode::GLOBAL:\n        O << \".global\";\n        break;\n      case NVPTX::PTXLdStInstCode::SHARED:\n        O << \".shared\";\n        break;\n      case NVPTX::PTXLdStInstCode::LOCAL:\n        O << \".local\";\n        break;\n      case NVPTX::PTXLdStInstCode::PARAM:\n        O << \".param\";\n        break;\n      case NVPTX::PTXLdStInstCode::CONSTANT:\n        O << \".const\";\n        break;\n      case NVPTX::PTXLdStInstCode::GENERIC:\n        break;\n      default:\n        llvm_unreachable(\"Wrong Address Space\");\n      }\n    } else if (!strcmp(Modifier, \"sign\")) {\n      if (Imm == NVPTX::PTXLdStInstCode::Signed)\n        O << \"s\";\n      else if (Imm == NVPTX::PTXLdStInstCode::Unsigned)\n        O << \"u\";\n      else\n        O << \"f\";\n    } else if (!strcmp(Modifier, \"vec\")) {\n      if (Imm == NVPTX::PTXLdStInstCode::V2)\n        O << \".v2\";\n      else if (Imm == NVPTX::PTXLdStInstCode::V4)\n        O << \".v4\";\n    } else\n      llvm_unreachable(\"Unknown Modifier\");\n  } else\n    llvm_unreachable(\"Empty Modifier\");\n}\n\n"},"NVPTXInstPrinter":{"range":[31,34],"code":"NVPTXInstPrinter::NVPTXInstPrinter(const MCAsmInfo &MAI, const MCInstrInfo &MII,\n                                   const MCRegisterInfo &MRI)\n    : MCInstPrinter(MAI, MII, MRI) {}\n\n"},"printCmpMode":{"range":[142,214],"code":"void NVPTXInstPrinter::printCmpMode(const MCInst *MI, int OpNum, raw_ostream &O,\n                                    const char *Modifier) {\n  const MCOperand &MO = MI->getOperand(OpNum);\n  int64_t Imm = MO.getImm();\n\n  if (strcmp(Modifier, \"ftz\") == 0) {\n    // FTZ flag\n    if (Imm & NVPTX::PTXCmpMode::FTZ_FLAG)\n      O << \".ftz\";\n  } else if (strcmp(Modifier, \"base\") == 0) {\n    switch (Imm & NVPTX::PTXCmpMode::BASE_MASK) {\n    default:\n      return;\n    case NVPTX::PTXCmpMode::EQ:\n      O << \".eq\";\n      break;\n    case NVPTX::PTXCmpMode::NE:\n      O << \".ne\";\n      break;\n    case NVPTX::PTXCmpMode::LT:\n      O << \".lt\";\n      break;\n    case NVPTX::PTXCmpMode::LE:\n      O << \".le\";\n      break;\n    case NVPTX::PTXCmpMode::GT:\n      O << \".gt\";\n      break;\n    case NVPTX::PTXCmpMode::GE:\n      O << \".ge\";\n      break;\n    case NVPTX::PTXCmpMode::LO:\n      O << \".lo\";\n      break;\n    case NVPTX::PTXCmpMode::LS:\n      O << \".ls\";\n      break;\n    case NVPTX::PTXCmpMode::HI:\n      O << \".hi\";\n      break;\n    case NVPTX::PTXCmpMode::HS:\n      O << \".hs\";\n      break;\n    case NVPTX::PTXCmpMode::EQU:\n      O << \".equ\";\n      break;\n    case NVPTX::PTXCmpMode::NEU:\n      O << \".neu\";\n      break;\n    case NVPTX::PTXCmpMode::LTU:\n      O << \".ltu\";\n      break;\n    case NVPTX::PTXCmpMode::LEU:\n      O << \".leu\";\n      break;\n    case NVPTX::PTXCmpMode::GTU:\n      O << \".gtu\";\n      break;\n    case NVPTX::PTXCmpMode::GEU:\n      O << \".geu\";\n      break;\n    case NVPTX::PTXCmpMode::NUM:\n      O << \".num\";\n      break;\n    case NVPTX::PTXCmpMode::NotANumber:\n      O << \".nan\";\n      break;\n    }\n  } else {\n    llvm_unreachable(\"Empty Modifier\");\n  }\n}\n\n"},"printProtoIdent":{"range":[279,287],"code":"void NVPTXInstPrinter::printProtoIdent(const MCInst *MI, int OpNum,\n                                       raw_ostream &O, const char *Modifier) {\n  const MCOperand &Op = MI->getOperand(OpNum);\n  assert(Op.isExpr() && \"Call prototype is not an MCExpr?\");\n  const MCExpr *Expr = Op.getExpr();\n  const MCSymbol &Sym = cast<MCSymbolRefExpr>(Expr)->getSymbol();\n  O << Sym.getName();\n}\n"}}}},"directories":{}},"":{"files":{"NVPTXAssignValidGlobalNames.cpp":{"path":"NVPTXAssignValidGlobalNames.cpp","size":2611,"lines":84,"functions":{"runOnModule":{"range":[52,66],"code":"bool NVPTXAssignValidGlobalNames::runOnModule(Module &M) {\n  for (GlobalVariable &GV : M.globals()) {\n    // We are only allowed to rename local symbols.\n    if (GV.hasLocalLinkage()) {\n      // setName doesn't do extra work if the name does not change.\n      // Note: this does not create collisions - if setName is asked to set the\n      // name to something that already exists, it adds a proper postfix to\n      // avoid collisions.\n      GV.setName(cleanUpName(GV.getName()));\n    }\n  }\n\n  return true;\n}\n\n"},"cleanUpName":{"range":[67,81],"code":"std::string NVPTXAssignValidGlobalNames::cleanUpName(StringRef Name) {\n  std::string ValidName;\n  raw_string_ostream ValidNameStream(ValidName);\n  for (unsigned I = 0, E = Name.size(); I != E; ++I) {\n    char C = Name[I];\n    if (C == '.' || C == '@') {\n      ValidNameStream << \"_$_\";\n    } else {\n      ValidNameStream << C;\n    }\n  }\n\n  return ValidNameStream.str();\n}\n\n"},"createNVPTXAssignValidGlobalNamesPass":{"range":[82,85],"code":"ModulePass *llvm::createNVPTXAssignValidGlobalNamesPass() {\n  return new NVPTXAssignValidGlobalNames();\n}\n"},"NVPTXAssignValidGlobalNames":{"range":[34,51],"code":"  NVPTXAssignValidGlobalNames() : ModulePass(ID) {}\n\n  bool runOnModule(Module &M) override;\n\n  /// \\brief Clean up the name to remove symbols invalid in PTX.\n  std::string cleanUpName(StringRef Name);\n};\n}\n\nchar NVPTXAssignValidGlobalNames::ID = 0;\n\nnamespace llvm {\nvoid initializeNVPTXAssignValidGlobalNamesPass(PassRegistry &);\n}\n\nINITIALIZE_PASS(NVPTXAssignValidGlobalNames, \"nvptx-assign-valid-global-names\",\n                \"Assign valid PTX names to globals\", false, false)\n\n"}}},"NVPTXPrologEpilogPass.cpp":{"path":"NVPTXPrologEpilogPass.cpp","size":8262,"lines":227,"functions":{"NVPTXPrologEpilogPass":{"range":[35,43],"code":"  NVPTXPrologEpilogPass() : MachineFunctionPass(ID) {}\n\n  bool runOnMachineFunction(MachineFunction &MF) override;\n\nprivate:\n  void calculateFrameObjectOffsets(MachineFunction &Fn);\n};\n}\n\n"},"createNVPTXPrologEpilogPass":{"range":[44,49],"code":"MachineFunctionPass *llvm::createNVPTXPrologEpilogPass() {\n  return new NVPTXPrologEpilogPass();\n}\n\nchar NVPTXPrologEpilogPass::ID = 0;\n\n"},"runOnMachineFunction":{"range":[50,82],"code":"bool NVPTXPrologEpilogPass::runOnMachineFunction(MachineFunction &MF) {\n  const TargetSubtargetInfo &STI = MF.getSubtarget();\n  const TargetFrameLowering &TFI = *STI.getFrameLowering();\n  const TargetRegisterInfo &TRI = *STI.getRegisterInfo();\n  bool Modified = false;\n\n  calculateFrameObjectOffsets(MF);\n\n  for (MachineBasicBlock &MBB : MF) {\n    for (MachineInstr &MI : MBB) {\n      for (unsigned i = 0, e = MI.getNumOperands(); i != e; ++i) {\n        if (!MI.getOperand(i).isFI())\n          continue;\n        TRI.eliminateFrameIndex(MI, 0, i, nullptr);\n        Modified = true;\n      }\n    }\n  }\n\n  // Add function prolog/epilog\n  TFI.emitPrologue(MF, MF.front());\n\n  for (MachineFunction::iterator I = MF.begin(), E = MF.end(); I != E; ++I) {\n    // If last instruction is a return instruction, add an epilogue\n    if (I->isReturnBlock())\n      TFI.emitEpilogue(MF, *I);\n  }\n\n  return Modified;\n}\n\n/// AdjustStackOffset - Helper function used to adjust the stack frame offset.\nstatic inline void\n"},"AdjustStackOffset":{"range":[83,109],"code":"AdjustStackOffset(MachineFrameInfo *MFI, int FrameIdx,\n                  bool StackGrowsDown, int64_t &Offset,\n                  unsigned &MaxAlign) {\n  // If the stack grows down, add the object size to find the lowest address.\n  if (StackGrowsDown)\n    Offset += MFI->getObjectSize(FrameIdx);\n\n  unsigned Align = MFI->getObjectAlignment(FrameIdx);\n\n  // If the alignment of this object is greater than that of the stack, then\n  // increase the stack alignment to match.\n  MaxAlign = std::max(MaxAlign, Align);\n\n  // Adjust to alignment boundary.\n  Offset = (Offset + Align - 1) / Align * Align;\n\n  if (StackGrowsDown) {\n    DEBUG(dbgs() << \"alloc FI(\" << FrameIdx << \") at SP[\" << -Offset << \"]\\n\");\n    MFI->setObjectOffset(FrameIdx, -Offset); // Set the computed offset\n  } else {\n    DEBUG(dbgs() << \"alloc FI(\" << FrameIdx << \") at SP[\" << Offset << \"]\\n\");\n    MFI->setObjectOffset(FrameIdx, Offset);\n    Offset += MFI->getObjectSize(FrameIdx);\n  }\n}\n\nvoid\n"},"calculateFrameObjectOffsets":{"range":[110,228],"code":"NVPTXPrologEpilogPass::calculateFrameObjectOffsets(MachineFunction &Fn) {\n  const TargetFrameLowering &TFI = *Fn.getSubtarget().getFrameLowering();\n  const TargetRegisterInfo *RegInfo = Fn.getSubtarget().getRegisterInfo();\n\n  bool StackGrowsDown =\n    TFI.getStackGrowthDirection() == TargetFrameLowering::StackGrowsDown;\n\n  // Loop over all of the stack objects, assigning sequential addresses...\n  MachineFrameInfo *MFI = Fn.getFrameInfo();\n\n  // Start at the beginning of the local area.\n  // The Offset is the distance from the stack top in the direction\n  // of stack growth -- so it's always nonnegative.\n  int LocalAreaOffset = TFI.getOffsetOfLocalArea();\n  if (StackGrowsDown)\n    LocalAreaOffset = -LocalAreaOffset;\n  assert(LocalAreaOffset >= 0\n         && \"Local area offset should be in direction of stack growth\");\n  int64_t Offset = LocalAreaOffset;\n\n  // If there are fixed sized objects that are preallocated in the local area,\n  // non-fixed objects can't be allocated right at the start of local area.\n  // We currently don't support filling in holes in between fixed sized\n  // objects, so we adjust 'Offset' to point to the end of last fixed sized\n  // preallocated object.\n  for (int i = MFI->getObjectIndexBegin(); i != 0; ++i) {\n    int64_t FixedOff;\n    if (StackGrowsDown) {\n      // The maximum distance from the stack pointer is at lower address of\n      // the object -- which is given by offset. For down growing stack\n      // the offset is negative, so we negate the offset to get the distance.\n      FixedOff = -MFI->getObjectOffset(i);\n    } else {\n      // The maximum distance from the start pointer is at the upper\n      // address of the object.\n      FixedOff = MFI->getObjectOffset(i) + MFI->getObjectSize(i);\n    }\n    if (FixedOff > Offset) Offset = FixedOff;\n  }\n\n  // NOTE: We do not have a call stack\n\n  unsigned MaxAlign = MFI->getMaxAlignment();\n\n  // No scavenger\n\n  // FIXME: Once this is working, then enable flag will change to a target\n  // check for whether the frame is large enough to want to use virtual\n  // frame index registers. Functions which don't want/need this optimization\n  // will continue to use the existing code path.\n  if (MFI->getUseLocalStackAllocationBlock()) {\n    unsigned Align = MFI->getLocalFrameMaxAlign();\n\n    // Adjust to alignment boundary.\n    Offset = (Offset + Align - 1) / Align * Align;\n\n    DEBUG(dbgs() << \"Local frame base offset: \" << Offset << \"\\n\");\n\n    // Resolve offsets for objects in the local block.\n    for (unsigned i = 0, e = MFI->getLocalFrameObjectCount(); i != e; ++i) {\n      std::pair<int, int64_t> Entry = MFI->getLocalFrameObjectMap(i);\n      int64_t FIOffset = (StackGrowsDown ? -Offset : Offset) + Entry.second;\n      DEBUG(dbgs() << \"alloc FI(\" << Entry.first << \") at SP[\" <<\n            FIOffset << \"]\\n\");\n      MFI->setObjectOffset(Entry.first, FIOffset);\n    }\n    // Allocate the local block\n    Offset += MFI->getLocalFrameSize();\n\n    MaxAlign = std::max(Align, MaxAlign);\n  }\n\n  // No stack protector\n\n  // Then assign frame offsets to stack objects that are not used to spill\n  // callee saved registers.\n  for (unsigned i = 0, e = MFI->getObjectIndexEnd(); i != e; ++i) {\n    if (MFI->isObjectPreAllocated(i) &&\n        MFI->getUseLocalStackAllocationBlock())\n      continue;\n    if (MFI->isDeadObjectIndex(i))\n      continue;\n\n    AdjustStackOffset(MFI, i, StackGrowsDown, Offset, MaxAlign);\n  }\n\n  // No scavenger\n\n  if (!TFI.targetHandlesStackFrameRounding()) {\n    // If we have reserved argument space for call sites in the function\n    // immediately on entry to the current function, count it as part of the\n    // overall stack size.\n    if (MFI->adjustsStack() && TFI.hasReservedCallFrame(Fn))\n      Offset += MFI->getMaxCallFrameSize();\n\n    // Round up the size to a multiple of the alignment.  If the function has\n    // any calls or alloca's, align to the target's StackAlignment value to\n    // ensure that the callee's frame or the alloca data is suitably aligned;\n    // otherwise, for leaf functions, align to the TransientStackAlignment\n    // value.\n    unsigned StackAlign;\n    if (MFI->adjustsStack() || MFI->hasVarSizedObjects() ||\n        (RegInfo->needsStackRealignment(Fn) && MFI->getObjectIndexEnd() != 0))\n      StackAlign = TFI.getStackAlignment();\n    else\n      StackAlign = TFI.getTransientStackAlignment();\n\n    // If the frame pointer is eliminated, all frame offsets will be relative to\n    // SP not FP. Align to MaxAlign so this works.\n    StackAlign = std::max(StackAlign, MaxAlign);\n    unsigned AlignMask = StackAlign - 1;\n    Offset = (Offset + AlignMask) & ~uint64_t(AlignMask);\n  }\n\n  // Update frame info to pretend that this is part of the stack...\n  int64_t StackSize = Offset - LocalAreaOffset;\n  MFI->setStackSize(StackSize);\n}\n"}}},"NVPTXRegisterInfo.cpp":{"path":"NVPTXRegisterInfo.cpp","size":3358,"lines":110,"functions":{"NVPTXRegisterInfo":{"range":[74,80],"code":"NVPTXRegisterInfo::NVPTXRegisterInfo() : NVPTXGenRegisterInfo(0) {}\n\n#define GET_REGINFO_TARGET_DESC\n#include \"NVPTXGenRegisterInfo.inc\"\n\n/// NVPTX Callee Saved Registers\nconst MCPhysReg *\n"},"getFrameRegister":{"range":[108,111],"code":"unsigned NVPTXRegisterInfo::getFrameRegister(const MachineFunction &MF) const {\n  return NVPTX::VRFrame;\n}\n"},"eliminateFrameIndex":{"range":[91,107],"code":"void NVPTXRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,\n                                            int SPAdj, unsigned FIOperandNum,\n                                            RegScavenger *RS) const {\n  assert(SPAdj == 0 && \"Unexpected\");\n\n  MachineInstr &MI = *II;\n  int FrameIndex = MI.getOperand(FIOperandNum).getIndex();\n\n  MachineFunction &MF = *MI.getParent()->getParent();\n  int Offset = MF.getFrameInfo()->getObjectOffset(FrameIndex) +\n               MI.getOperand(FIOperandNum + 1).getImm();\n\n  // Using I0 as the frame pointer\n  MI.getOperand(FIOperandNum).ChangeToRegister(NVPTX::VRFrame, false);\n  MI.getOperand(FIOperandNum + 1).ChangeToImmediate(Offset);\n}\n\n"},"getNVPTXRegClassName":{"range":[29,50],"code":"std::string getNVPTXRegClassName(TargetRegisterClass const *RC) {\n  if (RC == &NVPTX::Float32RegsRegClass) {\n    return \".f32\";\n  }\n  if (RC == &NVPTX::Float64RegsRegClass) {\n    return \".f64\";\n  } else if (RC == &NVPTX::Int64RegsRegClass) {\n    return \".s64\";\n  } else if (RC == &NVPTX::Int32RegsRegClass) {\n    return \".s32\";\n  } else if (RC == &NVPTX::Int16RegsRegClass) {\n    return \".s16\";\n  } else if (RC == &NVPTX::Int1RegsRegClass) {\n    return \".pred\";\n  } else if (RC == &NVPTX::SpecialRegsRegClass) {\n    return \"!Special!\";\n  } else {\n    return \"INTERNAL\";\n  }\n  return \"\";\n}\n\n"},"getReservedRegs":{"range":[86,90],"code":"BitVector NVPTXRegisterInfo::getReservedRegs(const MachineFunction &MF) const {\n  BitVector Reserved(getNumRegs());\n  return Reserved;\n}\n\n"},"getCalleeSavedRegs":{"range":[81,85],"code":"NVPTXRegisterInfo::getCalleeSavedRegs(const MachineFunction *) const {\n  static const MCPhysReg CalleeSavedRegs[] = { 0 };\n  return CalleeSavedRegs;\n}\n\n"},"getNVPTXRegClassStr":{"range":[51,73],"code":"std::string getNVPTXRegClassStr(TargetRegisterClass const *RC) {\n  if (RC == &NVPTX::Float32RegsRegClass) {\n    return \"%f\";\n  }\n  if (RC == &NVPTX::Float64RegsRegClass) {\n    return \"%fd\";\n  } else if (RC == &NVPTX::Int64RegsRegClass) {\n    return \"%rd\";\n  } else if (RC == &NVPTX::Int32RegsRegClass) {\n    return \"%r\";\n  } else if (RC == &NVPTX::Int16RegsRegClass) {\n    return \"%rs\";\n  } else if (RC == &NVPTX::Int1RegsRegClass) {\n    return \"%p\";\n  } else if (RC == &NVPTX::SpecialRegsRegClass) {\n    return \"!Special!\";\n  } else {\n    return \"INTERNAL\";\n  }\n  return \"\";\n}\n}\n\n"}}},"NVPTXInstrInfo.cpp":{"path":"NVPTXInstrInfo.cpp","size":9246,"lines":256,"functions":{"CanTailMerge":{"range":[113,148],"code":"bool NVPTXInstrInfo::CanTailMerge(const MachineInstr *MI) const {\n  unsigned addrspace = 0;\n  if (MI->getOpcode() == NVPTX::INT_BARRIER0)\n    return false;\n  if (isLoadInstr(*MI, addrspace))\n    if (addrspace == NVPTX::PTXLdStInstCode::SHARED)\n      return false;\n  if (isStoreInstr(*MI, addrspace))\n    if (addrspace == NVPTX::PTXLdStInstCode::SHARED)\n      return false;\n  return true;\n}\n\n/// AnalyzeBranch - Analyze the branching code at the end of MBB, returning\n/// true if it cannot be understood (e.g. it's a switch dispatch or isn't\n/// implemented for a target).  Upon success, this returns false and returns\n/// with the following information in various cases:\n///\n/// 1. If this block ends with no branches (it just falls through to its succ)\n///    just return false, leaving TBB/FBB null.\n/// 2. If this block ends with only an unconditional branch, it sets TBB to be\n///    the destination block.\n/// 3. If this block ends with an conditional branch and it falls through to\n///    an successor block, it sets TBB to be the branch destination block and a\n///    list of operands that evaluate the condition. These\n///    operands can be passed to other TargetInstrInfo methods to create new\n///    branches.\n/// 4. If this block ends with an conditional branch and an unconditional\n///    block, it returns the 'true' destination in TBB, the 'false' destination\n///    in FBB, and a list of operands that evaluate the condition. These\n///    operands can be passed to other TargetInstrInfo methods to create new\n///    branches.\n///\n/// Note that RemoveBranch and InsertBranch must be implemented to support\n/// cases where this method returns success.\n///\n"},"copyPhysReg":{"range":[33,67],"code":"void NVPTXInstrInfo::copyPhysReg(MachineBasicBlock &MBB,\n                                 MachineBasicBlock::iterator I,\n                                 const DebugLoc &DL, unsigned DestReg,\n                                 unsigned SrcReg, bool KillSrc) const {\n  const MachineRegisterInfo &MRI = MBB.getParent()->getRegInfo();\n  const TargetRegisterClass *DestRC = MRI.getRegClass(DestReg);\n  const TargetRegisterClass *SrcRC = MRI.getRegClass(SrcReg);\n\n  if (DestRC->getSize() != SrcRC->getSize())\n    report_fatal_error(\"Copy one register into another with a different width\");\n\n  unsigned Op;\n  if (DestRC == &NVPTX::Int1RegsRegClass) {\n    Op = NVPTX::IMOV1rr;\n  } else if (DestRC == &NVPTX::Int16RegsRegClass) {\n    Op = NVPTX::IMOV16rr;\n  } else if (DestRC == &NVPTX::Int32RegsRegClass) {\n    Op = (SrcRC == &NVPTX::Int32RegsRegClass ? NVPTX::IMOV32rr\n                                             : NVPTX::BITCONVERT_32_F2I);\n  } else if (DestRC == &NVPTX::Int64RegsRegClass) {\n    Op = (SrcRC == &NVPTX::Int64RegsRegClass ? NVPTX::IMOV64rr\n                                             : NVPTX::BITCONVERT_64_F2I);\n  } else if (DestRC == &NVPTX::Float32RegsRegClass) {\n    Op = (SrcRC == &NVPTX::Float32RegsRegClass ? NVPTX::FMOV32rr\n                                               : NVPTX::BITCONVERT_32_I2F);\n  } else if (DestRC == &NVPTX::Float64RegsRegClass) {\n    Op = (SrcRC == &NVPTX::Float64RegsRegClass ? NVPTX::FMOV64rr\n                                               : NVPTX::BITCONVERT_64_I2F);\n  } else {\n    llvm_unreachable(\"Bad register copy\");\n  }\n  BuildMI(MBB, I, DL, get(Op), DestReg)\n      .addReg(SrcReg, getKillRegState(KillSrc));\n}\n\n"},"isLoadInstr":{"range":[91,101],"code":"bool NVPTXInstrInfo::isLoadInstr(const MachineInstr &MI,\n                                 unsigned &AddrSpace) const {\n  bool isLoad = false;\n  unsigned TSFlags =\n      (MI.getDesc().TSFlags & NVPTX::isLoadMask) >> NVPTX::isLoadShift;\n  isLoad = (TSFlags == 1);\n  if (isLoad)\n    AddrSpace = getLdStCodeAddrSpace(MI);\n  return isLoad;\n}\n\n"},"analyzeBranch":{"range":[149,207],"code":"bool NVPTXInstrInfo::analyzeBranch(MachineBasicBlock &MBB,\n                                   MachineBasicBlock *&TBB,\n                                   MachineBasicBlock *&FBB,\n                                   SmallVectorImpl<MachineOperand> &Cond,\n                                   bool AllowModify) const {\n  // If the block has no terminators, it just falls into the block after it.\n  MachineBasicBlock::iterator I = MBB.end();\n  if (I == MBB.begin() || !isUnpredicatedTerminator(*--I))\n    return false;\n\n  // Get the last instruction in the block.\n  MachineInstr &LastInst = *I;\n\n  // If there is only one terminator instruction, process it.\n  if (I == MBB.begin() || !isUnpredicatedTerminator(*--I)) {\n    if (LastInst.getOpcode() == NVPTX::GOTO) {\n      TBB = LastInst.getOperand(0).getMBB();\n      return false;\n    } else if (LastInst.getOpcode() == NVPTX::CBranch) {\n      // Block ends with fall-through condbranch.\n      TBB = LastInst.getOperand(1).getMBB();\n      Cond.push_back(LastInst.getOperand(0));\n      return false;\n    }\n    // Otherwise, don't know what this is.\n    return true;\n  }\n\n  // Get the instruction before it if it's a terminator.\n  MachineInstr &SecondLastInst = *I;\n\n  // If there are three terminators, we don't know what sort of block this is.\n  if (I != MBB.begin() && isUnpredicatedTerminator(*--I))\n    return true;\n\n  // If the block ends with NVPTX::GOTO and NVPTX:CBranch, handle it.\n  if (SecondLastInst.getOpcode() == NVPTX::CBranch &&\n      LastInst.getOpcode() == NVPTX::GOTO) {\n    TBB = SecondLastInst.getOperand(1).getMBB();\n    Cond.push_back(SecondLastInst.getOperand(0));\n    FBB = LastInst.getOperand(0).getMBB();\n    return false;\n  }\n\n  // If the block ends with two NVPTX:GOTOs, handle it.  The second one is not\n  // executed, so remove it.\n  if (SecondLastInst.getOpcode() == NVPTX::GOTO &&\n      LastInst.getOpcode() == NVPTX::GOTO) {\n    TBB = SecondLastInst.getOperand(0).getMBB();\n    I = LastInst;\n    if (AllowModify)\n      I->eraseFromParent();\n    return false;\n  }\n\n  // Otherwise, can't handle this.\n  return true;\n}\n\n"},"NVPTXInstrInfo":{"range":[31,32],"code":"NVPTXInstrInfo::NVPTXInstrInfo() : NVPTXGenInstrInfo(), RegInfo() {}\n\n"},"isMoveInstr":{"range":[68,90],"code":"bool NVPTXInstrInfo::isMoveInstr(const MachineInstr &MI, unsigned &SrcReg,\n                                 unsigned &DestReg) const {\n  // Look for the appropriate part of TSFlags\n  bool isMove = false;\n\n  unsigned TSFlags =\n      (MI.getDesc().TSFlags & NVPTX::SimpleMoveMask) >> NVPTX::SimpleMoveShift;\n  isMove = (TSFlags == 1);\n\n  if (isMove) {\n    MachineOperand dest = MI.getOperand(0);\n    MachineOperand src = MI.getOperand(1);\n    assert(dest.isReg() && \"dest of a movrr is not a reg\");\n    assert(src.isReg() && \"src of a movrr is not a reg\");\n\n    SrcReg = src.getReg();\n    DestReg = dest.getReg();\n    return true;\n  }\n\n  return false;\n}\n\n"},"anchor":{"range":[29,30],"code":"void NVPTXInstrInfo::anchor() {}\n\n"},"isStoreInstr":{"range":[102,112],"code":"bool NVPTXInstrInfo::isStoreInstr(const MachineInstr &MI,\n                                  unsigned &AddrSpace) const {\n  bool isStore = false;\n  unsigned TSFlags =\n      (MI.getDesc().TSFlags & NVPTX::isStoreMask) >> NVPTX::isStoreShift;\n  isStore = (TSFlags == 1);\n  if (isStore)\n    AddrSpace = getLdStCodeAddrSpace(MI);\n  return isStore;\n}\n\n"},"InsertBranch":{"range":[232,257],"code":"unsigned NVPTXInstrInfo::InsertBranch(MachineBasicBlock &MBB,\n                                      MachineBasicBlock *TBB,\n                                      MachineBasicBlock *FBB,\n                                      ArrayRef<MachineOperand> Cond,\n                                      const DebugLoc &DL) const {\n  // Shouldn't be a fall through.\n  assert(TBB && \"InsertBranch must not be told to insert a fallthrough\");\n  assert((Cond.size() == 1 || Cond.size() == 0) &&\n         \"NVPTX branch conditions have two components!\");\n\n  // One-way branch.\n  if (!FBB) {\n    if (Cond.empty()) // Unconditional branch\n      BuildMI(&MBB, DL, get(NVPTX::GOTO)).addMBB(TBB);\n    else // Conditional branch\n      BuildMI(&MBB, DL, get(NVPTX::CBranch)).addReg(Cond[0].getReg())\n          .addMBB(TBB);\n    return 1;\n  }\n\n  // Two-way Conditional Branch.\n  BuildMI(&MBB, DL, get(NVPTX::CBranch)).addReg(Cond[0].getReg()).addMBB(TBB);\n  BuildMI(&MBB, DL, get(NVPTX::GOTO)).addMBB(FBB);\n  return 2;\n}\n"},"RemoveBranch":{"range":[208,231],"code":"unsigned NVPTXInstrInfo::RemoveBranch(MachineBasicBlock &MBB) const {\n  MachineBasicBlock::iterator I = MBB.end();\n  if (I == MBB.begin())\n    return 0;\n  --I;\n  if (I->getOpcode() != NVPTX::GOTO && I->getOpcode() != NVPTX::CBranch)\n    return 0;\n\n  // Remove the branch.\n  I->eraseFromParent();\n\n  I = MBB.end();\n\n  if (I == MBB.begin())\n    return 1;\n  --I;\n  if (I->getOpcode() != NVPTX::CBranch)\n    return 1;\n\n  // Remove the branch.\n  I->eraseFromParent();\n  return 2;\n}\n\n"}}},"NVVMIntrRange.cpp":{"path":"NVVMIntrRange.cpp","size":4774,"lines":148,"functions":{"runOnFunction":{"range":[77,149],"code":"bool NVVMIntrRange::runOnFunction(Function &F) {\n  // Go through the calls in this function.\n  bool Changed = false;\n  for (Instruction &I : instructions(F)) {\n    CallInst *Call = dyn_cast<CallInst>(&I);\n    if (!Call)\n      continue;\n\n    if (Function *Callee = Call->getCalledFunction()) {\n      switch (Callee->getIntrinsicID()) {\n      // Index within block\n      case Intrinsic::nvvm_read_ptx_sreg_tid_x:\n        Changed |= addRangeMetadata(0, MaxBlockSize.x, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_tid_y:\n        Changed |= addRangeMetadata(0, MaxBlockSize.y, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_tid_z:\n        Changed |= addRangeMetadata(0, MaxBlockSize.z, Call);\n        break;\n\n      // Block size\n      case Intrinsic::nvvm_read_ptx_sreg_ntid_x:\n        Changed |= addRangeMetadata(1, MaxBlockSize.x+1, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_ntid_y:\n        Changed |= addRangeMetadata(1, MaxBlockSize.y+1, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_ntid_z:\n        Changed |= addRangeMetadata(1, MaxBlockSize.z+1, Call);\n        break;\n\n      // Index within grid\n      case Intrinsic::nvvm_read_ptx_sreg_ctaid_x:\n        Changed |= addRangeMetadata(0, MaxGridSize.x, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_ctaid_y:\n        Changed |= addRangeMetadata(0, MaxGridSize.y, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_ctaid_z:\n        Changed |= addRangeMetadata(0, MaxGridSize.z, Call);\n        break;\n\n      // Grid size\n      case Intrinsic::nvvm_read_ptx_sreg_nctaid_x:\n        Changed |= addRangeMetadata(1, MaxGridSize.x+1, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_nctaid_y:\n        Changed |= addRangeMetadata(1, MaxGridSize.y+1, Call);\n        break;\n      case Intrinsic::nvvm_read_ptx_sreg_nctaid_z:\n        Changed |= addRangeMetadata(1, MaxGridSize.z+1, Call);\n        break;\n\n      // warp size is constant 32.\n      case Intrinsic::nvvm_read_ptx_sreg_warpsize:\n        Changed |= addRangeMetadata(32, 32+1, Call);\n        break;\n\n      // Lane ID is [0..warpsize)\n      case Intrinsic::nvvm_read_ptx_sreg_laneid:\n        Changed |= addRangeMetadata(0, 32, Call);\n        break;\n\n      default:\n        break;\n      }\n    }\n  }\n\n  return Changed;\n}\n"},"createNVVMIntrRangePass":{"range":[57,66],"code":"FunctionPass *llvm::createNVVMIntrRangePass(unsigned int SmVersion) {\n  return new NVVMIntrRange(SmVersion);\n}\n\nchar NVVMIntrRange::ID = 0;\nINITIALIZE_PASS(NVVMIntrRange, \"nvvm-intr-range\",\n                \"Add !range metadata to NVVM intrinsics.\", false, false)\n\n// Adds the passed-in [Low,High) range information as metadata to the\n// passed-in call instruction.\n"},"NVVMIntrRange":{"range":[41,56],"code":"   NVVMIntrRange(unsigned int SmVersion) : FunctionPass(ID) {\n     MaxBlockSize.x = 1024;\n     MaxBlockSize.y = 1024;\n     MaxBlockSize.z = 64;\n\n     MaxGridSize.x = SmVersion >= 30 ? 0x7fffffff : 0xffff;\n     MaxGridSize.y = 0xffff;\n     MaxGridSize.z = 0xffff;\n\n     initializeNVVMIntrRangePass(*PassRegistry::getPassRegistry());\n   }\n\n   bool runOnFunction(Function &) override;\n};\n}\n\n"},"addRangeMetadata":{"range":[67,76],"code":"static bool addRangeMetadata(uint64_t Low, uint64_t High, CallInst *C) {\n  LLVMContext &Context = C->getParent()->getContext();\n  IntegerType *Int32Ty = Type::getInt32Ty(Context);\n  Metadata *LowAndHigh[] = {\n      ConstantAsMetadata::get(ConstantInt::get(Int32Ty, Low)),\n      ConstantAsMetadata::get(ConstantInt::get(Int32Ty, High))};\n  C->setMetadata(LLVMContext::MD_range, MDNode::get(Context, LowAndHigh));\n  return true;\n}\n\n"}}},"NVPTXUtilities.cpp":{"path":"NVPTXUtilities.cpp","size":13481,"lines":442,"functions":{"isImageWriteOnly":{"range":[194,208],"code":"bool llvm::isImageWriteOnly(const llvm::Value &val) {\n  if (const Argument *arg = dyn_cast<Argument>(&val)) {\n    const Function *func = arg->getParent();\n    std::vector<unsigned> annot;\n    if (llvm::findAllNVVMAnnotation(func,\n                                    llvm::PropertyAnnotationNames[\n                                        llvm::PROPERTY_ISWRITEONLY_IMAGE_PARAM],\n                                    annot)) {\n      if (std::find(annot.begin(), annot.end(), arg->getArgNo()) != annot.end())\n        return true;\n    }\n  }\n  return false;\n}\n\n"},"findOneNVVMAnnotation":{"range":[102,115],"code":"bool llvm::findOneNVVMAnnotation(const GlobalValue *gv, const std::string &prop,\n                                 unsigned &retval) {\n  MutexGuard Guard(Lock);\n  const Module *m = gv->getParent();\n  if ((*annotationCache).find(m) == (*annotationCache).end())\n    cacheAnnotationFromMD(m, gv);\n  else if ((*annotationCache)[m].find(gv) == (*annotationCache)[m].end())\n    cacheAnnotationFromMD(m, gv);\n  if ((*annotationCache)[m][gv].find(prop) == (*annotationCache)[m][gv].end())\n    return false;\n  retval = (*annotationCache)[m][gv][prop][0];\n  return true;\n}\n\n"},"clearAnnotationCache":{"range":[38,42],"code":"void llvm::clearAnnotationCache(const llvm::Module *Mod) {\n  MutexGuard Guard(Lock);\n  annotationCache->erase(Mod);\n}\n\n"},"isSurface":{"range":[143,155],"code":"bool llvm::isSurface(const llvm::Value &val) {\n  if (const GlobalValue *gv = dyn_cast<GlobalValue>(&val)) {\n    unsigned annot;\n    if (llvm::findOneNVVMAnnotation(\n            gv, llvm::PropertyAnnotationNames[llvm::PROPERTY_ISSURFACE],\n            annot)) {\n      assert((annot == 1) && \"Unexpected annotation on a surface symbol\");\n      return true;\n    }\n  }\n  return false;\n}\n\n"},"getMaxNTIDx":{"range":[257,261],"code":"bool llvm::getMaxNTIDx(const Function &F, unsigned &x) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_MAXNTID_X], x));\n}\n\n"},"getMinCTASm":{"range":[287,291],"code":"bool llvm::getMinCTASm(const Function &F, unsigned &x) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_MINNCTAPERSM], x));\n}\n\n"},"getParentFunction":{"range":[350,363],"code":"Function *llvm::getParentFunction(Value *v) {\n  if (Function *F = dyn_cast<Function>(v))\n    return F;\n\n  if (Instruction *I = dyn_cast<Instruction>(v))\n    return I->getParent()->getParent();\n\n  if (BasicBlock *B = dyn_cast<BasicBlock>(v))\n    return B->getParent();\n\n  return nullptr;\n}\n\n// Dump a block by name\n"},"getParentBlock":{"range":[340,349],"code":"BasicBlock *llvm::getParentBlock(Value *v) {\n  if (BasicBlock *B = dyn_cast<BasicBlock>(v))\n    return B;\n\n  if (Instruction *I = dyn_cast<Instruction>(v))\n    return I->getParent();\n\n  return nullptr;\n}\n\n"},"getInst":{"range":[379,394],"code":"Instruction *llvm::getInst(Value *base, char *instName) {\n  Function *F = getParentFunction(base);\n  if (!F)\n    return nullptr;\n\n  for (inst_iterator it = inst_begin(F), ie = inst_end(F); it != ie; ++it) {\n    Instruction *I = &*it;\n    if (strcmp(I->getName().data(), instName) == 0) {\n      return I;\n    }\n  }\n\n  return nullptr;\n}\n\n// Dump an instruction by name\n"},"cacheAnnotationFromMD":{"range":[69,101],"code":"static void cacheAnnotationFromMD(const Module *m, const GlobalValue *gv) {\n  MutexGuard Guard(Lock);\n  NamedMDNode *NMD = m->getNamedMetadata(llvm::NamedMDForAnnotations);\n  if (!NMD)\n    return;\n  key_val_pair_t tmp;\n  for (unsigned i = 0, e = NMD->getNumOperands(); i != e; ++i) {\n    const MDNode *elem = NMD->getOperand(i);\n\n    GlobalValue *entity =\n        mdconst::dyn_extract_or_null<GlobalValue>(elem->getOperand(0));\n    // entity may be null due to DCE\n    if (!entity)\n      continue;\n    if (entity != gv)\n      continue;\n\n    // accumulate annotations for entity in tmp\n    cacheAnnotationFromMD(elem, tmp);\n  }\n\n  if (tmp.empty()) // no annotations for this gv\n    return;\n\n  if ((*annotationCache).find(m) != (*annotationCache).end())\n    (*annotationCache)[m][gv] = std::move(tmp);\n  else {\n    global_val_annot_t tmp1;\n    tmp1[gv] = std::move(tmp);\n    (*annotationCache)[m] = std::move(tmp1);\n  }\n}\n\n"},"getReqNTIDx":{"range":[272,276],"code":"bool llvm::getReqNTIDx(const Function &F, unsigned &x) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_REQNTID_X], x));\n}\n\n"},"getSamplerName":{"range":[252,256],"code":"std::string llvm::getSamplerName(const llvm::Value &val) {\n  assert(val.hasName() && \"Found sampler variable with no name\");\n  return val.getName();\n}\n\n"},"findAllNVVMAnnotation":{"range":[116,129],"code":"bool llvm::findAllNVVMAnnotation(const GlobalValue *gv, const std::string &prop,\n                                 std::vector<unsigned> &retval) {\n  MutexGuard Guard(Lock);\n  const Module *m = gv->getParent();\n  if ((*annotationCache).find(m) == (*annotationCache).end())\n    cacheAnnotationFromMD(m, gv);\n  else if ((*annotationCache)[m].find(gv) == (*annotationCache)[m].end())\n    cacheAnnotationFromMD(m, gv);\n  if ((*annotationCache)[m][gv].find(prop) == (*annotationCache)[m][gv].end())\n    return false;\n  retval = (*annotationCache)[m][gv][prop];\n  return true;\n}\n\n"},"isKernelFunction":{"range":[292,302],"code":"bool llvm::isKernelFunction(const Function &F) {\n  unsigned x = 0;\n  bool retval = llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_ISKERNEL_FUNCTION], x);\n  if (!retval) {\n    // There is no NVVM metadata, check the calling convention\n    return F.getCallingConv() == llvm::CallingConv::PTX_Kernel;\n  }\n  return (x == 1);\n}\n\n"},"getMaxNTIDz":{"range":[267,271],"code":"bool llvm::getMaxNTIDz(const Function &F, unsigned &z) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_MAXNTID_Z], z));\n}\n\n"},"getReqNTIDz":{"range":[282,286],"code":"bool llvm::getReqNTIDz(const Function &F, unsigned &z) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_REQNTID_Z], z));\n}\n\n"},"getAlign":{"range":[319,339],"code":"bool llvm::getAlign(const CallInst &I, unsigned index, unsigned &align) {\n  if (MDNode *alignNode = I.getMetadata(\"callalign\")) {\n    for (int i = 0, n = alignNode->getNumOperands(); i < n; i++) {\n      if (const ConstantInt *CI =\n              mdconst::dyn_extract<ConstantInt>(alignNode->getOperand(i))) {\n        unsigned v = CI->getZExtValue();\n        if ((v >> 16) == index) {\n          align = v & 0xFFFF;\n          return true;\n        }\n        if ((v >> 16) > index) {\n          return false;\n        }\n      }\n    }\n  }\n  return false;\n}\n\n// The following are some useful utilities for debugging\n\n"},"getTextureName":{"range":[242,246],"code":"std::string llvm::getTextureName(const llvm::Value &val) {\n  assert(val.hasName() && \"Found texture variable with no name\");\n  return val.getName();\n}\n\n"},"isImageReadWrite":{"range":[209,223],"code":"bool llvm::isImageReadWrite(const llvm::Value &val) {\n  if (const Argument *arg = dyn_cast<Argument>(&val)) {\n    const Function *func = arg->getParent();\n    std::vector<unsigned> annot;\n    if (llvm::findAllNVVMAnnotation(func,\n                                    llvm::PropertyAnnotationNames[\n                                        llvm::PROPERTY_ISREADWRITE_IMAGE_PARAM],\n                                    annot)) {\n      if (std::find(annot.begin(), annot.end(), arg->getArgNo()) != annot.end())\n        return true;\n    }\n  }\n  return false;\n}\n\n"},"isImage":{"range":[224,228],"code":"bool llvm::isImage(const llvm::Value &val) {\n  return llvm::isImageReadOnly(val) || llvm::isImageWriteOnly(val) ||\n         llvm::isImageReadWrite(val);\n}\n\n"},"isManaged":{"range":[229,241],"code":"bool llvm::isManaged(const llvm::Value &val) {\n  if(const GlobalValue *gv = dyn_cast<GlobalValue>(&val)) {\n    unsigned annot;\n    if(llvm::findOneNVVMAnnotation(gv,\n                          llvm::PropertyAnnotationNames[llvm::PROPERTY_MANAGED],\n                                   annot)) {\n      assert((annot == 1) && \"Unexpected annotation on a managed symbol\");\n      return true;\n    }\n  }\n  return false;\n}\n\n"},"getSurfaceName":{"range":[247,251],"code":"std::string llvm::getSurfaceName(const llvm::Value &val) {\n  assert(val.hasName() && \"Found surface variable with no name\");\n  return val.getName();\n}\n\n"},"isImageReadOnly":{"range":[179,193],"code":"bool llvm::isImageReadOnly(const llvm::Value &val) {\n  if (const Argument *arg = dyn_cast<Argument>(&val)) {\n    const Function *func = arg->getParent();\n    std::vector<unsigned> annot;\n    if (llvm::findAllNVVMAnnotation(func,\n                                    llvm::PropertyAnnotationNames[\n                                        llvm::PROPERTY_ISREADONLY_IMAGE_PARAM],\n                                    annot)) {\n      if (std::find(annot.begin(), annot.end(), arg->getArgNo()) != annot.end())\n        return true;\n    }\n  }\n  return false;\n}\n\n"},"getMaxNTIDy":{"range":[262,266],"code":"bool llvm::getMaxNTIDy(const Function &F, unsigned &y) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_MAXNTID_Y], y));\n}\n\n"},"dumpInstRec":{"range":[418,426],"code":"void llvm::dumpInstRec(Value *v) {\n  std::set<Instruction *> visited;\n\n  //BasicBlock *B = getParentBlock(v);\n\n  dumpInstRec(v, &visited);\n}\n\n// Dump the parent for Instruction, block or function\n"},"dumpInst":{"range":[395,401],"code":"void llvm::dumpInst(Value *base, char *instName) {\n  Instruction *I = getInst(base, instName);\n  if (I)\n    I->dump();\n}\n\n// Dump an instruction and all dependent instructions\n"},"isTexture":{"range":[130,142],"code":"bool llvm::isTexture(const llvm::Value &val) {\n  if (const GlobalValue *gv = dyn_cast<GlobalValue>(&val)) {\n    unsigned annot;\n    if (llvm::findOneNVVMAnnotation(\n            gv, llvm::PropertyAnnotationNames[llvm::PROPERTY_ISTEXTURE],\n            annot)) {\n      assert((annot == 1) && \"Unexpected annotation on a texture symbol\");\n      return true;\n    }\n  }\n  return false;\n}\n\n"},"isSampler":{"range":[156,178],"code":"bool llvm::isSampler(const llvm::Value &val) {\n  if (const GlobalValue *gv = dyn_cast<GlobalValue>(&val)) {\n    unsigned annot;\n    if (llvm::findOneNVVMAnnotation(\n            gv, llvm::PropertyAnnotationNames[llvm::PROPERTY_ISSAMPLER],\n            annot)) {\n      assert((annot == 1) && \"Unexpected annotation on a sampler symbol\");\n      return true;\n    }\n  }\n  if (const Argument *arg = dyn_cast<Argument>(&val)) {\n    const Function *func = arg->getParent();\n    std::vector<unsigned> annot;\n    if (llvm::findAllNVVMAnnotation(\n            func, llvm::PropertyAnnotationNames[llvm::PROPERTY_ISSAMPLER],\n            annot)) {\n      if (std::find(annot.begin(), annot.end(), arg->getArgNo()) != annot.end())\n        return true;\n    }\n  }\n  return false;\n}\n\n"},"getReqNTIDy":{"range":[277,281],"code":"bool llvm::getReqNTIDy(const Function &F, unsigned &y) {\n  return (llvm::findOneNVVMAnnotation(\n      &F, llvm::PropertyAnnotationNames[llvm::PROPERTY_REQNTID_Y], y));\n}\n\n"},"dumpParent":{"range":[427,443],"code":"void llvm::dumpParent(Value *v) {\n  if (Instruction *I = dyn_cast<Instruction>(v)) {\n    I->getParent()->dump();\n    return;\n  }\n\n  if (BasicBlock *B = dyn_cast<BasicBlock>(v)) {\n    B->getParent()->dump();\n    return;\n  }\n\n  if (Function *F = dyn_cast<Function>(v)) {\n    F->getParent()->dump();\n    return;\n  }\n}\n"},"dumpBlock":{"range":[364,378],"code":"void llvm::dumpBlock(Value *v, char *blockName) {\n  Function *F = getParentFunction(v);\n  if (!F)\n    return;\n\n  for (Function::iterator it = F->begin(), ie = F->end(); it != ie; ++it) {\n    BasicBlock *B = &*it;\n    if (strcmp(B->getName().data(), blockName) == 0) {\n      B->dump();\n      return;\n    }\n  }\n}\n\n// Find an instruction by name\n"}}},"NVVMReflect.cpp":{"path":"NVVMReflect.cpp","size":8155,"lines":220,"functions":{"runOnFunction":{"range":[116,221],"code":"bool NVVMReflect::runOnFunction(Function &F) {\n  if (!NVVMReflectEnabled)\n    return false;\n\n  if (F.getName() == NVVM_REFLECT_FUNCTION) {\n    assert(F.isDeclaration() && \"_reflect function should not have a body\");\n    assert(F.getReturnType()->isIntegerTy() &&\n           \"_reflect's return type should be integer\");\n    return false;\n  }\n\n  SmallVector<Instruction *, 4> ToRemove;\n\n  // Go through the calls in this function.  Each call to __nvvm_reflect or\n  // llvm.nvvm.reflect should be a CallInst with a ConstantArray argument.\n  // First validate that. If the c-string corresponding to the ConstantArray can\n  // be found successfully, see if it can be found in VarMap. If so, replace the\n  // uses of CallInst with the value found in VarMap. If not, replace the use\n  // with value 0.\n\n  // The IR for __nvvm_reflect calls differs between CUDA versions.\n  //\n  // CUDA 6.5 and earlier uses this sequence:\n  //    %ptr = tail call i8* @llvm.nvvm.ptr.constant.to.gen.p0i8.p4i8\n  //        (i8 addrspace(4)* getelementptr inbounds\n  //           ([8 x i8], [8 x i8] addrspace(4)* @str, i32 0, i32 0))\n  //    %reflect = tail call i32 @__nvvm_reflect(i8* %ptr)\n  //\n  // The value returned by Sym->getOperand(0) is a Constant with a\n  // ConstantDataSequential operand which can be converted to string and used\n  // for lookup.\n  //\n  // CUDA 7.0 does it slightly differently:\n  //   %reflect = call i32 @__nvvm_reflect(i8* addrspacecast\n  //        (i8 addrspace(1)* getelementptr inbounds\n  //           ([8 x i8], [8 x i8] addrspace(1)* @str, i32 0, i32 0) to i8*))\n  //\n  // In this case, we get a Constant with a GlobalVariable operand and we need\n  // to dig deeper to find its initializer with the string we'll use for lookup.\n  for (Instruction &I : instructions(F)) {\n    CallInst *Call = dyn_cast<CallInst>(&I);\n    if (!Call)\n      continue;\n    Function *Callee = Call->getCalledFunction();\n    if (!Callee || (Callee->getName() != NVVM_REFLECT_FUNCTION &&\n                    Callee->getIntrinsicID() != Intrinsic::nvvm_reflect))\n      continue;\n\n    // FIXME: Improve error handling here and elsewhere in this pass.\n    assert(Call->getNumOperands() == 2 &&\n           \"Wrong number of operands to __nvvm_reflect function\");\n\n    // In cuda 6.5 and earlier, we will have an extra constant-to-generic\n    // conversion of the string.\n    const Value *Str = Call->getArgOperand(0);\n    if (const CallInst *ConvCall = dyn_cast<CallInst>(Str)) {\n      // FIXME: Add assertions about ConvCall.\n      Str = ConvCall->getArgOperand(0);\n    }\n    assert(isa<ConstantExpr>(Str) &&\n           \"Format of __nvvm__reflect function not recognized\");\n    const ConstantExpr *GEP = cast<ConstantExpr>(Str);\n\n    const Value *Sym = GEP->getOperand(0);\n    assert(isa<Constant>(Sym) &&\n           \"Format of __nvvm_reflect function not recognized\");\n\n    const Value *Operand = cast<Constant>(Sym)->getOperand(0);\n    if (const GlobalVariable *GV = dyn_cast<GlobalVariable>(Operand)) {\n      // For CUDA-7.0 style __nvvm_reflect calls, we need to find the operand's\n      // initializer.\n      assert(GV->hasInitializer() &&\n             \"Format of _reflect function not recognized\");\n      const Constant *Initializer = GV->getInitializer();\n      Operand = Initializer;\n    }\n\n    assert(isa<ConstantDataSequential>(Operand) &&\n           \"Format of _reflect function not recognized\");\n    assert(cast<ConstantDataSequential>(Operand)->isCString() &&\n           \"Format of _reflect function not recognized\");\n\n    StringRef ReflectArg = cast<ConstantDataSequential>(Operand)->getAsString();\n    ReflectArg = ReflectArg.substr(0, ReflectArg.size() - 1);\n    DEBUG(dbgs() << \"Arg of _reflect : \" << ReflectArg << \"\\n\");\n\n    int ReflectVal = 0; // The default value is 0\n    auto Iter = VarMap.find(ReflectArg);\n    if (Iter != VarMap.end())\n      ReflectVal = Iter->second;\n    else if (ReflectArg == \"__CUDA_FTZ\") {\n      // Try to pull __CUDA_FTZ from the nvvm-reflect-ftz module flag.\n      if (auto *Flag = mdconst::extract_or_null<ConstantInt>(\n              F.getParent()->getModuleFlag(\"nvvm-reflect-ftz\")))\n        ReflectVal = Flag->getSExtValue();\n    }\n    Call->replaceAllUsesWith(ConstantInt::get(Call->getType(), ReflectVal));\n    ToRemove.push_back(Call);\n  }\n\n  for (Instruction *I : ToRemove)\n    I->eraseFromParent();\n\n  return ToRemove.size() > 0;\n}\n"},"setVarMap":{"range":[98,115],"code":"void NVVMReflect::setVarMap() {\n  for (unsigned i = 0, e = ReflectList.size(); i != e; ++i) {\n    DEBUG(dbgs() << \"Option : \"  << ReflectList[i] << \"\\n\");\n    SmallVector<StringRef, 4> NameValList;\n    StringRef(ReflectList[i]).split(NameValList, ',');\n    for (unsigned j = 0, ej = NameValList.size(); j != ej; ++j) {\n      SmallVector<StringRef, 2> NameValPair;\n      NameValList[j].split(NameValPair, '=');\n      assert(NameValPair.size() == 2 && \"name=val expected\");\n      std::stringstream ValStream(NameValPair[1]);\n      int Val;\n      ValStream >> Val;\n      assert((!(ValStream.fail())) && \"integer value expected\");\n      VarMap[NameValPair[0]] = Val;\n    }\n  }\n}\n\n"},"createNVVMReflectPass":{"range":[74,97],"code":"FunctionPass *llvm::createNVVMReflectPass(const StringMap<int> &Mapping) {\n  return new NVVMReflect(Mapping);\n}\n\nstatic cl::opt<bool>\nNVVMReflectEnabled(\"nvvm-reflect-enable\", cl::init(true), cl::Hidden,\n                   cl::desc(\"NVVM reflection, enabled by default\"));\n\nchar NVVMReflect::ID = 0;\nINITIALIZE_PASS(NVVMReflect, \"nvvm-reflect\",\n                \"Replace occurrences of __nvvm_reflect() calls with 0/1\", false,\n                false)\n\nstatic cl::list<std::string>\nReflectList(\"nvvm-reflect-list\", cl::value_desc(\"name=<int>\"), cl::Hidden,\n            cl::desc(\"A list of string=num assignments\"),\n            cl::ValueRequired);\n\n/// The command line can look as follows :\n/// -nvvm-reflect-list a=1,b=2 -nvvm-reflect-list c=3,d=0 -R e=2\n/// The strings \"a=1,b=2\", \"c=3,d=0\", \"e=2\" are available in the\n/// ReflectList vector. First, each of ReflectList[i] is 'split'\n/// using \",\" as the delimiter. Then each of this part is split\n/// using \"=\" as the delimiter.\n"},"NVVMReflect":{"range":[59,72],"code":"  NVVMReflect(const StringMap<int> &Mapping)\n      : FunctionPass(ID), VarMap(Mapping) {\n    initializeNVVMReflectPass(*PassRegistry::getPassRegistry());\n    setVarMap();\n  }\n\n  bool runOnFunction(Function &) override;\n\nprivate:\n  bool handleFunction(Function *ReflectFunction);\n  void setVarMap();\n};\n}\n\n"}}},"NVPTXLowerAlloca.cpp":{"path":"NVPTXLowerAlloca.cpp","size":4269,"lines":118,"functions":{"runOnBasicBlock":{"range":[64,115],"code":"bool NVPTXLowerAlloca::runOnBasicBlock(BasicBlock &BB) {\n  if (skipBasicBlock(BB))\n    return false;\n\n  bool Changed = false;\n  for (auto &I : BB) {\n    if (auto allocaInst = dyn_cast<AllocaInst>(&I)) {\n      Changed = true;\n      auto PTy = dyn_cast<PointerType>(allocaInst->getType());\n      auto ETy = PTy->getElementType();\n      auto LocalAddrTy = PointerType::get(ETy, ADDRESS_SPACE_LOCAL);\n      auto NewASCToLocal = new AddrSpaceCastInst(allocaInst, LocalAddrTy, \"\");\n      auto GenericAddrTy = PointerType::get(ETy, ADDRESS_SPACE_GENERIC);\n      auto NewASCToGeneric = new AddrSpaceCastInst(NewASCToLocal,\n                                                    GenericAddrTy, \"\");\n      NewASCToLocal->insertAfter(allocaInst);\n      NewASCToGeneric->insertAfter(NewASCToLocal);\n      for (Value::use_iterator UI = allocaInst->use_begin(),\n                                UE = allocaInst->use_end();\n            UI != UE; ) {\n        // Check Load, Store, GEP, and BitCast Uses on alloca and make them\n        // use the converted generic address, in order to expose non-generic\n        // addrspacecast to NVPTXFavorNonGenericAddrSpace. For other types\n        // of instructions this is unnecessary and may introduce redundant\n        // address cast.\n        const auto &AllocaUse = *UI++;\n        auto LI = dyn_cast<LoadInst>(AllocaUse.getUser());\n        if (LI && LI->getPointerOperand() == allocaInst && !LI->isVolatile()) {\n          LI->setOperand(LI->getPointerOperandIndex(), NewASCToGeneric);\n          continue;\n        }\n        auto SI = dyn_cast<StoreInst>(AllocaUse.getUser());\n        if (SI && SI->getPointerOperand() == allocaInst && !SI->isVolatile()) {\n          SI->setOperand(SI->getPointerOperandIndex(), NewASCToGeneric);\n          continue;\n        }\n        auto GI = dyn_cast<GetElementPtrInst>(AllocaUse.getUser());\n        if (GI && GI->getPointerOperand() == allocaInst) {\n          GI->setOperand(GI->getPointerOperandIndex(), NewASCToGeneric);\n          continue;\n        }\n        auto BI = dyn_cast<BitCastInst>(AllocaUse.getUser());\n        if (BI && BI->getOperand(0) == allocaInst) {\n          BI->setOperand(0, NewASCToGeneric);\n          continue;\n        }\n      }\n    }\n  }\n  return Changed;\n}\n\n"},"createNVPTXLowerAllocaPass":{"range":[116,119],"code":"BasicBlockPass *llvm::createNVPTXLowerAllocaPass() {\n  return new NVPTXLowerAlloca();\n}\n"},"NVPTXLowerAlloca":{"range":[49,63],"code":"  NVPTXLowerAlloca() : BasicBlockPass(ID) {}\n  const char *getPassName() const override {\n    return \"convert address space of alloca'ed memory to local\";\n  }\n};\n} // namespace\n\nchar NVPTXLowerAlloca::ID = 1;\n\nINITIALIZE_PASS(NVPTXLowerAlloca, \"nvptx-lower-alloca\",\n                \"Lower Alloca\", false, false)\n\n// =============================================================================\n// Main function for this pass.\n// =============================================================================\n"}}},"NVPTXTargetMachine.cpp":{"path":"NVPTXTargetMachine.cpp","size":13847,"lines":377,"functions":{"createTargetRegisterAllocator":{"range":[305,308],"code":"FunctionPass *NVPTXPassConfig::createTargetRegisterAllocator(bool) {\n  return nullptr; // No reg alloc\n}\n\n"},"anchor":{"range":[133,134],"code":"void NVPTXTargetMachine64::anchor() {}\n\n"},"NVPTXTargetMachine32":{"range":[125,132],"code":"NVPTXTargetMachine32::NVPTXTargetMachine32(const Target &T, const Triple &TT,\n                                           StringRef CPU, StringRef FS,\n                                           const TargetOptions &Options,\n                                           Optional<Reloc::Model> RM,\n                                           CodeModel::Model CM,\n                                           CodeGenOpt::Level OL)\n    : NVPTXTargetMachine(T, TT, CPU, FS, Options, RM, CM, OL, false) {}\n\n"},"getTargetIRAnalysis":{"range":[184,189],"code":"TargetIRAnalysis NVPTXTargetMachine::getTargetIRAnalysis() {\n  return TargetIRAnalysis([this](const Function &F) {\n    return TargetTransformInfo(NVPTXTTIImpl(this, F));\n  });\n}\n\n"},"addMachineSSAOptimization":{"range":[339,378],"code":"void NVPTXPassConfig::addMachineSSAOptimization() {\n  // Pre-ra tail duplication.\n  if (addPass(&EarlyTailDuplicateID))\n    printAndVerify(\"After Pre-RegAlloc TailDuplicate\");\n\n  // Optimize PHIs before DCE: removing dead PHI cycles may make more\n  // instructions dead.\n  addPass(&OptimizePHIsID);\n\n  // This pass merges large allocas. StackSlotColoring is a different pass\n  // which merges spill slots.\n  addPass(&StackColoringID);\n\n  // If the target requests it, assign local variables to stack slots relative\n  // to one another and simplify frame index references where possible.\n  addPass(&LocalStackSlotAllocationID);\n\n  // With optimization, dead code should already be eliminated. However\n  // there is one known exception: lowered code for arguments that are only\n  // used by tail calls, where the tail calls reuse the incoming stack\n  // arguments directly (see t11 in test/CodeGen/X86/sibcall.ll).\n  addPass(&DeadMachineInstructionElimID);\n  printAndVerify(\"After codegen DCE pass\");\n\n  // Allow targets to insert passes that improve instruction level parallelism,\n  // like if-conversion. Such passes will typically need dominator trees and\n  // loop info, just like LICM and CSE below.\n  if (addILPOpts())\n    printAndVerify(\"After ILP optimizations\");\n\n  addPass(&MachineLICMID);\n  addPass(&MachineCSEID);\n\n  addPass(&MachineSinkingID);\n  printAndVerify(\"After Machine LICM, CSE and Sinking passes\");\n\n  addPass(&PeepholeOptimizerID);\n  printAndVerify(\"After codegen peephole optimization pass\");\n}\n"},"NVPTXPassConfig":{"range":[146,148],"code":"  NVPTXPassConfig(NVPTXTargetMachine *TM, PassManagerBase &PM)\n      : TargetPassConfig(TM, PM) {}\n\n"},"addStraightLineScalarOptimizationPasses":{"range":[213,229],"code":"void NVPTXPassConfig::addStraightLineScalarOptimizationPasses() {\n  addPass(createSeparateConstOffsetFromGEPPass());\n  addPass(createSpeculativeExecutionPass());\n  // ReassociateGEPs exposes more opportunites for SLSR. See\n  // the example in reassociate-geps-and-slsr.ll.\n  addPass(createStraightLineStrengthReducePass());\n  // SeparateConstOffsetFromGEP and SLSR creates common expressions which GVN or\n  // EarlyCSE can reuse. GVN generates significantly better code than EarlyCSE\n  // for some of our benchmarks.\n  addEarlyCSEOrGVNPass();\n  // Run NaryReassociate after EarlyCSE/GVN to be more effective.\n  addPass(createNaryReassociatePass());\n  // NaryReassociate on GEPs creates redundant common expressions, so run\n  // EarlyCSE after it.\n  addPass(createEarlyCSEPass());\n}\n\n"},"addAddressSpaceInferencePasses":{"range":[197,212],"code":"void NVPTXPassConfig::addAddressSpaceInferencePasses() {\n  // NVPTXLowerKernelArgs emits alloca for byval parameters which can often\n  // be eliminated by SROA.\n  addPass(createSROAPass());\n  addPass(createNVPTXLowerAllocaPass());\n  if (UseInferAddressSpaces) {\n    addPass(createNVPTXInferAddressSpacesPass());\n  } else {\n    addPass(createNVPTXFavorNonGenericAddrSpacesPass());\n    // FavorNonGenericAddrSpaces shortcuts unnecessary addrspacecasts, and leave\n    // them unused. We could remove dead code in an ad-hoc manner, but that\n    // requires manual work and might be error-prone.\n    addPass(createDeadCodeEliminationPass());\n  }\n}\n\n"},"addOptimizedRegAlloc":{"range":[315,338],"code":"void NVPTXPassConfig::addOptimizedRegAlloc(FunctionPass *RegAllocPass) {\n  assert(!RegAllocPass && \"NVPTX uses no regalloc!\");\n\n  addPass(&ProcessImplicitDefsID);\n  addPass(&LiveVariablesID);\n  addPass(&MachineLoopInfoID);\n  addPass(&PHIEliminationID);\n\n  addPass(&TwoAddressInstructionPassID);\n  addPass(&RegisterCoalescerID);\n\n  // PreRA instruction scheduling.\n  if (addPass(&MachineSchedulerID))\n    printAndVerify(\"After Machine Scheduling\");\n\n\n  addPass(&StackSlotColoringID);\n\n  // FIXME: Needs physical registers\n  //addPass(&PostRAMachineLICMID);\n\n  printAndVerify(\"After StackSlotColoring\");\n}\n\n"},"NVPTXTargetMachine64":{"range":[135,145],"code":"NVPTXTargetMachine64::NVPTXTargetMachine64(const Target &T, const Triple &TT,\n                                           StringRef CPU, StringRef FS,\n                                           const TargetOptions &Options,\n                                           Optional<Reloc::Model> RM,\n                                           CodeModel::Model CM,\n                                           CodeGenOpt::Level OL)\n    : NVPTXTargetMachine(T, TT, CPU, FS, Options, RM, CM, OL, true) {}\n\nnamespace {\nclass NVPTXPassConfig : public TargetPassConfig {\npublic:\n"},"addEarlyCSEOrGVNPass":{"range":[190,196],"code":"void NVPTXPassConfig::addEarlyCSEOrGVNPass() {\n  if (getOptLevel() == CodeGenOpt::Aggressive)\n    addPass(createGVNPass());\n  else\n    addPass(createEarlyCSEPass());\n}\n\n"},"addInstSelector":{"range":[282,294],"code":"bool NVPTXPassConfig::addInstSelector() {\n  const NVPTXSubtarget &ST = *getTM<NVPTXTargetMachine>().getSubtargetImpl();\n\n  addPass(createLowerAggrCopies());\n  addPass(createAllocaHoisting());\n  addPass(createNVPTXISelDag(getNVPTXTargetMachine(), getOptLevel()));\n\n  if (!ST.hasImageHandles())\n    addPass(createNVPTXReplaceImageHandlesPass());\n\n  return false;\n}\n\n"},"NVPTXTargetMachine":{"range":[101,122],"code":"NVPTXTargetMachine::NVPTXTargetMachine(const Target &T, const Triple &TT,\n                                       StringRef CPU, StringRef FS,\n                                       const TargetOptions &Options,\n                                       Optional<Reloc::Model> RM,\n                                       CodeModel::Model CM,\n                                       CodeGenOpt::Level OL, bool is64bit)\n    // The pic relocation model is used regardless of what the client has\n    // specified, as it is the only relocation model currently supported.\n    : LLVMTargetMachine(T, computeDataLayout(is64bit), TT, CPU, FS, Options,\n                        Reloc::PIC_, CM, OL),\n      is64bit(is64bit),\n      TLOF(make_unique<NVPTXTargetObjectFile>()),\n      Subtarget(TT, CPU, FS, *this) {\n  if (TT.getOS() == Triple::NVCL)\n    drvInterface = NVPTX::NVCL;\n  else\n    drvInterface = NVPTX::CUDA;\n  initAsmInfo();\n}\n\nNVPTXTargetMachine::~NVPTXTargetMachine() {}\n\n"},"addPostRegAlloc":{"range":[295,304],"code":"void NVPTXPassConfig::addPostRegAlloc() {\n  addPass(createNVPTXPrologEpilogPass(), false);\n  if (getOptLevel() != CodeGenOpt::None) {\n    // NVPTXPrologEpilogPass calculates frame object offset and replace frame\n    // index with VRFrame register. NVPTXPeephole need to be run after that and\n    // will replace VRFrame with VRFrameLocal when possible.\n    addPass(createNVPTXPeephole());\n  }\n}\n\n"},"addIRPasses":{"range":[230,281],"code":"void NVPTXPassConfig::addIRPasses() {\n  // The following passes are known to not play well with virtual regs hanging\n  // around after register allocation (which in our case, is *all* registers).\n  // We explicitly disable them here.  We do, however, need some functionality\n  // of the PrologEpilogCodeInserter pass, so we emulate that behavior in the\n  // NVPTXPrologEpilog pass (see NVPTXPrologEpilogPass.cpp).\n  disablePass(&PrologEpilogCodeInserterID);\n  disablePass(&MachineCopyPropagationID);\n  disablePass(&TailDuplicateID);\n  disablePass(&StackMapLivenessID);\n  disablePass(&LiveDebugValuesID);\n  disablePass(&PostRASchedulerID);\n  disablePass(&FuncletLayoutID);\n  disablePass(&PatchableFunctionID);\n\n  // NVVMReflectPass is added in addEarlyAsPossiblePasses, so hopefully running\n  // it here does nothing.  But since we need it for correctness when lowering\n  // to NVPTX, run it here too, in case whoever built our pass pipeline didn't\n  // call addEarlyAsPossiblePasses.\n  addPass(createNVVMReflectPass());\n\n  if (getOptLevel() != CodeGenOpt::None)\n    addPass(createNVPTXImageOptimizerPass());\n  addPass(createNVPTXAssignValidGlobalNamesPass());\n  addPass(createGenericToNVVMPass());\n\n  // NVPTXLowerKernelArgs is required for correctness and should be run right\n  // before the address space inference passes.\n  addPass(createNVPTXLowerKernelArgsPass(&getNVPTXTargetMachine()));\n  if (getOptLevel() != CodeGenOpt::None) {\n    addAddressSpaceInferencePasses();\n    addStraightLineScalarOptimizationPasses();\n  }\n\n  // === LSR and other generic IR passes ===\n  TargetPassConfig::addIRPasses();\n  // EarlyCSE is not always strong enough to clean up what LSR produces. For\n  // example, GVN can combine\n  //\n  //   %0 = add %a, %b\n  //   %1 = add %b, %a\n  //\n  // and\n  //\n  //   %0 = shl nsw %a, 2\n  //   %1 = shl %a, 2\n  //\n  // but EarlyCSE can do neither of them.\n  if (getOptLevel() != CodeGenOpt::None)\n    addEarlyCSEOrGVNPass();\n}\n\n"},"computeDataLayout":{"range":[90,100],"code":"static std::string computeDataLayout(bool is64Bit) {\n  std::string Ret = \"e\";\n\n  if (!is64Bit)\n    Ret += \"-p:32:32\";\n\n  Ret += \"-i64:64-v16:16-v32:32-n16:32:64\";\n\n  return Ret;\n}\n\n"},"addFastRegAlloc":{"range":[309,314],"code":"void NVPTXPassConfig::addFastRegAlloc(FunctionPass *RegAllocPass) {\n  assert(!RegAllocPass && \"NVPTX uses no regalloc!\");\n  addPass(&PHIEliminationID);\n  addPass(&TwoAddressInstructionPassID);\n}\n\n"},"LLVMInitializeNVPTXTarget":{"range":[70,89],"code":"extern \"C\" void LLVMInitializeNVPTXTarget() {\n  // Register the target.\n  RegisterTargetMachine<NVPTXTargetMachine32> X(TheNVPTXTarget32);\n  RegisterTargetMachine<NVPTXTargetMachine64> Y(TheNVPTXTarget64);\n\n  // FIXME: This pass is really intended to be invoked during IR optimization,\n  // but it's very NVPTX-specific.\n  PassRegistry &PR = *PassRegistry::getPassRegistry();\n  initializeNVVMReflectPass(PR);\n  initializeNVVMIntrRangePass(PR);\n  initializeGenericToNVVMPass(PR);\n  initializeNVPTXAllocaHoistingPass(PR);\n  initializeNVPTXAssignValidGlobalNamesPass(PR);\n  initializeNVPTXFavorNonGenericAddrSpacesPass(PR);\n  initializeNVPTXInferAddressSpacesPass(PR);\n  initializeNVPTXLowerKernelArgsPass(PR);\n  initializeNVPTXLowerAllocaPass(PR);\n  initializeNVPTXLowerAggrCopiesPass(PR);\n}\n\n"},"getNVPTXTargetMachine":{"range":[149,174],"code":"  NVPTXTargetMachine &getNVPTXTargetMachine() const {\n    return getTM<NVPTXTargetMachine>();\n  }\n\n  void addIRPasses() override;\n  bool addInstSelector() override;\n  void addPostRegAlloc() override;\n  void addMachineSSAOptimization() override;\n\n  FunctionPass *createTargetRegisterAllocator(bool) override;\n  void addFastRegAlloc(FunctionPass *RegAllocPass) override;\n  void addOptimizedRegAlloc(FunctionPass *RegAllocPass) override;\n\nprivate:\n  // If the opt level is aggressive, add GVN; otherwise, add EarlyCSE. This\n  // function is only called in opt mode.\n  void addEarlyCSEOrGVNPass();\n\n  // Add passes that propagate special memory spaces.\n  void addAddressSpaceInferencePasses();\n\n  // Add passes that perform straight-line scalar optimizations.\n  void addStraightLineScalarOptimizationPasses();\n};\n} // end anonymous namespace\n\n"},"addEarlyAsPossiblePasses":{"range":[179,183],"code":"void NVPTXTargetMachine::addEarlyAsPossiblePasses(PassManagerBase &PM) {\n  PM.add(createNVVMReflectPass());\n  PM.add(createNVVMIntrRangePass(Subtarget.getSmVersion()));\n}\n\n"},"createPassConfig":{"range":[175,178],"code":"TargetPassConfig *NVPTXTargetMachine::createPassConfig(PassManagerBase &PM) {\n  return new NVPTXPassConfig(this, PM);\n}\n\n"}}},"NVPTXSubtarget.cpp":{"path":"NVPTXSubtarget.cpp","size":1941,"lines":61,"functions":{"hasImageHandles":{"range":[53,62],"code":"bool NVPTXSubtarget::hasImageHandles() const {\n  // Enable handles for Kepler+, where CUDA supports indirect surfaces and\n  // textures\n  if (TM.getDrvInterface() == NVPTX::CUDA)\n    return (SmVersion >= 30);\n\n  // Disabled, otherwise\n  return false;\n}\n"},"NVPTXSubtarget":{"range":[46,52],"code":"NVPTXSubtarget::NVPTXSubtarget(const Triple &TT, const std::string &CPU,\n                               const std::string &FS,\n                               const NVPTXTargetMachine &TM)\n    : NVPTXGenSubtargetInfo(TT, CPU, FS), PTXVersion(0), SmVersion(20), TM(TM),\n      InstrInfo(), TLInfo(TM, initializeSubtargetDependencies(CPU, FS)),\n      FrameLowering() {}\n\n"},"initializeSubtargetDependencies":{"range":[29,45],"code":"NVPTXSubtarget &NVPTXSubtarget::initializeSubtargetDependencies(StringRef CPU,\n                                                                StringRef FS) {\n    // Provide the default CPU if we don't have one.\n  if (CPU.empty() && FS.size())\n    llvm_unreachable(\"we are not using FeatureStr\");\n  TargetName = CPU.empty() ? \"sm_20\" : CPU;\n\n  ParseSubtargetFeatures(TargetName, FS);\n\n  // Set default to PTX 3.2 (CUDA 5.5)\n  if (PTXVersion == 0) {\n    PTXVersion = 32;\n  }\n\n  return *this;\n}\n\n"},"anchor":{"range":[27,28],"code":"void NVPTXSubtarget::anchor() {}\n\n"}}},"NVPTXInferAddressSpaces.cpp":{"path":"NVPTXInferAddressSpaces.cpp","size":24045,"lines":586,"functions":{"isAddressExpression":{"range":[160,177],"code":"static bool isAddressExpression(const Value &V) {\n  if (!isa<Operator>(V))\n    return false;\n\n  switch (cast<Operator>(V).getOpcode()) {\n  case Instruction::PHI:\n  case Instruction::BitCast:\n  case Instruction::AddrSpaceCast:\n  case Instruction::GetElementPtr:\n    return true;\n  default:\n    return false;\n  }\n}\n\n// Returns the pointer operands of V.\n//\n// Precondition: V is an address expression.\n"},"inferAddressSpaces":{"range":[438,482],"code":"void NVPTXInferAddressSpaces::inferAddressSpaces(\n    const std::vector<Value *> &Postorder,\n    ValueToAddrSpaceMapTy *InferredAddrSpace) {\n  SetVector<Value *> Worklist(Postorder.begin(), Postorder.end());\n  // Initially, all expressions are in the uninitialized address space.\n  for (Value *V : Postorder)\n    (*InferredAddrSpace)[V] = ADDRESS_SPACE_UNINITIALIZED;\n\n  while (!Worklist.empty()) {\n    Value* V = Worklist.pop_back_val();\n\n    // Tries to update the address space of the stack top according to the\n    // address spaces of its operands.\n    DEBUG(dbgs() << \"Updating the address space of\\n\"\n                 << \"  \" << *V << \"\\n\");\n    Optional<unsigned> NewAS = updateAddressSpace(*V, *InferredAddrSpace);\n    if (!NewAS.hasValue())\n      continue;\n    // If any updates are made, grabs its users to the worklist because\n    // their address spaces can also be possibly updated.\n    DEBUG(dbgs() << \"  to \" << NewAS.getValue() << \"\\n\");\n    (*InferredAddrSpace)[V] = NewAS.getValue();\n\n    for (Value *User : V->users()) {\n      // Skip if User is already in the worklist.\n      if (Worklist.count(User))\n        continue;\n\n      auto Pos = InferredAddrSpace->find(User);\n      // Our algorithm only updates the address spaces of generic address\n      // expressions, which are those in InferredAddrSpace.\n      if (Pos == InferredAddrSpace->end())\n        continue;\n\n      // Function updateAddressSpace moves the address space down a lattice\n      // path. Therefore, nothing to do if User is already inferred as\n      // generic (the bottom element in the lattice).\n      if (Pos->second == AddressSpace::ADDRESS_SPACE_GENERIC)\n        continue;\n\n      Worklist.insert(User);\n    }\n  }\n}\n\n"},"joinAddressSpaces":{"range":[407,420],"code":"static unsigned joinAddressSpaces(unsigned AS1, unsigned AS2) {\n  if (AS1 == AddressSpace::ADDRESS_SPACE_GENERIC ||\n      AS2 == AddressSpace::ADDRESS_SPACE_GENERIC)\n    return AddressSpace::ADDRESS_SPACE_GENERIC;\n\n  if (AS1 == ADDRESS_SPACE_UNINITIALIZED)\n    return AS2;\n  if (AS2 == ADDRESS_SPACE_UNINITIALIZED)\n    return AS1;\n\n  // The join of two different specific address spaces is generic.\n  return AS1 == AS2 ? AS1 : (unsigned)AddressSpace::ADDRESS_SPACE_GENERIC;\n}\n\n"},"rewriteWithNewAddressSpaces":{"range":[509,583],"code":"bool NVPTXInferAddressSpaces::rewriteWithNewAddressSpaces(\n    const std::vector<Value *> &Postorder,\n    const ValueToAddrSpaceMapTy &InferredAddrSpace, Function *F) {\n  // For each address expression to be modified, creates a clone of it with its\n  // pointer operands converted to the new address space. Since the pointer\n  // operands are converted, the clone is naturally in the new address space by\n  // construction.\n  ValueToValueMapTy ValueWithNewAddrSpace;\n  SmallVector<const Use *, 32> UndefUsesToFix;\n  for (Value* V : Postorder) {\n    unsigned NewAddrSpace = InferredAddrSpace.lookup(V);\n    if (V->getType()->getPointerAddressSpace() != NewAddrSpace) {\n      ValueWithNewAddrSpace[V] = cloneValueWithNewAddressSpace(\n          V, NewAddrSpace, ValueWithNewAddrSpace, &UndefUsesToFix);\n    }\n  }\n\n  if (ValueWithNewAddrSpace.empty())\n    return false;\n\n  // Fixes all the undef uses generated by cloneInstructionWithNewAddressSpace.\n  for (const Use* UndefUse : UndefUsesToFix) {\n    User *V = UndefUse->getUser();\n    User *NewV = cast<User>(ValueWithNewAddrSpace.lookup(V));\n    unsigned OperandNo = UndefUse->getOperandNo();\n    assert(isa<UndefValue>(NewV->getOperand(OperandNo)));\n    NewV->setOperand(OperandNo, ValueWithNewAddrSpace.lookup(UndefUse->get()));\n  }\n\n  // Replaces the uses of the old address expressions with the new ones.\n  for (Value *V : Postorder) {\n    Value *NewV = ValueWithNewAddrSpace.lookup(V);\n    if (NewV == nullptr)\n      continue;\n\n    SmallVector<Use *, 4> Uses;\n    for (Use &U : V->uses())\n      Uses.push_back(&U);\n    DEBUG(dbgs() << \"Replacing the uses of \" << *V << \"\\n  to\\n  \" << *NewV\n                 << \"\\n\");\n    for (Use *U : Uses) {\n      if (isa<LoadInst>(U->getUser()) ||\n          (isa<StoreInst>(U->getUser()) && U->getOperandNo() == 1)) {\n        // If V is used as the pointer operand of a load/store, sets the pointer\n        // operand to NewV. This replacement does not change the element type,\n        // so the resultant load/store is still valid.\n        U->set(NewV);\n      } else if (isa<Instruction>(U->getUser())) {\n        // Otherwise, replaces the use with generic(NewV).\n        // TODO: Some optimization opportunities are missed. For example, in\n        //   %0 = icmp eq float* %p, %q\n        // if both p and q are inferred to be shared, we can rewrite %0 as\n        //   %0 = icmp eq float addrspace(3)* %new_p, %new_q\n        // instead of currently\n        //   %generic_p = addrspacecast float addrspace(3)* %new_p to float*\n        //   %generic_q = addrspacecast float addrspace(3)* %new_q to float*\n        //   %0 = icmp eq float* %generic_p, %generic_q\n        if (Instruction *I = dyn_cast<Instruction>(V)) {\n          BasicBlock::iterator InsertPos = std::next(I->getIterator());\n          while (isa<PHINode>(InsertPos))\n            ++InsertPos;\n          U->set(new AddrSpaceCastInst(NewV, V->getType(), \"\", &*InsertPos));\n        } else {\n          U->set(ConstantExpr::getAddrSpaceCast(cast<Constant>(NewV),\n                                                V->getType()));\n        }\n      }\n    }\n    if (V->use_empty())\n      RecursivelyDeleteTriviallyDeadInstructions(V);\n  }\n\n  return true;\n}\n\n"},"createNVPTXInferAddressSpacesPass":{"range":[584,587],"code":"FunctionPass *llvm::createNVPTXInferAddressSpacesPass() {\n  return new NVPTXInferAddressSpaces();\n}\n"},"operandWithNewAddressSpaceOrCreateUndef":{"range":[252,273],"code":"static Value *operandWithNewAddressSpaceOrCreateUndef(\n    const Use &OperandUse, unsigned NewAddrSpace,\n    const ValueToValueMapTy &ValueWithNewAddrSpace,\n    SmallVectorImpl<const Use *> *UndefUsesToFix) {\n  Value *Operand = OperandUse.get();\n  if (Value *NewOperand = ValueWithNewAddrSpace.lookup(Operand))\n    return NewOperand;\n\n  UndefUsesToFix->push_back(&OperandUse);\n  return UndefValue::get(\n      Operand->getType()->getPointerElementType()->getPointerTo(NewAddrSpace));\n}\n\n// Returns a clone of `I` with its operands converted to those specified in\n// ValueWithNewAddrSpace. Due to potential cycles in the data flow graph, an\n// operand whose address space needs to be modified might not exist in\n// ValueWithNewAddrSpace. In that case, uses undef as a placeholder operand and\n// adds that operand use to UndefUsesToFix so that caller can fix them later.\n//\n// Note that we do not necessarily clone `I`, e.g., if it is an addrspacecast\n// from a pointer whose type already matches. Therefore, this function returns a\n// Value* instead of an Instruction*.\n"},"cloneInstructionWithNewAddressSpace":{"range":[274,331],"code":"static Value *cloneInstructionWithNewAddressSpace(\n    Instruction *I, unsigned NewAddrSpace,\n    const ValueToValueMapTy &ValueWithNewAddrSpace,\n    SmallVectorImpl<const Use *> *UndefUsesToFix) {\n  Type *NewPtrType =\n      I->getType()->getPointerElementType()->getPointerTo(NewAddrSpace);\n\n  if (I->getOpcode() == Instruction::AddrSpaceCast) {\n    Value *Src = I->getOperand(0);\n    // Because `I` is generic, the source address space must be specific.\n    // Therefore, the inferred address space must be the source space, according\n    // to our algorithm.\n    assert(Src->getType()->getPointerAddressSpace() == NewAddrSpace);\n    if (Src->getType() != NewPtrType)\n      return new BitCastInst(Src, NewPtrType);\n    return Src;\n  }\n\n  // Computes the converted pointer operands.\n  SmallVector<Value *, 4> NewPointerOperands;\n  for (const Use &OperandUse : I->operands()) {\n    if (!OperandUse.get()->getType()->isPointerTy())\n      NewPointerOperands.push_back(nullptr);\n    else\n      NewPointerOperands.push_back(operandWithNewAddressSpaceOrCreateUndef(\n          OperandUse, NewAddrSpace, ValueWithNewAddrSpace, UndefUsesToFix));\n  }\n\n  switch (I->getOpcode()) {\n  case Instruction::BitCast:\n    return new BitCastInst(NewPointerOperands[0], NewPtrType);\n  case Instruction::PHI: {\n    assert(I->getType()->isPointerTy());\n    PHINode *PHI = cast<PHINode>(I);\n    PHINode *NewPHI = PHINode::Create(NewPtrType, PHI->getNumIncomingValues());\n    for (unsigned Index = 0; Index < PHI->getNumIncomingValues(); ++Index) {\n      unsigned OperandNo = PHINode::getOperandNumForIncomingValue(Index);\n      NewPHI->addIncoming(NewPointerOperands[OperandNo],\n                          PHI->getIncomingBlock(Index));\n    }\n    return NewPHI;\n  }\n  case Instruction::GetElementPtr: {\n    GetElementPtrInst *GEP = cast<GetElementPtrInst>(I);\n    GetElementPtrInst *NewGEP = GetElementPtrInst::Create(\n        GEP->getSourceElementType(), NewPointerOperands[0],\n        SmallVector<Value *, 4>(GEP->idx_begin(), GEP->idx_end()));\n    NewGEP->setIsInBounds(GEP->isInBounds());\n    return NewGEP;\n  }\n  default:\n    llvm_unreachable(\"Unexpected opcode\");\n  }\n}\n\n// Similar to cloneInstructionWithNewAddressSpace, returns a clone of the\n// constant expression `CE` with its operands replaced as specified in\n// ValueWithNewAddrSpace.\n"},"cloneConstantExprWithNewAddressSpace":{"range":[332,380],"code":"static Value *cloneConstantExprWithNewAddressSpace(\n    ConstantExpr *CE, unsigned NewAddrSpace,\n    const ValueToValueMapTy &ValueWithNewAddrSpace) {\n  Type *TargetType =\n      CE->getType()->getPointerElementType()->getPointerTo(NewAddrSpace);\n\n  if (CE->getOpcode() == Instruction::AddrSpaceCast) {\n    // Because CE is generic, the source address space must be specific.\n    // Therefore, the inferred address space must be the source space according\n    // to our algorithm.\n    assert(CE->getOperand(0)->getType()->getPointerAddressSpace() ==\n           NewAddrSpace);\n    return ConstantExpr::getBitCast(CE->getOperand(0), TargetType);\n  }\n\n  // Computes the operands of the new constant expression.\n  SmallVector<Constant *, 4> NewOperands;\n  for (unsigned Index = 0; Index < CE->getNumOperands(); ++Index) {\n    Constant *Operand = CE->getOperand(Index);\n    // If the address space of `Operand` needs to be modified, the new operand\n    // with the new address space should already be in ValueWithNewAddrSpace\n    // because (1) the constant expressions we consider (i.e. addrspacecast,\n    // bitcast, and getelementptr) do not incur cycles in the data flow graph\n    // and (2) this function is called on constant expressions in postorder.\n    if (Value *NewOperand = ValueWithNewAddrSpace.lookup(Operand)) {\n      NewOperands.push_back(cast<Constant>(NewOperand));\n    } else {\n      // Otherwise, reuses the old operand.\n      NewOperands.push_back(Operand);\n    }\n  }\n\n  if (CE->getOpcode() == Instruction::GetElementPtr) {\n    // Needs to specify the source type while constructing a getelementptr\n    // constant expression.\n    return CE->getWithOperands(\n        NewOperands, TargetType, /*OnlyIfReduced=*/false,\n        NewOperands[0]->getType()->getPointerElementType());\n  }\n\n  return CE->getWithOperands(NewOperands, TargetType);\n}\n\n// Returns a clone of the value `V`, with its operands replaced as specified in\n// ValueWithNewAddrSpace. This function is called on every generic address\n// expression whose address space needs to be modified, in postorder.\n//\n// See cloneInstructionWithNewAddressSpace for the meaning of UndefUsesToFix.\nstatic Value *\n"},"runOnFunction":{"range":[421,437],"code":"bool NVPTXInferAddressSpaces::runOnFunction(Function &F) {\n  if (skipFunction(F))\n    return false;\n\n  // Collects all generic address expressions in postorder.\n  std::vector<Value *> Postorder = collectGenericAddressExpressions(F);\n\n  // Runs a data-flow analysis to refine the address spaces of every expression\n  // in Postorder.\n  ValueToAddrSpaceMapTy InferredAddrSpace;\n  inferAddressSpaces(Postorder, &InferredAddrSpace);\n\n  // Changes the address spaces of the generic address expressions who are\n  // inferred to point to a specific address space.\n  return rewriteWithNewAddressSpaces(Postorder, InferredAddrSpace, &F);\n}\n\n"},"appendsGenericAddressExpressionToPostorderStack":{"range":[198,211],"code":"static void appendsGenericAddressExpressionToPostorderStack(\n    Value *V, std::vector<std::pair<Value *, bool>> *PostorderStack,\n    DenseSet<Value *> *Visited) {\n  assert(V->getType()->isPointerTy());\n  if (isAddressExpression(*V) &&\n      V->getType()->getPointerAddressSpace() ==\n          AddressSpace::ADDRESS_SPACE_GENERIC) {\n    if (Visited->insert(V).second)\n      PostorderStack->push_back(std::make_pair(V, false));\n  }\n}\n\n// Returns all generic address expressions in function F. The elements are\n// ordered in postorder.\n"},"collectGenericAddressExpressions":{"range":[212,251],"code":"static std::vector<Value *> collectGenericAddressExpressions(Function &F) {\n  // This function implements a non-recursive postorder traversal of a partial\n  // use-def graph of function F.\n  std::vector<std::pair<Value*, bool>> PostorderStack;\n  // The set of visited expressions.\n  DenseSet<Value*> Visited;\n  // We only explore address expressions that are reachable from loads and\n  // stores for now because we aim at generating faster loads and stores.\n  for (Instruction &I : instructions(F)) {\n    if (isa<LoadInst>(I)) {\n      appendsGenericAddressExpressionToPostorderStack(\n          I.getOperand(0), &PostorderStack, &Visited);\n    } else if (isa<StoreInst>(I)) {\n      appendsGenericAddressExpressionToPostorderStack(\n          I.getOperand(1), &PostorderStack, &Visited);\n    }\n  }\n\n  std::vector<Value *> Postorder; // The resultant postorder.\n  while (!PostorderStack.empty()) {\n    // If the operands of the expression on the top are already explored,\n    // adds that expression to the resultant postorder.\n    if (PostorderStack.back().second) {\n      Postorder.push_back(PostorderStack.back().first);\n      PostorderStack.pop_back();\n      continue;\n    }\n    // Otherwise, adds its operands to the stack and explores them.\n    PostorderStack.back().second = true;\n    for (Value *PtrOperand : getPointerOperands(*PostorderStack.back().first)) {\n      appendsGenericAddressExpressionToPostorderStack(\n          PtrOperand, &PostorderStack, &Visited);\n    }\n  }\n  return Postorder;\n}\n\n// A helper function for cloneInstructionWithNewAddressSpace. Returns the clone\n// of OperandUse.get() in the new address space. If the clone is not ready yet,\n// returns an undef in the new address space as a placeholder.\n"},"getPointerOperands":{"range":[178,197],"code":"static SmallVector<Value *, 2> getPointerOperands(const Value &V) {\n  assert(isAddressExpression(V));\n  const Operator& Op = cast<Operator>(V);\n  switch (Op.getOpcode()) {\n  case Instruction::PHI: {\n    auto IncomingValues = cast<PHINode>(Op).incoming_values();\n    return SmallVector<Value *, 2>(IncomingValues.begin(),\n                                   IncomingValues.end());\n  }\n  case Instruction::BitCast:\n  case Instruction::AddrSpaceCast:\n  case Instruction::GetElementPtr:\n    return {Op.getOperand(0)};\n  default:\n    llvm_unreachable(\"Unexpected instruction type.\");\n  }\n}\n\n// If V is an unvisited generic address expression, appends V to PostorderStack\n// and marks it as visited.\n"},"updateAddressSpace":{"range":[483,508],"code":"Optional<unsigned> NVPTXInferAddressSpaces::updateAddressSpace(\n    const Value &V, const ValueToAddrSpaceMapTy &InferredAddrSpace) {\n  assert(InferredAddrSpace.count(&V));\n\n  // The new inferred address space equals the join of the address spaces\n  // of all its pointer operands.\n  unsigned NewAS = ADDRESS_SPACE_UNINITIALIZED;\n  for (Value *PtrOperand : getPointerOperands(V)) {\n    unsigned OperandAS;\n    if (InferredAddrSpace.count(PtrOperand))\n      OperandAS = InferredAddrSpace.lookup(PtrOperand);\n    else\n      OperandAS = PtrOperand->getType()->getPointerAddressSpace();\n    NewAS = joinAddressSpaces(NewAS, OperandAS);\n    // join(generic, *) = generic. So we can break if NewAS is already generic.\n    if (NewAS == AddressSpace::ADDRESS_SPACE_GENERIC)\n      break;\n  }\n\n  unsigned OldAS = InferredAddrSpace.lookup(&V);\n  assert(OldAS != AddressSpace::ADDRESS_SPACE_GENERIC);\n  if (OldAS == NewAS)\n    return None;\n  return NewAS;\n}\n\n"},"cloneValueWithNewAddressSpace":{"range":[381,406],"code":"cloneValueWithNewAddressSpace(Value *V, unsigned NewAddrSpace,\n                              const ValueToValueMapTy &ValueWithNewAddrSpace,\n                              SmallVectorImpl<const Use *> *UndefUsesToFix) {\n  // All values in Postorder are generic address expressions.\n  assert(isAddressExpression(*V) &&\n         V->getType()->getPointerAddressSpace() ==\n             AddressSpace::ADDRESS_SPACE_GENERIC);\n\n  if (Instruction *I = dyn_cast<Instruction>(V)) {\n    Value *NewV = cloneInstructionWithNewAddressSpace(\n        I, NewAddrSpace, ValueWithNewAddrSpace, UndefUsesToFix);\n    if (Instruction *NewI = dyn_cast<Instruction>(NewV)) {\n      if (NewI->getParent() == nullptr) {\n        NewI->insertBefore(I);\n        NewI->takeName(I);\n      }\n    }\n    return NewV;\n  }\n\n  return cloneConstantExprWithNewAddressSpace(\n      cast<ConstantExpr>(V), NewAddrSpace, ValueWithNewAddrSpace);\n}\n\n// Defines the join operation on the address space lattice (see the file header\n// comments).\n"},"NVPTXInferAddressSpaces":{"range":[123,159],"code":"  NVPTXInferAddressSpaces() : FunctionPass(ID) {}\n\n  bool runOnFunction(Function &F) override;\n\nprivate:\n  // Returns the new address space of V if updated; otherwise, returns None.\n  Optional<unsigned>\n  updateAddressSpace(const Value &V,\n                     const ValueToAddrSpaceMapTy &InferredAddrSpace);\n\n  // Tries to infer the specific address space of each address expression in\n  // Postorder.\n  void inferAddressSpaces(const std::vector<Value *> &Postorder,\n                          ValueToAddrSpaceMapTy *InferredAddrSpace);\n\n  // Changes the generic address expressions in function F to point to specific\n  // address spaces if InferredAddrSpace says so. Postorder is the postorder of\n  // all generic address expressions in the use-def graph of function F.\n  bool\n  rewriteWithNewAddressSpaces(const std::vector<Value *> &Postorder,\n                              const ValueToAddrSpaceMapTy &InferredAddrSpace,\n                              Function *F);\n};\n} // end anonymous namespace\n\nchar NVPTXInferAddressSpaces::ID = 0;\n\nnamespace llvm {\nvoid initializeNVPTXInferAddressSpacesPass(PassRegistry &);\n}\nINITIALIZE_PASS(NVPTXInferAddressSpaces, \"nvptx-infer-addrspace\",\n                \"Infer address spaces\",\n                false, false)\n\n// Returns true if V is an address expression.\n// TODO: Currently, we consider only phi, bitcast, addrspacecast, and\n// getelementptr operators.\n"}}},"NVPTXFavorNonGenericAddrSpaces.cpp":{"path":"NVPTXFavorNonGenericAddrSpaces.cpp","size":11234,"lines":289,"functions":{"NVPTXFavorNonGenericAddrSpaces":{"range":[87,125],"code":"  NVPTXFavorNonGenericAddrSpaces() : FunctionPass(ID) {}\n  bool runOnFunction(Function &F) override;\n\nprivate:\n  /// Optimizes load/store instructions. Idx is the index of the pointer operand\n  /// (0 for load, and 1 for store). Returns true if it changes anything.\n  bool optimizeMemoryInstruction(Instruction *I, unsigned Idx);\n  /// Recursively traces into a GEP's pointer operand or a bitcast's source to\n  /// find an eliminable addrspacecast, and hoists that addrspacecast to the\n  /// outermost level. For example, this function transforms\n  ///   bitcast(gep(gep(addrspacecast(X))))\n  /// to\n  ///   addrspacecast(bitcast(gep(gep(X)))).\n  ///\n  /// This reordering exposes to optimizeMemoryInstruction more\n  /// optimization opportunities on loads and stores.\n  ///\n  /// If this function successfully hoists an eliminable addrspacecast or V is\n  /// already such an addrspacecast, it returns the transformed value (which is\n  /// guaranteed to be an addrspacecast); otherwise, it returns nullptr.\n  Value *hoistAddrSpaceCastFrom(Value *V, int Depth = 0);\n  /// Helper function for GEPs.\n  Value *hoistAddrSpaceCastFromGEP(GEPOperator *GEP, int Depth);\n  /// Helper function for bitcasts.\n  Value *hoistAddrSpaceCastFromBitCast(BitCastOperator *BC, int Depth);\n};\n}\n\nchar NVPTXFavorNonGenericAddrSpaces::ID = 0;\n\nnamespace llvm {\nvoid initializeNVPTXFavorNonGenericAddrSpacesPass(PassRegistry &);\n}\nINITIALIZE_PASS(NVPTXFavorNonGenericAddrSpaces, \"nvptx-favor-non-generic\",\n                \"Remove unnecessary non-generic-to-generic addrspacecasts\",\n                false, false)\n\n// Decides whether V is an addrspacecast and shortcutting V in load/store is\n// valid and beneficial.\n"},"hoistAddrSpaceCastFromGEP":{"range":[148,185],"code":"Value *NVPTXFavorNonGenericAddrSpaces::hoistAddrSpaceCastFromGEP(\n    GEPOperator *GEP, int Depth) {\n  Value *NewOperand =\n      hoistAddrSpaceCastFrom(GEP->getPointerOperand(), Depth + 1);\n  if (NewOperand == nullptr)\n    return nullptr;\n\n  // hoistAddrSpaceCastFrom returns an eliminable addrspacecast or nullptr.\n  assert(isEliminableAddrSpaceCast(NewOperand));\n  Operator *Cast = cast<Operator>(NewOperand);\n\n  SmallVector<Value *, 8> Indices(GEP->idx_begin(), GEP->idx_end());\n  Value *NewASC;\n  if (Instruction *GEPI = dyn_cast<Instruction>(GEP)) {\n    // GEP = gep (addrspacecast X), indices\n    // =>\n    // NewGEP = gep X, indices\n    // NewASC = addrspacecast NewGEP\n    GetElementPtrInst *NewGEP = GetElementPtrInst::Create(\n        GEP->getSourceElementType(), Cast->getOperand(0), Indices,\n        \"\", GEPI);\n    NewGEP->setIsInBounds(GEP->isInBounds());\n    NewGEP->takeName(GEP);\n    NewASC = new AddrSpaceCastInst(NewGEP, GEP->getType(), \"\", GEPI);\n    // Without RAUWing GEP, the compiler would visit GEP again and emit\n    // redundant instructions. This is exercised in test @rauw in\n    // access-non-generic.ll.\n    GEP->replaceAllUsesWith(NewASC);\n  } else {\n    // GEP is a constant expression.\n    Constant *NewGEP = ConstantExpr::getGetElementPtr(\n        GEP->getSourceElementType(), cast<Constant>(Cast->getOperand(0)),\n        Indices, GEP->isInBounds());\n    NewASC = ConstantExpr::getAddrSpaceCast(NewGEP, GEP->getType());\n  }\n  return NewASC;\n}\n\n"},"runOnFunction":{"range":[268,286],"code":"bool NVPTXFavorNonGenericAddrSpaces::runOnFunction(Function &F) {\n  if (DisableFavorNonGeneric || skipFunction(F))\n    return false;\n\n  bool Changed = false;\n  for (BasicBlock &B : F) {\n    for (Instruction &I : B) {\n      if (isa<LoadInst>(I)) {\n        // V = load P\n        Changed |= optimizeMemoryInstruction(&I, 0);\n      } else if (isa<StoreInst>(I)) {\n        // store V, P\n        Changed |= optimizeMemoryInstruction(&I, 1);\n      }\n    }\n  }\n  return Changed;\n}\n\n"},"isEliminableAddrSpaceCast":{"range":[126,147],"code":"static bool isEliminableAddrSpaceCast(Value *V) {\n  // Returns false if V is not even an addrspacecast.\n  Operator *Cast = dyn_cast<Operator>(V);\n  if (Cast == nullptr || Cast->getOpcode() != Instruction::AddrSpaceCast)\n    return false;\n\n  Value *Src = Cast->getOperand(0);\n  PointerType *SrcTy = cast<PointerType>(Src->getType());\n  PointerType *DestTy = cast<PointerType>(Cast->getType());\n  // TODO: For now, we only handle the case where the addrspacecast only changes\n  // the address space but not the type. If the type also changes, we could\n  // still get rid of the addrspacecast by adding an extra bitcast, but we\n  // rarely see such scenarios.\n  if (SrcTy->getElementType() != DestTy->getElementType())\n    return false;\n\n  // Checks whether the addrspacecast is from a non-generic address space to the\n  // generic address space.\n  return (SrcTy->getAddressSpace() != AddressSpace::ADDRESS_SPACE_GENERIC &&\n          DestTy->getAddressSpace() == AddressSpace::ADDRESS_SPACE_GENERIC);\n}\n\n"},"createNVPTXFavorNonGenericAddrSpacesPass":{"range":[287,290],"code":"FunctionPass *llvm::createNVPTXFavorNonGenericAddrSpacesPass() {\n  return new NVPTXFavorNonGenericAddrSpaces();\n}\n"},"hoistAddrSpaceCastFrom":{"range":[223,245],"code":"Value *NVPTXFavorNonGenericAddrSpaces::hoistAddrSpaceCastFrom(Value *V,\n                                                              int Depth) {\n  // Returns V if V is already an eliminable addrspacecast.\n  if (isEliminableAddrSpaceCast(V))\n    return V;\n\n  // Limit the depth to prevent this recursive function from running too long.\n  const int MaxDepth = 20;\n  if (Depth >= MaxDepth)\n    return nullptr;\n\n  // If V is a GEP or bitcast, hoist the addrspacecast if any from its pointer\n  // operand. This enables optimizeMemoryInstruction to shortcut addrspacecasts\n  // that are not directly used by the load/store.\n  if (GEPOperator *GEP = dyn_cast<GEPOperator>(V))\n    return hoistAddrSpaceCastFromGEP(GEP, Depth);\n\n  if (BitCastOperator *BC = dyn_cast<BitCastOperator>(V))\n    return hoistAddrSpaceCastFromBitCast(BC, Depth);\n\n  return nullptr;\n}\n\n"},"optimizeMemoryInstruction":{"range":[246,267],"code":"bool NVPTXFavorNonGenericAddrSpaces::optimizeMemoryInstruction(Instruction *MI,\n                                                               unsigned Idx) {\n  Value *NewOperand = hoistAddrSpaceCastFrom(MI->getOperand(Idx));\n  if (NewOperand == nullptr)\n    return false;\n\n  // load/store (addrspacecast X) => load/store X if shortcutting the\n  // addrspacecast is valid and can improve performance.\n  //\n  // e.g.,\n  // %1 = addrspacecast float addrspace(3)* %0 to float*\n  // %2 = load float* %1\n  // ->\n  // %2 = load float addrspace(3)* %0\n  //\n  // Note: the addrspacecast can also be a constant expression.\n  assert(isEliminableAddrSpaceCast(NewOperand));\n  Operator *ASC = dyn_cast<Operator>(NewOperand);\n  MI->setOperand(Idx, ASC->getOperand(0));\n  return true;\n}\n\n"},"hoistAddrSpaceCastFromBitCast":{"range":[186,222],"code":"Value *NVPTXFavorNonGenericAddrSpaces::hoistAddrSpaceCastFromBitCast(\n    BitCastOperator *BC, int Depth) {\n  Value *NewOperand = hoistAddrSpaceCastFrom(BC->getOperand(0), Depth + 1);\n  if (NewOperand == nullptr)\n    return nullptr;\n\n  // hoistAddrSpaceCastFrom returns an eliminable addrspacecast or nullptr.\n  assert(isEliminableAddrSpaceCast(NewOperand));\n  Operator *Cast = cast<Operator>(NewOperand);\n\n  // Cast  = addrspacecast Src\n  // BC    = bitcast Cast\n  //   =>\n  // Cast' = bitcast Src\n  // BC'   = addrspacecast Cast'\n  Value *Src = Cast->getOperand(0);\n  Type *TypeOfNewCast =\n      PointerType::get(BC->getType()->getPointerElementType(),\n                       Src->getType()->getPointerAddressSpace());\n  Value *NewBC;\n  if (BitCastInst *BCI = dyn_cast<BitCastInst>(BC)) {\n    Value *NewCast = new BitCastInst(Src, TypeOfNewCast, \"\", BCI);\n    NewBC = new AddrSpaceCastInst(NewCast, BC->getType(), \"\", BCI);\n    NewBC->takeName(BC);\n    // Without RAUWing BC, the compiler would visit BC again and emit\n    // redundant instructions. This is exercised in test @rauw in\n    // access-non-generic.ll.\n    BC->replaceAllUsesWith(NewBC);\n  } else {\n    // BC is a constant expression.\n    Constant *NewCast =\n        ConstantExpr::getBitCast(cast<Constant>(Src), TypeOfNewCast);\n    NewBC = ConstantExpr::getAddrSpaceCast(NewCast, BC->getType());\n  }\n  return NewBC;\n}\n\n"}}},"NVPTXISelDAGToDAG.cpp":{"path":"NVPTXISelDAGToDAG.cpp","size":159268,"lines":5250,"functions":{"tryStore":{"range":[2118,2350],"code":"bool NVPTXDAGToDAGISel::tryStore(SDNode *N) {\n  SDLoc dl(N);\n  StoreSDNode *ST = cast<StoreSDNode>(N);\n  EVT StoreVT = ST->getMemoryVT();\n  SDNode *NVPTXST = nullptr;\n\n  // do not support pre/post inc/dec\n  if (ST->isIndexed())\n    return false;\n\n  if (!StoreVT.isSimple())\n    return false;\n\n  // Address Space Setting\n  unsigned int codeAddrSpace = getCodeAddrSpace(ST);\n\n  // Volatile Setting\n  // - .volatile is only availalble for .global and .shared\n  bool isVolatile = ST->isVolatile();\n  if (codeAddrSpace != NVPTX::PTXLdStInstCode::GLOBAL &&\n      codeAddrSpace != NVPTX::PTXLdStInstCode::SHARED &&\n      codeAddrSpace != NVPTX::PTXLdStInstCode::GENERIC)\n    isVolatile = false;\n\n  // Vector Setting\n  MVT SimpleVT = StoreVT.getSimpleVT();\n  unsigned vecType = NVPTX::PTXLdStInstCode::Scalar;\n  if (SimpleVT.isVector()) {\n    unsigned num = SimpleVT.getVectorNumElements();\n    if (num == 2)\n      vecType = NVPTX::PTXLdStInstCode::V2;\n    else if (num == 4)\n      vecType = NVPTX::PTXLdStInstCode::V4;\n    else\n      return false;\n  }\n\n  // Type Setting: toType + toTypeWidth\n  // - for integer type, always use 'u'\n  //\n  MVT ScalarVT = SimpleVT.getScalarType();\n  unsigned toTypeWidth = ScalarVT.getSizeInBits();\n  unsigned int toType;\n  if (ScalarVT.isFloatingPoint())\n    toType = NVPTX::PTXLdStInstCode::Float;\n  else\n    toType = NVPTX::PTXLdStInstCode::Unsigned;\n\n  // Create the machine instruction DAG\n  SDValue Chain = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  SDValue N2 = N->getOperand(2);\n  SDValue Addr;\n  SDValue Offset, Base;\n  unsigned Opcode;\n  MVT::SimpleValueType SourceVT = N1.getNode()->getSimpleValueType(0).SimpleTy;\n\n  if (SelectDirectAddr(N2, Addr)) {\n    switch (SourceVT) {\n    case MVT::i8:\n      Opcode = NVPTX::ST_i8_avar;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::ST_i16_avar;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::ST_i32_avar;\n      break;\n    case MVT::i64:\n      Opcode = NVPTX::ST_i64_avar;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::ST_f32_avar;\n      break;\n    case MVT::f64:\n      Opcode = NVPTX::ST_f64_avar;\n      break;\n    default:\n      return false;\n    }\n    SDValue Ops[] = { N1, getI32Imm(isVolatile, dl),\n                      getI32Imm(codeAddrSpace, dl), getI32Imm(vecType, dl),\n                      getI32Imm(toType, dl), getI32Imm(toTypeWidth, dl), Addr,\n                      Chain };\n    NVPTXST = CurDAG->getMachineNode(Opcode, dl, MVT::Other, Ops);\n  } else if (TM.is64Bit() ? SelectADDRsi64(N2.getNode(), N2, Base, Offset)\n                          : SelectADDRsi(N2.getNode(), N2, Base, Offset)) {\n    switch (SourceVT) {\n    case MVT::i8:\n      Opcode = NVPTX::ST_i8_asi;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::ST_i16_asi;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::ST_i32_asi;\n      break;\n    case MVT::i64:\n      Opcode = NVPTX::ST_i64_asi;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::ST_f32_asi;\n      break;\n    case MVT::f64:\n      Opcode = NVPTX::ST_f64_asi;\n      break;\n    default:\n      return false;\n    }\n    SDValue Ops[] = { N1, getI32Imm(isVolatile, dl),\n                      getI32Imm(codeAddrSpace, dl), getI32Imm(vecType, dl),\n                      getI32Imm(toType, dl), getI32Imm(toTypeWidth, dl), Base,\n                      Offset, Chain };\n    NVPTXST = CurDAG->getMachineNode(Opcode, dl, MVT::Other, Ops);\n  } else if (TM.is64Bit() ? SelectADDRri64(N2.getNode(), N2, Base, Offset)\n                          : SelectADDRri(N2.getNode(), N2, Base, Offset)) {\n    if (TM.is64Bit()) {\n      switch (SourceVT) {\n      case MVT::i8:\n        Opcode = NVPTX::ST_i8_ari_64;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::ST_i16_ari_64;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::ST_i32_ari_64;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::ST_i64_ari_64;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::ST_f32_ari_64;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::ST_f64_ari_64;\n        break;\n      default:\n        return false;\n      }\n    } else {\n      switch (SourceVT) {\n      case MVT::i8:\n        Opcode = NVPTX::ST_i8_ari;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::ST_i16_ari;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::ST_i32_ari;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::ST_i64_ari;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::ST_f32_ari;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::ST_f64_ari;\n        break;\n      default:\n        return false;\n      }\n    }\n    SDValue Ops[] = { N1, getI32Imm(isVolatile, dl),\n                      getI32Imm(codeAddrSpace, dl), getI32Imm(vecType, dl),\n                      getI32Imm(toType, dl), getI32Imm(toTypeWidth, dl), Base,\n                      Offset, Chain };\n    NVPTXST = CurDAG->getMachineNode(Opcode, dl, MVT::Other, Ops);\n  } else {\n    if (TM.is64Bit()) {\n      switch (SourceVT) {\n      case MVT::i8:\n        Opcode = NVPTX::ST_i8_areg_64;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::ST_i16_areg_64;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::ST_i32_areg_64;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::ST_i64_areg_64;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::ST_f32_areg_64;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::ST_f64_areg_64;\n        break;\n      default:\n        return false;\n      }\n    } else {\n      switch (SourceVT) {\n      case MVT::i8:\n        Opcode = NVPTX::ST_i8_areg;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::ST_i16_areg;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::ST_i32_areg;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::ST_i64_areg;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::ST_f32_areg;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::ST_f64_areg;\n        break;\n      default:\n        return false;\n      }\n    }\n    SDValue Ops[] = { N1, getI32Imm(isVolatile, dl),\n                      getI32Imm(codeAddrSpace, dl), getI32Imm(vecType, dl),\n                      getI32Imm(toType, dl), getI32Imm(toTypeWidth, dl), N2,\n                      Chain };\n    NVPTXST = CurDAG->getMachineNode(Opcode, dl, MVT::Other, Ops);\n  }\n\n  if (!NVPTXST)\n    return false;\n\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = cast<MemSDNode>(N)->getMemOperand();\n  cast<MachineSDNode>(NVPTXST)->setMemRefs(MemRefs0, MemRefs0 + 1);\n  ReplaceNode(N, NVPTXST);\n  return true;\n}\n\n"},"runOnMachineFunction":{"range":[59,63],"code":"bool NVPTXDAGToDAGISel::runOnMachineFunction(MachineFunction &MF) {\n    Subtarget = &static_cast<const NVPTXSubtarget &>(MF.getSubtarget());\n    return SelectionDAGISel::runOnMachineFunction(MF);\n}\n\n"},"createNVPTXISelDag":{"range":[48,52],"code":"FunctionPass *llvm::createNVPTXISelDag(NVPTXTargetMachine &TM,\n                                       llvm::CodeGenOpt::Level OptLevel) {\n  return new NVPTXDAGToDAGISel(TM, OptLevel);\n}\n\n"},"GetConvertOpcode":{"range":[5200,5251],"code":"unsigned NVPTXDAGToDAGISel::GetConvertOpcode(MVT DestTy, MVT SrcTy,\n                                             bool IsSigned) {\n  switch (SrcTy.SimpleTy) {\n  default:\n    llvm_unreachable(\"Unhandled source type\");\n  case MVT::i8:\n    switch (DestTy.SimpleTy) {\n    default:\n      llvm_unreachable(\"Unhandled dest type\");\n    case MVT::i16:\n      return IsSigned ? NVPTX::CVT_s16_s8 : NVPTX::CVT_u16_u8;\n    case MVT::i32:\n      return IsSigned ? NVPTX::CVT_s32_s8 : NVPTX::CVT_u32_u8;\n    case MVT::i64:\n      return IsSigned ? NVPTX::CVT_s64_s8 : NVPTX::CVT_u64_u8;\n    }\n  case MVT::i16:\n    switch (DestTy.SimpleTy) {\n    default:\n      llvm_unreachable(\"Unhandled dest type\");\n    case MVT::i8:\n      return IsSigned ? NVPTX::CVT_s8_s16 : NVPTX::CVT_u8_u16;\n    case MVT::i32:\n      return IsSigned ? NVPTX::CVT_s32_s16 : NVPTX::CVT_u32_u16;\n    case MVT::i64:\n      return IsSigned ? NVPTX::CVT_s64_s16 : NVPTX::CVT_u64_u16;\n    }\n  case MVT::i32:\n    switch (DestTy.SimpleTy) {\n    default:\n      llvm_unreachable(\"Unhandled dest type\");\n    case MVT::i8:\n      return IsSigned ? NVPTX::CVT_s8_s32 : NVPTX::CVT_u8_u32;\n    case MVT::i16:\n      return IsSigned ? NVPTX::CVT_s16_s32 : NVPTX::CVT_u16_u32;\n    case MVT::i64:\n      return IsSigned ? NVPTX::CVT_s64_s32 : NVPTX::CVT_u64_u32;\n    }\n  case MVT::i64:\n    switch (DestTy.SimpleTy) {\n    default:\n      llvm_unreachable(\"Unhandled dest type\");\n    case MVT::i8:\n      return IsSigned ? NVPTX::CVT_s8_s64 : NVPTX::CVT_u8_u64;\n    case MVT::i16:\n      return IsSigned ? NVPTX::CVT_s16_s64 : NVPTX::CVT_u16_u64;\n    case MVT::i32:\n      return IsSigned ? NVPTX::CVT_s32_s64 : NVPTX::CVT_u32_u64;\n    }\n  }\n}\n"},"getDivF32Level":{"range":[64,76],"code":"int NVPTXDAGToDAGISel::getDivF32Level() const {\n  if (UsePrecDivF32.getNumOccurrences() > 0) {\n    // If nvptx-prec-div32=N is used on the command-line, always honor it\n    return UsePrecDivF32;\n  } else {\n    // Otherwise, use div.approx if fast math is enabled\n    if (TM.Options.UnsafeFPMath)\n      return 0;\n    else\n      return 2;\n  }\n}\n\n"},"tryLoadVector":{"range":[914,1299],"code":"bool NVPTXDAGToDAGISel::tryLoadVector(SDNode *N) {\n\n  SDValue Chain = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDValue Addr, Offset, Base;\n  unsigned Opcode;\n  SDLoc DL(N);\n  SDNode *LD;\n  MemSDNode *MemSD = cast<MemSDNode>(N);\n  EVT LoadedVT = MemSD->getMemoryVT();\n\n  if (!LoadedVT.isSimple())\n    return false;\n\n  // Address Space Setting\n  unsigned int CodeAddrSpace = getCodeAddrSpace(MemSD);\n\n  if (canLowerToLDG(MemSD, *Subtarget, CodeAddrSpace, MF)) {\n    return tryLDGLDU(N);\n  }\n\n  // Volatile Setting\n  // - .volatile is only availalble for .global and .shared\n  bool IsVolatile = MemSD->isVolatile();\n  if (CodeAddrSpace != NVPTX::PTXLdStInstCode::GLOBAL &&\n      CodeAddrSpace != NVPTX::PTXLdStInstCode::SHARED &&\n      CodeAddrSpace != NVPTX::PTXLdStInstCode::GENERIC)\n    IsVolatile = false;\n\n  // Vector Setting\n  MVT SimpleVT = LoadedVT.getSimpleVT();\n\n  // Type Setting: fromType + fromTypeWidth\n  //\n  // Sign   : ISD::SEXTLOAD\n  // Unsign : ISD::ZEXTLOAD, ISD::NON_EXTLOAD or ISD::EXTLOAD and the\n  //          type is integer\n  // Float  : ISD::NON_EXTLOAD or ISD::EXTLOAD and the type is float\n  MVT ScalarVT = SimpleVT.getScalarType();\n  // Read at least 8 bits (predicates are stored as 8-bit values)\n  unsigned FromTypeWidth = std::max(8U, ScalarVT.getSizeInBits());\n  unsigned int FromType;\n  // The last operand holds the original LoadSDNode::getExtensionType() value\n  unsigned ExtensionType = cast<ConstantSDNode>(\n      N->getOperand(N->getNumOperands() - 1))->getZExtValue();\n  if (ExtensionType == ISD::SEXTLOAD)\n    FromType = NVPTX::PTXLdStInstCode::Signed;\n  else if (ScalarVT.isFloatingPoint())\n    FromType = NVPTX::PTXLdStInstCode::Float;\n  else\n    FromType = NVPTX::PTXLdStInstCode::Unsigned;\n\n  unsigned VecType;\n\n  switch (N->getOpcode()) {\n  case NVPTXISD::LoadV2:\n    VecType = NVPTX::PTXLdStInstCode::V2;\n    break;\n  case NVPTXISD::LoadV4:\n    VecType = NVPTX::PTXLdStInstCode::V4;\n    break;\n  default:\n    return false;\n  }\n\n  EVT EltVT = N->getValueType(0);\n\n  if (SelectDirectAddr(Op1, Addr)) {\n    switch (N->getOpcode()) {\n    default:\n      return false;\n    case NVPTXISD::LoadV2:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::LDV_i8_v2_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LDV_i16_v2_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LDV_i32_v2_avar;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::LDV_i64_v2_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LDV_f32_v2_avar;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::LDV_f64_v2_avar;\n        break;\n      }\n      break;\n    case NVPTXISD::LoadV4:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::LDV_i8_v4_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LDV_i16_v4_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LDV_i32_v4_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LDV_f32_v4_avar;\n        break;\n      }\n      break;\n    }\n\n    SDValue Ops[] = { getI32Imm(IsVolatile, DL), getI32Imm(CodeAddrSpace, DL),\n                      getI32Imm(VecType, DL), getI32Imm(FromType, DL),\n                      getI32Imm(FromTypeWidth, DL), Addr, Chain };\n    LD = CurDAG->getMachineNode(Opcode, DL, N->getVTList(), Ops);\n  } else if (TM.is64Bit() ? SelectADDRsi64(Op1.getNode(), Op1, Base, Offset)\n                          : SelectADDRsi(Op1.getNode(), Op1, Base, Offset)) {\n    switch (N->getOpcode()) {\n    default:\n      return false;\n    case NVPTXISD::LoadV2:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::LDV_i8_v2_asi;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LDV_i16_v2_asi;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LDV_i32_v2_asi;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::LDV_i64_v2_asi;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LDV_f32_v2_asi;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::LDV_f64_v2_asi;\n        break;\n      }\n      break;\n    case NVPTXISD::LoadV4:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::LDV_i8_v4_asi;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LDV_i16_v4_asi;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LDV_i32_v4_asi;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LDV_f32_v4_asi;\n        break;\n      }\n      break;\n    }\n\n    SDValue Ops[] = { getI32Imm(IsVolatile, DL), getI32Imm(CodeAddrSpace, DL),\n                      getI32Imm(VecType, DL), getI32Imm(FromType, DL),\n                      getI32Imm(FromTypeWidth, DL), Base, Offset, Chain };\n    LD = CurDAG->getMachineNode(Opcode, DL, N->getVTList(), Ops);\n  } else if (TM.is64Bit() ? SelectADDRri64(Op1.getNode(), Op1, Base, Offset)\n                          : SelectADDRri(Op1.getNode(), Op1, Base, Offset)) {\n    if (TM.is64Bit()) {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::LoadV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v2_ari_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v2_ari_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v2_ari_64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::LDV_i64_v2_ari_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v2_ari_64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::LDV_f64_v2_ari_64;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v4_ari_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v4_ari_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v4_ari_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v4_ari_64;\n          break;\n        }\n        break;\n      }\n    } else {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::LoadV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v2_ari;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v2_ari;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v2_ari;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::LDV_i64_v2_ari;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v2_ari;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::LDV_f64_v2_ari;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v4_ari;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v4_ari;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v4_ari;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v4_ari;\n          break;\n        }\n        break;\n      }\n    }\n\n    SDValue Ops[] = { getI32Imm(IsVolatile, DL), getI32Imm(CodeAddrSpace, DL),\n                      getI32Imm(VecType, DL), getI32Imm(FromType, DL),\n                      getI32Imm(FromTypeWidth, DL), Base, Offset, Chain };\n\n    LD = CurDAG->getMachineNode(Opcode, DL, N->getVTList(), Ops);\n  } else {\n    if (TM.is64Bit()) {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::LoadV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v2_areg_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v2_areg_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v2_areg_64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::LDV_i64_v2_areg_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v2_areg_64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::LDV_f64_v2_areg_64;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v4_areg_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v4_areg_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v4_areg_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v4_areg_64;\n          break;\n        }\n        break;\n      }\n    } else {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::LoadV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v2_areg;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v2_areg;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v2_areg;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::LDV_i64_v2_areg;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v2_areg;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::LDV_f64_v2_areg;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::LDV_i8_v4_areg;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::LDV_i16_v4_areg;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::LDV_i32_v4_areg;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::LDV_f32_v4_areg;\n          break;\n        }\n        break;\n      }\n    }\n\n    SDValue Ops[] = { getI32Imm(IsVolatile, DL), getI32Imm(CodeAddrSpace, DL),\n                      getI32Imm(VecType, DL), getI32Imm(FromType, DL),\n                      getI32Imm(FromTypeWidth, DL), Op1, Chain };\n    LD = CurDAG->getMachineNode(Opcode, DL, N->getVTList(), Ops);\n  }\n\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = cast<MemSDNode>(N)->getMemOperand();\n  cast<MachineSDNode>(LD)->setMemRefs(MemRefs0, MemRefs0 + 1);\n\n  ReplaceNode(N, LD);\n  return true;\n}\n\n"},"trySurfaceIntrinsic":{"range":[3658,4860],"code":"bool NVPTXDAGToDAGISel::trySurfaceIntrinsic(SDNode *N) {\n  SDValue Chain = N->getOperand(0);\n  SDValue TexHandle = N->getOperand(1);\n  unsigned Opc = 0;\n  SmallVector<SDValue, 8> Ops;\n  switch (N->getOpcode()) {\n  default: return false;\n  case NVPTXISD::Suld1DI8Clamp:\n    Opc = NVPTX::SULD_1D_I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI16Clamp:\n    Opc = NVPTX::SULD_1D_I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI32Clamp:\n    Opc = NVPTX::SULD_1D_I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI64Clamp:\n    Opc = NVPTX::SULD_1D_I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I8Clamp:\n    Opc = NVPTX::SULD_1D_V2I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I16Clamp:\n    Opc = NVPTX::SULD_1D_V2I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I32Clamp:\n    Opc = NVPTX::SULD_1D_V2I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I64Clamp:\n    Opc = NVPTX::SULD_1D_V2I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I8Clamp:\n    Opc = NVPTX::SULD_1D_V4I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I16Clamp:\n    Opc = NVPTX::SULD_1D_V4I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I32Clamp:\n    Opc = NVPTX::SULD_1D_V4I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI8Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI16Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI32Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI64Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I8Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I16Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I32Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I64Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I8Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I16Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I32Clamp:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI8Clamp:\n    Opc = NVPTX::SULD_2D_I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI16Clamp:\n    Opc = NVPTX::SULD_2D_I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI32Clamp:\n    Opc = NVPTX::SULD_2D_I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI64Clamp:\n    Opc = NVPTX::SULD_2D_I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I8Clamp:\n    Opc = NVPTX::SULD_2D_V2I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I16Clamp:\n    Opc = NVPTX::SULD_2D_V2I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I32Clamp:\n    Opc = NVPTX::SULD_2D_V2I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I64Clamp:\n    Opc = NVPTX::SULD_2D_V2I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I8Clamp:\n    Opc = NVPTX::SULD_2D_V4I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I16Clamp:\n    Opc = NVPTX::SULD_2D_V4I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I32Clamp:\n    Opc = NVPTX::SULD_2D_V4I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI8Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI16Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI32Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI64Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I8Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I16Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I32Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I64Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I8Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I16Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I32Clamp:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI8Clamp:\n    Opc = NVPTX::SULD_3D_I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI16Clamp:\n    Opc = NVPTX::SULD_3D_I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI32Clamp:\n    Opc = NVPTX::SULD_3D_I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI64Clamp:\n    Opc = NVPTX::SULD_3D_I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I8Clamp:\n    Opc = NVPTX::SULD_3D_V2I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I16Clamp:\n    Opc = NVPTX::SULD_3D_V2I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I32Clamp:\n    Opc = NVPTX::SULD_3D_V2I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I64Clamp:\n    Opc = NVPTX::SULD_3D_V2I64_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I8Clamp:\n    Opc = NVPTX::SULD_3D_V4I8_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I16Clamp:\n    Opc = NVPTX::SULD_3D_V4I16_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I32Clamp:\n    Opc = NVPTX::SULD_3D_V4I32_CLAMP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI8Trap:\n    Opc = NVPTX::SULD_1D_I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI16Trap:\n    Opc = NVPTX::SULD_1D_I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI32Trap:\n    Opc = NVPTX::SULD_1D_I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI64Trap:\n    Opc = NVPTX::SULD_1D_I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I8Trap:\n    Opc = NVPTX::SULD_1D_V2I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I16Trap:\n    Opc = NVPTX::SULD_1D_V2I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I32Trap:\n    Opc = NVPTX::SULD_1D_V2I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I64Trap:\n    Opc = NVPTX::SULD_1D_V2I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I8Trap:\n    Opc = NVPTX::SULD_1D_V4I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I16Trap:\n    Opc = NVPTX::SULD_1D_V4I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I32Trap:\n    Opc = NVPTX::SULD_1D_V4I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI8Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI16Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI32Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI64Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I8Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I16Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I32Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I64Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I8Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I16Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I32Trap:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI8Trap:\n    Opc = NVPTX::SULD_2D_I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI16Trap:\n    Opc = NVPTX::SULD_2D_I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI32Trap:\n    Opc = NVPTX::SULD_2D_I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI64Trap:\n    Opc = NVPTX::SULD_2D_I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I8Trap:\n    Opc = NVPTX::SULD_2D_V2I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I16Trap:\n    Opc = NVPTX::SULD_2D_V2I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I32Trap:\n    Opc = NVPTX::SULD_2D_V2I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I64Trap:\n    Opc = NVPTX::SULD_2D_V2I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I8Trap:\n    Opc = NVPTX::SULD_2D_V4I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I16Trap:\n    Opc = NVPTX::SULD_2D_V4I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I32Trap:\n    Opc = NVPTX::SULD_2D_V4I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI8Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI16Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI32Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI64Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I8Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I16Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I32Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I64Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I8Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I16Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I32Trap:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI8Trap:\n    Opc = NVPTX::SULD_3D_I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI16Trap:\n    Opc = NVPTX::SULD_3D_I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI32Trap:\n    Opc = NVPTX::SULD_3D_I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI64Trap:\n    Opc = NVPTX::SULD_3D_I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I8Trap:\n    Opc = NVPTX::SULD_3D_V2I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I16Trap:\n    Opc = NVPTX::SULD_3D_V2I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I32Trap:\n    Opc = NVPTX::SULD_3D_V2I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I64Trap:\n    Opc = NVPTX::SULD_3D_V2I64_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I8Trap:\n    Opc = NVPTX::SULD_3D_V4I8_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I16Trap:\n    Opc = NVPTX::SULD_3D_V4I16_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I32Trap:\n    Opc = NVPTX::SULD_3D_V4I32_TRAP;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI8Zero:\n    Opc = NVPTX::SULD_1D_I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI16Zero:\n    Opc = NVPTX::SULD_1D_I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI32Zero:\n    Opc = NVPTX::SULD_1D_I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DI64Zero:\n    Opc = NVPTX::SULD_1D_I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I8Zero:\n    Opc = NVPTX::SULD_1D_V2I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I16Zero:\n    Opc = NVPTX::SULD_1D_V2I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I32Zero:\n    Opc = NVPTX::SULD_1D_V2I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV2I64Zero:\n    Opc = NVPTX::SULD_1D_V2I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I8Zero:\n    Opc = NVPTX::SULD_1D_V4I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I16Zero:\n    Opc = NVPTX::SULD_1D_V4I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DV4I32Zero:\n    Opc = NVPTX::SULD_1D_V4I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI8Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI16Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI32Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayI64Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I8Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I16Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I32Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV2I64Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V2I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I8Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I16Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld1DArrayV4I32Zero:\n    Opc = NVPTX::SULD_1D_ARRAY_V4I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI8Zero:\n    Opc = NVPTX::SULD_2D_I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI16Zero:\n    Opc = NVPTX::SULD_2D_I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI32Zero:\n    Opc = NVPTX::SULD_2D_I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DI64Zero:\n    Opc = NVPTX::SULD_2D_I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I8Zero:\n    Opc = NVPTX::SULD_2D_V2I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I16Zero:\n    Opc = NVPTX::SULD_2D_V2I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I32Zero:\n    Opc = NVPTX::SULD_2D_V2I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV2I64Zero:\n    Opc = NVPTX::SULD_2D_V2I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I8Zero:\n    Opc = NVPTX::SULD_2D_V4I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I16Zero:\n    Opc = NVPTX::SULD_2D_V4I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DV4I32Zero:\n    Opc = NVPTX::SULD_2D_V4I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI8Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI16Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI32Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayI64Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I8Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I16Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I32Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV2I64Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V2I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I8Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I16Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld2DArrayV4I32Zero:\n    Opc = NVPTX::SULD_2D_ARRAY_V4I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI8Zero:\n    Opc = NVPTX::SULD_3D_I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI16Zero:\n    Opc = NVPTX::SULD_3D_I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI32Zero:\n    Opc = NVPTX::SULD_3D_I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DI64Zero:\n    Opc = NVPTX::SULD_3D_I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I8Zero:\n    Opc = NVPTX::SULD_3D_V2I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I16Zero:\n    Opc = NVPTX::SULD_3D_V2I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I32Zero:\n    Opc = NVPTX::SULD_3D_V2I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV2I64Zero:\n    Opc = NVPTX::SULD_3D_V2I64_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I8Zero:\n    Opc = NVPTX::SULD_3D_V4I8_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I16Zero:\n    Opc = NVPTX::SULD_3D_V4I16_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  case NVPTXISD::Suld3DV4I32Zero:\n    Opc = NVPTX::SULD_3D_V4I32_ZERO;\n    Ops.push_back(TexHandle);\n    Ops.push_back(N->getOperand(2));\n    Ops.push_back(N->getOperand(3));\n    Ops.push_back(N->getOperand(4));\n    Ops.push_back(Chain);\n    break;\n  }\n  ReplaceNode(N, CurDAG->getMachineNode(Opc, SDLoc(N), N->getVTList(), Ops));\n  return true;\n}\n\n\n/// SelectBFE - Look for instruction sequences that can be made more efficient\n/// by using the 'bfe' (bit-field extract) PTX instruction\n"},"tryStoreRetval":{"range":[2854,2977],"code":"bool NVPTXDAGToDAGISel::tryStoreRetval(SDNode *N) {\n  SDLoc DL(N);\n  SDValue Chain = N->getOperand(0);\n  SDValue Offset = N->getOperand(1);\n  unsigned OffsetVal = cast<ConstantSDNode>(Offset)->getZExtValue();\n  MemSDNode *Mem = cast<MemSDNode>(N);\n\n  // How many elements do we have?\n  unsigned NumElts = 1;\n  switch (N->getOpcode()) {\n  default:\n    return false;\n  case NVPTXISD::StoreRetval:\n    NumElts = 1;\n    break;\n  case NVPTXISD::StoreRetvalV2:\n    NumElts = 2;\n    break;\n  case NVPTXISD::StoreRetvalV4:\n    NumElts = 4;\n    break;\n  }\n\n  // Build vector of operands\n  SmallVector<SDValue, 6> Ops;\n  for (unsigned i = 0; i < NumElts; ++i)\n    Ops.push_back(N->getOperand(i + 2));\n  Ops.push_back(CurDAG->getTargetConstant(OffsetVal, DL, MVT::i32));\n  Ops.push_back(Chain);\n\n  // Determine target opcode\n  // If we have an i1, use an 8-bit store. The lowering code in\n  // NVPTXISelLowering will have already emitted an upcast.\n  unsigned Opcode = 0;\n  switch (NumElts) {\n  default:\n    return false;\n  case 1:\n    switch (Mem->getMemoryVT().getSimpleVT().SimpleTy) {\n    default:\n      return false;\n    case MVT::i1:\n      Opcode = NVPTX::StoreRetvalI8;\n      break;\n    case MVT::i8:\n      Opcode = NVPTX::StoreRetvalI8;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::StoreRetvalI16;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::StoreRetvalI32;\n      break;\n    case MVT::i64:\n      Opcode = NVPTX::StoreRetvalI64;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::StoreRetvalF32;\n      break;\n    case MVT::f64:\n      Opcode = NVPTX::StoreRetvalF64;\n      break;\n    }\n    break;\n  case 2:\n    switch (Mem->getMemoryVT().getSimpleVT().SimpleTy) {\n    default:\n      return false;\n    case MVT::i1:\n      Opcode = NVPTX::StoreRetvalV2I8;\n      break;\n    case MVT::i8:\n      Opcode = NVPTX::StoreRetvalV2I8;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::StoreRetvalV2I16;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::StoreRetvalV2I32;\n      break;\n    case MVT::i64:\n      Opcode = NVPTX::StoreRetvalV2I64;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::StoreRetvalV2F32;\n      break;\n    case MVT::f64:\n      Opcode = NVPTX::StoreRetvalV2F64;\n      break;\n    }\n    break;\n  case 4:\n    switch (Mem->getMemoryVT().getSimpleVT().SimpleTy) {\n    default:\n      return false;\n    case MVT::i1:\n      Opcode = NVPTX::StoreRetvalV4I8;\n      break;\n    case MVT::i8:\n      Opcode = NVPTX::StoreRetvalV4I8;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::StoreRetvalV4I16;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::StoreRetvalV4I32;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::StoreRetvalV4F32;\n      break;\n    }\n    break;\n  }\n\n  SDNode *Ret =\n      CurDAG->getMachineNode(Opcode, DL, MVT::Other, Ops);\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = cast<MemSDNode>(N)->getMemOperand();\n  cast<MachineSDNode>(Ret)->setMemRefs(MemRefs0, MemRefs0 + 1);\n\n  ReplaceNode(N, Ret);\n  return true;\n}\n\n"},"tryTextureIntrinsic":{"range":[3135,3657],"code":"bool NVPTXDAGToDAGISel::tryTextureIntrinsic(SDNode *N) {\n  SDValue Chain = N->getOperand(0);\n  unsigned Opc = 0;\n  SmallVector<SDValue, 8> Ops;\n\n  switch (N->getOpcode()) {\n  default: return false;\n  case NVPTXISD::Tex1DFloatS32:\n    Opc = NVPTX::TEX_1D_F32_S32;\n    break;\n  case NVPTXISD::Tex1DFloatFloat:\n    Opc = NVPTX::TEX_1D_F32_F32;\n    break;\n  case NVPTXISD::Tex1DFloatFloatLevel:\n    Opc = NVPTX::TEX_1D_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex1DFloatFloatGrad:\n    Opc = NVPTX::TEX_1D_F32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex1DS32S32:\n    Opc = NVPTX::TEX_1D_S32_S32;\n    break;\n  case NVPTXISD::Tex1DS32Float:\n    Opc = NVPTX::TEX_1D_S32_F32;\n    break;\n  case NVPTXISD::Tex1DS32FloatLevel:\n    Opc = NVPTX::TEX_1D_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex1DS32FloatGrad:\n    Opc = NVPTX::TEX_1D_S32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex1DU32S32:\n    Opc = NVPTX::TEX_1D_U32_S32;\n    break;\n  case NVPTXISD::Tex1DU32Float:\n    Opc = NVPTX::TEX_1D_U32_F32;\n    break;\n  case NVPTXISD::Tex1DU32FloatLevel:\n    Opc = NVPTX::TEX_1D_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex1DU32FloatGrad:\n    Opc = NVPTX::TEX_1D_U32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex1DArrayFloatS32:\n    Opc = NVPTX::TEX_1D_ARRAY_F32_S32;\n    break;\n  case NVPTXISD::Tex1DArrayFloatFloat:\n    Opc = NVPTX::TEX_1D_ARRAY_F32_F32;\n    break;\n  case NVPTXISD::Tex1DArrayFloatFloatLevel:\n    Opc = NVPTX::TEX_1D_ARRAY_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex1DArrayFloatFloatGrad:\n    Opc = NVPTX::TEX_1D_ARRAY_F32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex1DArrayS32S32:\n    Opc = NVPTX::TEX_1D_ARRAY_S32_S32;\n    break;\n  case NVPTXISD::Tex1DArrayS32Float:\n    Opc = NVPTX::TEX_1D_ARRAY_S32_F32;\n    break;\n  case NVPTXISD::Tex1DArrayS32FloatLevel:\n    Opc = NVPTX::TEX_1D_ARRAY_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex1DArrayS32FloatGrad:\n    Opc = NVPTX::TEX_1D_ARRAY_S32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex1DArrayU32S32:\n    Opc = NVPTX::TEX_1D_ARRAY_U32_S32;\n    break;\n  case NVPTXISD::Tex1DArrayU32Float:\n    Opc = NVPTX::TEX_1D_ARRAY_U32_F32;\n    break;\n  case NVPTXISD::Tex1DArrayU32FloatLevel:\n    Opc = NVPTX::TEX_1D_ARRAY_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex1DArrayU32FloatGrad:\n    Opc = NVPTX::TEX_1D_ARRAY_U32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex2DFloatS32:\n    Opc = NVPTX::TEX_2D_F32_S32;\n    break;\n  case NVPTXISD::Tex2DFloatFloat:\n    Opc = NVPTX::TEX_2D_F32_F32;\n    break;\n  case NVPTXISD::Tex2DFloatFloatLevel:\n    Opc = NVPTX::TEX_2D_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex2DFloatFloatGrad:\n    Opc = NVPTX::TEX_2D_F32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex2DS32S32:\n    Opc = NVPTX::TEX_2D_S32_S32;\n    break;\n  case NVPTXISD::Tex2DS32Float:\n    Opc = NVPTX::TEX_2D_S32_F32;\n    break;\n  case NVPTXISD::Tex2DS32FloatLevel:\n    Opc = NVPTX::TEX_2D_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex2DS32FloatGrad:\n    Opc = NVPTX::TEX_2D_S32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex2DU32S32:\n    Opc = NVPTX::TEX_2D_U32_S32;\n    break;\n  case NVPTXISD::Tex2DU32Float:\n    Opc = NVPTX::TEX_2D_U32_F32;\n    break;\n  case NVPTXISD::Tex2DU32FloatLevel:\n    Opc = NVPTX::TEX_2D_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex2DU32FloatGrad:\n    Opc = NVPTX::TEX_2D_U32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex2DArrayFloatS32:\n    Opc = NVPTX::TEX_2D_ARRAY_F32_S32;\n    break;\n  case NVPTXISD::Tex2DArrayFloatFloat:\n    Opc = NVPTX::TEX_2D_ARRAY_F32_F32;\n    break;\n  case NVPTXISD::Tex2DArrayFloatFloatLevel:\n    Opc = NVPTX::TEX_2D_ARRAY_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex2DArrayFloatFloatGrad:\n    Opc = NVPTX::TEX_2D_ARRAY_F32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex2DArrayS32S32:\n    Opc = NVPTX::TEX_2D_ARRAY_S32_S32;\n    break;\n  case NVPTXISD::Tex2DArrayS32Float:\n    Opc = NVPTX::TEX_2D_ARRAY_S32_F32;\n    break;\n  case NVPTXISD::Tex2DArrayS32FloatLevel:\n    Opc = NVPTX::TEX_2D_ARRAY_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex2DArrayS32FloatGrad:\n    Opc = NVPTX::TEX_2D_ARRAY_S32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex2DArrayU32S32:\n    Opc = NVPTX::TEX_2D_ARRAY_U32_S32;\n    break;\n  case NVPTXISD::Tex2DArrayU32Float:\n    Opc = NVPTX::TEX_2D_ARRAY_U32_F32;\n    break;\n  case NVPTXISD::Tex2DArrayU32FloatLevel:\n    Opc = NVPTX::TEX_2D_ARRAY_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex2DArrayU32FloatGrad:\n    Opc = NVPTX::TEX_2D_ARRAY_U32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex3DFloatS32:\n    Opc = NVPTX::TEX_3D_F32_S32;\n    break;\n  case NVPTXISD::Tex3DFloatFloat:\n    Opc = NVPTX::TEX_3D_F32_F32;\n    break;\n  case NVPTXISD::Tex3DFloatFloatLevel:\n    Opc = NVPTX::TEX_3D_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex3DFloatFloatGrad:\n    Opc = NVPTX::TEX_3D_F32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex3DS32S32:\n    Opc = NVPTX::TEX_3D_S32_S32;\n    break;\n  case NVPTXISD::Tex3DS32Float:\n    Opc = NVPTX::TEX_3D_S32_F32;\n    break;\n  case NVPTXISD::Tex3DS32FloatLevel:\n    Opc = NVPTX::TEX_3D_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex3DS32FloatGrad:\n    Opc = NVPTX::TEX_3D_S32_F32_GRAD;\n    break;\n  case NVPTXISD::Tex3DU32S32:\n    Opc = NVPTX::TEX_3D_U32_S32;\n    break;\n  case NVPTXISD::Tex3DU32Float:\n    Opc = NVPTX::TEX_3D_U32_F32;\n    break;\n  case NVPTXISD::Tex3DU32FloatLevel:\n    Opc = NVPTX::TEX_3D_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tex3DU32FloatGrad:\n    Opc = NVPTX::TEX_3D_U32_F32_GRAD;\n    break;\n  case NVPTXISD::TexCubeFloatFloat:\n    Opc = NVPTX::TEX_CUBE_F32_F32;\n    break;\n  case NVPTXISD::TexCubeFloatFloatLevel:\n    Opc = NVPTX::TEX_CUBE_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexCubeS32Float:\n    Opc = NVPTX::TEX_CUBE_S32_F32;\n    break;\n  case NVPTXISD::TexCubeS32FloatLevel:\n    Opc = NVPTX::TEX_CUBE_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexCubeU32Float:\n    Opc = NVPTX::TEX_CUBE_U32_F32;\n    break;\n  case NVPTXISD::TexCubeU32FloatLevel:\n    Opc = NVPTX::TEX_CUBE_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexCubeArrayFloatFloat:\n    Opc = NVPTX::TEX_CUBE_ARRAY_F32_F32;\n    break;\n  case NVPTXISD::TexCubeArrayFloatFloatLevel:\n    Opc = NVPTX::TEX_CUBE_ARRAY_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexCubeArrayS32Float:\n    Opc = NVPTX::TEX_CUBE_ARRAY_S32_F32;\n    break;\n  case NVPTXISD::TexCubeArrayS32FloatLevel:\n    Opc = NVPTX::TEX_CUBE_ARRAY_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexCubeArrayU32Float:\n    Opc = NVPTX::TEX_CUBE_ARRAY_U32_F32;\n    break;\n  case NVPTXISD::TexCubeArrayU32FloatLevel:\n    Opc = NVPTX::TEX_CUBE_ARRAY_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tld4R2DFloatFloat:\n    Opc = NVPTX::TLD4_R_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4G2DFloatFloat:\n    Opc = NVPTX::TLD4_G_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4B2DFloatFloat:\n    Opc = NVPTX::TLD4_B_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4A2DFloatFloat:\n    Opc = NVPTX::TLD4_A_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4R2DS64Float:\n    Opc = NVPTX::TLD4_R_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4G2DS64Float:\n    Opc = NVPTX::TLD4_G_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4B2DS64Float:\n    Opc = NVPTX::TLD4_B_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4A2DS64Float:\n    Opc = NVPTX::TLD4_A_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4R2DU64Float:\n    Opc = NVPTX::TLD4_R_2D_U32_F32;\n    break;\n  case NVPTXISD::Tld4G2DU64Float:\n    Opc = NVPTX::TLD4_G_2D_U32_F32;\n    break;\n  case NVPTXISD::Tld4B2DU64Float:\n    Opc = NVPTX::TLD4_B_2D_U32_F32;\n    break;\n  case NVPTXISD::Tld4A2DU64Float:\n    Opc = NVPTX::TLD4_A_2D_U32_F32;\n    break;\n  case NVPTXISD::TexUnified1DFloatS32:\n    Opc = NVPTX::TEX_UNIFIED_1D_F32_S32;\n    break;\n  case NVPTXISD::TexUnified1DFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_1D_F32_F32;\n    break;\n  case NVPTXISD::TexUnified1DFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_1D_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified1DFloatFloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_1D_F32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified1DS32S32:\n    Opc = NVPTX::TEX_UNIFIED_1D_S32_S32;\n    break;\n  case NVPTXISD::TexUnified1DS32Float:\n    Opc = NVPTX::TEX_UNIFIED_1D_S32_F32;\n    break;\n  case NVPTXISD::TexUnified1DS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_1D_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified1DS32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_1D_S32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified1DU32S32:\n    Opc = NVPTX::TEX_UNIFIED_1D_U32_S32;\n    break;\n  case NVPTXISD::TexUnified1DU32Float:\n    Opc = NVPTX::TEX_UNIFIED_1D_U32_F32;\n    break;\n  case NVPTXISD::TexUnified1DU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_1D_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified1DU32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_1D_U32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified1DArrayFloatS32:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_F32_S32;\n    break;\n  case NVPTXISD::TexUnified1DArrayFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_F32_F32;\n    break;\n  case NVPTXISD::TexUnified1DArrayFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified1DArrayFloatFloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_F32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified1DArrayS32S32:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_S32_S32;\n    break;\n  case NVPTXISD::TexUnified1DArrayS32Float:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_S32_F32;\n    break;\n  case NVPTXISD::TexUnified1DArrayS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified1DArrayS32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_S32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified1DArrayU32S32:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_U32_S32;\n    break;\n  case NVPTXISD::TexUnified1DArrayU32Float:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_U32_F32;\n    break;\n  case NVPTXISD::TexUnified1DArrayU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified1DArrayU32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_1D_ARRAY_U32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified2DFloatS32:\n    Opc = NVPTX::TEX_UNIFIED_2D_F32_S32;\n    break;\n  case NVPTXISD::TexUnified2DFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_2D_F32_F32;\n    break;\n  case NVPTXISD::TexUnified2DFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_2D_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified2DFloatFloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_2D_F32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified2DS32S32:\n    Opc = NVPTX::TEX_UNIFIED_2D_S32_S32;\n    break;\n  case NVPTXISD::TexUnified2DS32Float:\n    Opc = NVPTX::TEX_UNIFIED_2D_S32_F32;\n    break;\n  case NVPTXISD::TexUnified2DS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_2D_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified2DS32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_2D_S32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified2DU32S32:\n    Opc = NVPTX::TEX_UNIFIED_2D_U32_S32;\n    break;\n  case NVPTXISD::TexUnified2DU32Float:\n    Opc = NVPTX::TEX_UNIFIED_2D_U32_F32;\n    break;\n  case NVPTXISD::TexUnified2DU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_2D_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified2DU32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_2D_U32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified2DArrayFloatS32:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_F32_S32;\n    break;\n  case NVPTXISD::TexUnified2DArrayFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_F32_F32;\n    break;\n  case NVPTXISD::TexUnified2DArrayFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified2DArrayFloatFloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_F32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified2DArrayS32S32:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_S32_S32;\n    break;\n  case NVPTXISD::TexUnified2DArrayS32Float:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_S32_F32;\n    break;\n  case NVPTXISD::TexUnified2DArrayS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified2DArrayS32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_S32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified2DArrayU32S32:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_U32_S32;\n    break;\n  case NVPTXISD::TexUnified2DArrayU32Float:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_U32_F32;\n    break;\n  case NVPTXISD::TexUnified2DArrayU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified2DArrayU32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_2D_ARRAY_U32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified3DFloatS32:\n    Opc = NVPTX::TEX_UNIFIED_3D_F32_S32;\n    break;\n  case NVPTXISD::TexUnified3DFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_3D_F32_F32;\n    break;\n  case NVPTXISD::TexUnified3DFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_3D_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified3DFloatFloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_3D_F32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified3DS32S32:\n    Opc = NVPTX::TEX_UNIFIED_3D_S32_S32;\n    break;\n  case NVPTXISD::TexUnified3DS32Float:\n    Opc = NVPTX::TEX_UNIFIED_3D_S32_F32;\n    break;\n  case NVPTXISD::TexUnified3DS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_3D_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified3DS32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_3D_S32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnified3DU32S32:\n    Opc = NVPTX::TEX_UNIFIED_3D_U32_S32;\n    break;\n  case NVPTXISD::TexUnified3DU32Float:\n    Opc = NVPTX::TEX_UNIFIED_3D_U32_F32;\n    break;\n  case NVPTXISD::TexUnified3DU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_3D_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnified3DU32FloatGrad:\n    Opc = NVPTX::TEX_UNIFIED_3D_U32_F32_GRAD;\n    break;\n  case NVPTXISD::TexUnifiedCubeFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_F32_F32;\n    break;\n  case NVPTXISD::TexUnifiedCubeFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnifiedCubeS32Float:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_S32_F32;\n    break;\n  case NVPTXISD::TexUnifiedCubeS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnifiedCubeU32Float:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_U32_F32;\n    break;\n  case NVPTXISD::TexUnifiedCubeU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnifiedCubeArrayFloatFloat:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_ARRAY_F32_F32;\n    break;\n  case NVPTXISD::TexUnifiedCubeArrayFloatFloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_ARRAY_F32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnifiedCubeArrayS32Float:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_ARRAY_S32_F32;\n    break;\n  case NVPTXISD::TexUnifiedCubeArrayS32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_ARRAY_S32_F32_LEVEL;\n    break;\n  case NVPTXISD::TexUnifiedCubeArrayU32Float:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_ARRAY_U32_F32;\n    break;\n  case NVPTXISD::TexUnifiedCubeArrayU32FloatLevel:\n    Opc = NVPTX::TEX_UNIFIED_CUBE_ARRAY_U32_F32_LEVEL;\n    break;\n  case NVPTXISD::Tld4UnifiedR2DFloatFloat:\n    Opc = NVPTX::TLD4_UNIFIED_R_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedG2DFloatFloat:\n    Opc = NVPTX::TLD4_UNIFIED_G_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedB2DFloatFloat:\n    Opc = NVPTX::TLD4_UNIFIED_B_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedA2DFloatFloat:\n    Opc = NVPTX::TLD4_UNIFIED_A_2D_F32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedR2DS64Float:\n    Opc = NVPTX::TLD4_UNIFIED_R_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedG2DS64Float:\n    Opc = NVPTX::TLD4_UNIFIED_G_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedB2DS64Float:\n    Opc = NVPTX::TLD4_UNIFIED_B_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedA2DS64Float:\n    Opc = NVPTX::TLD4_UNIFIED_A_2D_S32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedR2DU64Float:\n    Opc = NVPTX::TLD4_UNIFIED_R_2D_U32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedG2DU64Float:\n    Opc = NVPTX::TLD4_UNIFIED_G_2D_U32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedB2DU64Float:\n    Opc = NVPTX::TLD4_UNIFIED_B_2D_U32_F32;\n    break;\n  case NVPTXISD::Tld4UnifiedA2DU64Float:\n    Opc = NVPTX::TLD4_UNIFIED_A_2D_U32_F32;\n    break;\n  }\n\n  // Copy over operands\n  for (unsigned i = 1; i < N->getNumOperands(); ++i) {\n    Ops.push_back(N->getOperand(i));\n  }\n\n  Ops.push_back(Chain);\n  ReplaceNode(N, CurDAG->getMachineNode(Opc, SDLoc(N), N->getVTList(), Ops));\n  return true;\n}\n\n"},"SelectInlineAsmMemoryOperand":{"range":[5176,5199],"code":"bool NVPTXDAGToDAGISel::SelectInlineAsmMemoryOperand(\n    const SDValue &Op, unsigned ConstraintID, std::vector<SDValue> &OutOps) {\n  SDValue Op0, Op1;\n  switch (ConstraintID) {\n  default:\n    return true;\n  case InlineAsm::Constraint_m: // memory\n    if (SelectDirectAddr(Op, Op0)) {\n      OutOps.push_back(Op0);\n      OutOps.push_back(CurDAG->getTargetConstant(0, SDLoc(Op), MVT::i32));\n      return false;\n    }\n    if (SelectADDRri(Op.getNode(), Op, Op0, Op1)) {\n      OutOps.push_back(Op0);\n      OutOps.push_back(Op1);\n      return false;\n    }\n    break;\n  }\n  return true;\n}\n\n/// GetConvertOpcode - Returns the CVT_ instruction opcode that implements a\n/// conversion from \\p SrcTy to \\p DestTy.\n"},"useF32FTZ":{"range":[87,100],"code":"bool NVPTXDAGToDAGISel::useF32FTZ() const {\n  if (FtzEnabled.getNumOccurrences() > 0) {\n    // If nvptx-f32ftz is used on the command-line, always honor it\n    return FtzEnabled;\n  } else {\n    const Function *F = MF->getFunction();\n    // Otherwise, check for an nvptx-f32ftz attribute on the function\n    if (F->hasFnAttribute(\"nvptx-f32ftz\"))\n      return F->getFnAttribute(\"nvptx-f32ftz\").getValueAsString() == \"true\";\n    else\n      return false;\n  }\n}\n\n"},"ChkMemSDNodeAddressSpace":{"range":[5159,5175],"code":"bool NVPTXDAGToDAGISel::ChkMemSDNodeAddressSpace(SDNode *N,\n                                                 unsigned int spN) const {\n  const Value *Src = nullptr;\n  if (MemSDNode *mN = dyn_cast<MemSDNode>(N)) {\n    if (spN == 0 && mN->getMemOperand()->getPseudoValue())\n      return true;\n    Src = mN->getMemOperand()->getValue();\n  }\n  if (!Src)\n    return false;\n  if (auto *PT = dyn_cast<PointerType>(Src->getType()))\n    return (PT->getAddressSpace() == spN);\n  return false;\n}\n\n/// SelectInlineAsmMemoryOperand - Implement addressing mode selection for\n/// inline asm expressions.\n"},"SelectADDRri":{"range":[5148,5153],"code":"bool NVPTXDAGToDAGISel::SelectADDRri(SDNode *OpNode, SDValue Addr,\n                                     SDValue &Base, SDValue &Offset) {\n  return SelectADDRri_imp(OpNode, Addr, Base, Offset, MVT::i32);\n}\n\n// register+offset\n"},"SelectADDRri64":{"range":[5154,5158],"code":"bool NVPTXDAGToDAGISel::SelectADDRri64(SDNode *OpNode, SDValue Addr,\n                                       SDValue &Base, SDValue &Offset) {\n  return SelectADDRri_imp(OpNode, Addr, Base, Offset, MVT::i64);\n}\n\n"},"tryLoadParam":{"range":[2726,2853],"code":"bool NVPTXDAGToDAGISel::tryLoadParam(SDNode *Node) {\n  SDValue Chain = Node->getOperand(0);\n  SDValue Offset = Node->getOperand(2);\n  SDValue Flag = Node->getOperand(3);\n  SDLoc DL(Node);\n  MemSDNode *Mem = cast<MemSDNode>(Node);\n\n  unsigned VecSize;\n  switch (Node->getOpcode()) {\n  default:\n    return false;\n  case NVPTXISD::LoadParam:\n    VecSize = 1;\n    break;\n  case NVPTXISD::LoadParamV2:\n    VecSize = 2;\n    break;\n  case NVPTXISD::LoadParamV4:\n    VecSize = 4;\n    break;\n  }\n\n  EVT EltVT = Node->getValueType(0);\n  EVT MemVT = Mem->getMemoryVT();\n\n  unsigned Opc = 0;\n\n  switch (VecSize) {\n  default:\n    return false;\n  case 1:\n    switch (MemVT.getSimpleVT().SimpleTy) {\n    default:\n      return false;\n    case MVT::i1:\n      Opc = NVPTX::LoadParamMemI8;\n      break;\n    case MVT::i8:\n      Opc = NVPTX::LoadParamMemI8;\n      break;\n    case MVT::i16:\n      Opc = NVPTX::LoadParamMemI16;\n      break;\n    case MVT::i32:\n      Opc = NVPTX::LoadParamMemI32;\n      break;\n    case MVT::i64:\n      Opc = NVPTX::LoadParamMemI64;\n      break;\n    case MVT::f32:\n      Opc = NVPTX::LoadParamMemF32;\n      break;\n    case MVT::f64:\n      Opc = NVPTX::LoadParamMemF64;\n      break;\n    }\n    break;\n  case 2:\n    switch (MemVT.getSimpleVT().SimpleTy) {\n    default:\n      return false;\n    case MVT::i1:\n      Opc = NVPTX::LoadParamMemV2I8;\n      break;\n    case MVT::i8:\n      Opc = NVPTX::LoadParamMemV2I8;\n      break;\n    case MVT::i16:\n      Opc = NVPTX::LoadParamMemV2I16;\n      break;\n    case MVT::i32:\n      Opc = NVPTX::LoadParamMemV2I32;\n      break;\n    case MVT::i64:\n      Opc = NVPTX::LoadParamMemV2I64;\n      break;\n    case MVT::f32:\n      Opc = NVPTX::LoadParamMemV2F32;\n      break;\n    case MVT::f64:\n      Opc = NVPTX::LoadParamMemV2F64;\n      break;\n    }\n    break;\n  case 4:\n    switch (MemVT.getSimpleVT().SimpleTy) {\n    default:\n      return false;\n    case MVT::i1:\n      Opc = NVPTX::LoadParamMemV4I8;\n      break;\n    case MVT::i8:\n      Opc = NVPTX::LoadParamMemV4I8;\n      break;\n    case MVT::i16:\n      Opc = NVPTX::LoadParamMemV4I16;\n      break;\n    case MVT::i32:\n      Opc = NVPTX::LoadParamMemV4I32;\n      break;\n    case MVT::f32:\n      Opc = NVPTX::LoadParamMemV4F32;\n      break;\n    }\n    break;\n  }\n\n  SDVTList VTs;\n  if (VecSize == 1) {\n    VTs = CurDAG->getVTList(EltVT, MVT::Other, MVT::Glue);\n  } else if (VecSize == 2) {\n    VTs = CurDAG->getVTList(EltVT, EltVT, MVT::Other, MVT::Glue);\n  } else {\n    EVT EVTs[] = { EltVT, EltVT, EltVT, EltVT, MVT::Other, MVT::Glue };\n    VTs = CurDAG->getVTList(EVTs);\n  }\n\n  unsigned OffsetVal = cast<ConstantSDNode>(Offset)->getZExtValue();\n\n  SmallVector<SDValue, 2> Ops;\n  Ops.push_back(CurDAG->getTargetConstant(OffsetVal, DL, MVT::i32));\n  Ops.push_back(Chain);\n  Ops.push_back(Flag);\n\n  ReplaceNode(Node, CurDAG->getMachineNode(Opc, DL, VTs, Ops));\n  return true;\n}\n\n"},"tryBFE":{"range":[4861,5067],"code":"bool NVPTXDAGToDAGISel::tryBFE(SDNode *N) {\n  SDLoc DL(N);\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n  SDValue Len;\n  SDValue Start;\n  SDValue Val;\n  bool IsSigned = false;\n\n  if (N->getOpcode() == ISD::AND) {\n    // Canonicalize the operands\n    // We want 'and %val, %mask'\n    if (isa<ConstantSDNode>(LHS) && !isa<ConstantSDNode>(RHS)) {\n      std::swap(LHS, RHS);\n    }\n\n    ConstantSDNode *Mask = dyn_cast<ConstantSDNode>(RHS);\n    if (!Mask) {\n      // We need a constant mask on the RHS of the AND\n      return false;\n    }\n\n    // Extract the mask bits\n    uint64_t MaskVal = Mask->getZExtValue();\n    if (!isMask_64(MaskVal)) {\n      // We *could* handle shifted masks here, but doing so would require an\n      // 'and' operation to fix up the low-order bits so we would trade\n      // shr+and for bfe+and, which has the same throughput\n      return false;\n    }\n\n    // How many bits are in our mask?\n    uint64_t NumBits = countTrailingOnes(MaskVal);\n    Len = CurDAG->getTargetConstant(NumBits, DL, MVT::i32);\n\n    if (LHS.getOpcode() == ISD::SRL || LHS.getOpcode() == ISD::SRA) {\n      // We have a 'srl/and' pair, extract the effective start bit and length\n      Val = LHS.getNode()->getOperand(0);\n      Start = LHS.getNode()->getOperand(1);\n      ConstantSDNode *StartConst = dyn_cast<ConstantSDNode>(Start);\n      if (StartConst) {\n        uint64_t StartVal = StartConst->getZExtValue();\n        // How many \"good\" bits do we have left?  \"good\" is defined here as bits\n        // that exist in the original value, not shifted in.\n        uint64_t GoodBits = Start.getValueType().getSizeInBits() - StartVal;\n        if (NumBits > GoodBits) {\n          // Do not handle the case where bits have been shifted in. In theory\n          // we could handle this, but the cost is likely higher than just\n          // emitting the srl/and pair.\n          return false;\n        }\n        Start = CurDAG->getTargetConstant(StartVal, DL, MVT::i32);\n      } else {\n        // Do not handle the case where the shift amount (can be zero if no srl\n        // was found) is not constant. We could handle this case, but it would\n        // require run-time logic that would be more expensive than just\n        // emitting the srl/and pair.\n        return false;\n      }\n    } else {\n      // Do not handle the case where the LHS of the and is not a shift. While\n      // it would be trivial to handle this case, it would just transform\n      // 'and' -> 'bfe', but 'and' has higher-throughput.\n      return false;\n    }\n  } else if (N->getOpcode() == ISD::SRL || N->getOpcode() == ISD::SRA) {\n    if (LHS->getOpcode() == ISD::AND) {\n      ConstantSDNode *ShiftCnst = dyn_cast<ConstantSDNode>(RHS);\n      if (!ShiftCnst) {\n        // Shift amount must be constant\n        return false;\n      }\n\n      uint64_t ShiftAmt = ShiftCnst->getZExtValue();\n\n      SDValue AndLHS = LHS->getOperand(0);\n      SDValue AndRHS = LHS->getOperand(1);\n\n      // Canonicalize the AND to have the mask on the RHS\n      if (isa<ConstantSDNode>(AndLHS)) {\n        std::swap(AndLHS, AndRHS);\n      }\n\n      ConstantSDNode *MaskCnst = dyn_cast<ConstantSDNode>(AndRHS);\n      if (!MaskCnst) {\n        // Mask must be constant\n        return false;\n      }\n\n      uint64_t MaskVal = MaskCnst->getZExtValue();\n      uint64_t NumZeros;\n      uint64_t NumBits;\n      if (isMask_64(MaskVal)) {\n        NumZeros = 0;\n        // The number of bits in the result bitfield will be the number of\n        // trailing ones (the AND) minus the number of bits we shift off\n        NumBits = countTrailingOnes(MaskVal) - ShiftAmt;\n      } else if (isShiftedMask_64(MaskVal)) {\n        NumZeros = countTrailingZeros(MaskVal);\n        unsigned NumOnes = countTrailingOnes(MaskVal >> NumZeros);\n        // The number of bits in the result bitfield will be the number of\n        // trailing zeros plus the number of set bits in the mask minus the\n        // number of bits we shift off\n        NumBits = NumZeros + NumOnes - ShiftAmt;\n      } else {\n        // This is not a mask we can handle\n        return false;\n      }\n\n      if (ShiftAmt < NumZeros) {\n        // Handling this case would require extra logic that would make this\n        // transformation non-profitable\n        return false;\n      }\n\n      Val = AndLHS;\n      Start = CurDAG->getTargetConstant(ShiftAmt, DL, MVT::i32);\n      Len = CurDAG->getTargetConstant(NumBits, DL, MVT::i32);\n    } else if (LHS->getOpcode() == ISD::SHL) {\n      // Here, we have a pattern like:\n      //\n      // (sra (shl val, NN), MM)\n      // or\n      // (srl (shl val, NN), MM)\n      //\n      // If MM >= NN, we can efficiently optimize this with bfe\n      Val = LHS->getOperand(0);\n\n      SDValue ShlRHS = LHS->getOperand(1);\n      ConstantSDNode *ShlCnst = dyn_cast<ConstantSDNode>(ShlRHS);\n      if (!ShlCnst) {\n        // Shift amount must be constant\n        return false;\n      }\n      uint64_t InnerShiftAmt = ShlCnst->getZExtValue();\n\n      SDValue ShrRHS = RHS;\n      ConstantSDNode *ShrCnst = dyn_cast<ConstantSDNode>(ShrRHS);\n      if (!ShrCnst) {\n        // Shift amount must be constant\n        return false;\n      }\n      uint64_t OuterShiftAmt = ShrCnst->getZExtValue();\n\n      // To avoid extra codegen and be profitable, we need Outer >= Inner\n      if (OuterShiftAmt < InnerShiftAmt) {\n        return false;\n      }\n\n      // If the outer shift is more than the type size, we have no bitfield to\n      // extract (since we also check that the inner shift is <= the outer shift\n      // then this also implies that the inner shift is < the type size)\n      if (OuterShiftAmt >= Val.getValueType().getSizeInBits()) {\n        return false;\n      }\n\n      Start =\n        CurDAG->getTargetConstant(OuterShiftAmt - InnerShiftAmt, DL, MVT::i32);\n      Len =\n        CurDAG->getTargetConstant(Val.getValueType().getSizeInBits() -\n                                  OuterShiftAmt, DL, MVT::i32);\n\n      if (N->getOpcode() == ISD::SRA) {\n        // If we have a arithmetic right shift, we need to use the signed bfe\n        // variant\n        IsSigned = true;\n      }\n    } else {\n      // No can do...\n      return false;\n    }\n  } else {\n    // No can do...\n    return false;\n  }\n\n\n  unsigned Opc;\n  // For the BFE operations we form here from \"and\" and \"srl\", always use the\n  // unsigned variants.\n  if (Val.getValueType() == MVT::i32) {\n    if (IsSigned) {\n      Opc = NVPTX::BFE_S32rii;\n    } else {\n      Opc = NVPTX::BFE_U32rii;\n    }\n  } else if (Val.getValueType() == MVT::i64) {\n    if (IsSigned) {\n      Opc = NVPTX::BFE_S64rii;\n    } else {\n      Opc = NVPTX::BFE_U64rii;\n    }\n  } else {\n    // We cannot handle this type\n    return false;\n  }\n\n  SDValue Ops[] = {\n    Val, Start, Len\n  };\n\n  ReplaceNode(N, CurDAG->getMachineNode(Opc, DL, N->getVTList(), Ops));\n  return true;\n}\n\n// SelectDirectAddr - Match a direct address for DAG.\n// A direct address could be a globaladdress or externalsymbol.\n"},"usePrecSqrtF32":{"range":[77,86],"code":"bool NVPTXDAGToDAGISel::usePrecSqrtF32() const {\n  if (UsePrecSqrtF32.getNumOccurrences() > 0) {\n    // If nvptx-prec-sqrtf32 is used on the command-line, always honor it\n    return UsePrecSqrtF32;\n  } else {\n    // Otherwise, use sqrt.approx if fast math is enabled\n    return !TM.Options.UnsafeFPMath;\n  }\n}\n\n"},"getCodeAddrSpace":{"range":[539,558],"code":"static unsigned int getCodeAddrSpace(MemSDNode *N) {\n  const Value *Src = N->getMemOperand()->getValue();\n\n  if (!Src)\n    return NVPTX::PTXLdStInstCode::GENERIC;\n\n  if (auto *PT = dyn_cast<PointerType>(Src->getType())) {\n    switch (PT->getAddressSpace()) {\n    case llvm::ADDRESS_SPACE_LOCAL: return NVPTX::PTXLdStInstCode::LOCAL;\n    case llvm::ADDRESS_SPACE_GLOBAL: return NVPTX::PTXLdStInstCode::GLOBAL;\n    case llvm::ADDRESS_SPACE_SHARED: return NVPTX::PTXLdStInstCode::SHARED;\n    case llvm::ADDRESS_SPACE_GENERIC: return NVPTX::PTXLdStInstCode::GENERIC;\n    case llvm::ADDRESS_SPACE_PARAM: return NVPTX::PTXLdStInstCode::PARAM;\n    case llvm::ADDRESS_SPACE_CONST: return NVPTX::PTXLdStInstCode::CONSTANT;\n    default: break;\n    }\n  }\n  return NVPTX::PTXLdStInstCode::GENERIC;\n}\n\n"},"NVPTXDAGToDAGISel":{"range":[53,58],"code":"NVPTXDAGToDAGISel::NVPTXDAGToDAGISel(NVPTXTargetMachine &tm,\n                                     CodeGenOpt::Level OptLevel)\n    : SelectionDAGISel(tm, OptLevel), TM(tm) {\n  doMulWide = (OptLevel > 0);\n}\n\n"},"allowFMA":{"range":[101,107],"code":"bool NVPTXDAGToDAGISel::allowFMA() const {\n  const NVPTXTargetLowering *TL = Subtarget->getTargetLowering();\n  return TL->allowFMA(*MF, OptLevel);\n}\n\n/// Select - Select instructions not customized! Used for\n/// expanded, promoted and normal instructions.\n"},"tryIntrinsicNoChain":{"range":[592,602],"code":"bool NVPTXDAGToDAGISel::tryIntrinsicNoChain(SDNode *N) {\n  unsigned IID = cast<ConstantSDNode>(N->getOperand(0))->getZExtValue();\n  switch (IID) {\n  default:\n    return false;\n  case Intrinsic::nvvm_texsurf_handle_internal:\n    SelectTexSurfHandle(N);\n    return true;\n  }\n}\n\n"},"tryStoreVector":{"range":[2351,2725],"code":"bool NVPTXDAGToDAGISel::tryStoreVector(SDNode *N) {\n  SDValue Chain = N->getOperand(0);\n  SDValue Op1 = N->getOperand(1);\n  SDValue Addr, Offset, Base;\n  unsigned Opcode;\n  SDLoc DL(N);\n  SDNode *ST;\n  EVT EltVT = Op1.getValueType();\n  MemSDNode *MemSD = cast<MemSDNode>(N);\n  EVT StoreVT = MemSD->getMemoryVT();\n\n  // Address Space Setting\n  unsigned CodeAddrSpace = getCodeAddrSpace(MemSD);\n\n  if (CodeAddrSpace == NVPTX::PTXLdStInstCode::CONSTANT) {\n    report_fatal_error(\"Cannot store to pointer that points to constant \"\n                       \"memory space\");\n  }\n\n  // Volatile Setting\n  // - .volatile is only availalble for .global and .shared\n  bool IsVolatile = MemSD->isVolatile();\n  if (CodeAddrSpace != NVPTX::PTXLdStInstCode::GLOBAL &&\n      CodeAddrSpace != NVPTX::PTXLdStInstCode::SHARED &&\n      CodeAddrSpace != NVPTX::PTXLdStInstCode::GENERIC)\n    IsVolatile = false;\n\n  // Type Setting: toType + toTypeWidth\n  // - for integer type, always use 'u'\n  assert(StoreVT.isSimple() && \"Store value is not simple\");\n  MVT ScalarVT = StoreVT.getSimpleVT().getScalarType();\n  unsigned ToTypeWidth = ScalarVT.getSizeInBits();\n  unsigned ToType;\n  if (ScalarVT.isFloatingPoint())\n    ToType = NVPTX::PTXLdStInstCode::Float;\n  else\n    ToType = NVPTX::PTXLdStInstCode::Unsigned;\n\n  SmallVector<SDValue, 12> StOps;\n  SDValue N2;\n  unsigned VecType;\n\n  switch (N->getOpcode()) {\n  case NVPTXISD::StoreV2:\n    VecType = NVPTX::PTXLdStInstCode::V2;\n    StOps.push_back(N->getOperand(1));\n    StOps.push_back(N->getOperand(2));\n    N2 = N->getOperand(3);\n    break;\n  case NVPTXISD::StoreV4:\n    VecType = NVPTX::PTXLdStInstCode::V4;\n    StOps.push_back(N->getOperand(1));\n    StOps.push_back(N->getOperand(2));\n    StOps.push_back(N->getOperand(3));\n    StOps.push_back(N->getOperand(4));\n    N2 = N->getOperand(5);\n    break;\n  default:\n    return false;\n  }\n\n  StOps.push_back(getI32Imm(IsVolatile, DL));\n  StOps.push_back(getI32Imm(CodeAddrSpace, DL));\n  StOps.push_back(getI32Imm(VecType, DL));\n  StOps.push_back(getI32Imm(ToType, DL));\n  StOps.push_back(getI32Imm(ToTypeWidth, DL));\n\n  if (SelectDirectAddr(N2, Addr)) {\n    switch (N->getOpcode()) {\n    default:\n      return false;\n    case NVPTXISD::StoreV2:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::STV_i8_v2_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::STV_i16_v2_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::STV_i32_v2_avar;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::STV_i64_v2_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::STV_f32_v2_avar;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::STV_f64_v2_avar;\n        break;\n      }\n      break;\n    case NVPTXISD::StoreV4:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::STV_i8_v4_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::STV_i16_v4_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::STV_i32_v4_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::STV_f32_v4_avar;\n        break;\n      }\n      break;\n    }\n    StOps.push_back(Addr);\n  } else if (TM.is64Bit() ? SelectADDRsi64(N2.getNode(), N2, Base, Offset)\n                          : SelectADDRsi(N2.getNode(), N2, Base, Offset)) {\n    switch (N->getOpcode()) {\n    default:\n      return false;\n    case NVPTXISD::StoreV2:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::STV_i8_v2_asi;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::STV_i16_v2_asi;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::STV_i32_v2_asi;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::STV_i64_v2_asi;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::STV_f32_v2_asi;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::STV_f64_v2_asi;\n        break;\n      }\n      break;\n    case NVPTXISD::StoreV4:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::STV_i8_v4_asi;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::STV_i16_v4_asi;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::STV_i32_v4_asi;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::STV_f32_v4_asi;\n        break;\n      }\n      break;\n    }\n    StOps.push_back(Base);\n    StOps.push_back(Offset);\n  } else if (TM.is64Bit() ? SelectADDRri64(N2.getNode(), N2, Base, Offset)\n                          : SelectADDRri(N2.getNode(), N2, Base, Offset)) {\n    if (TM.is64Bit()) {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::StoreV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v2_ari_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v2_ari_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v2_ari_64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::STV_i64_v2_ari_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v2_ari_64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::STV_f64_v2_ari_64;\n          break;\n        }\n        break;\n      case NVPTXISD::StoreV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v4_ari_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v4_ari_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v4_ari_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v4_ari_64;\n          break;\n        }\n        break;\n      }\n    } else {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::StoreV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v2_ari;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v2_ari;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v2_ari;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::STV_i64_v2_ari;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v2_ari;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::STV_f64_v2_ari;\n          break;\n        }\n        break;\n      case NVPTXISD::StoreV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v4_ari;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v4_ari;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v4_ari;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v4_ari;\n          break;\n        }\n        break;\n      }\n    }\n    StOps.push_back(Base);\n    StOps.push_back(Offset);\n  } else {\n    if (TM.is64Bit()) {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::StoreV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v2_areg_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v2_areg_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v2_areg_64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::STV_i64_v2_areg_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v2_areg_64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::STV_f64_v2_areg_64;\n          break;\n        }\n        break;\n      case NVPTXISD::StoreV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v4_areg_64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v4_areg_64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v4_areg_64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v4_areg_64;\n          break;\n        }\n        break;\n      }\n    } else {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case NVPTXISD::StoreV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v2_areg;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v2_areg;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v2_areg;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::STV_i64_v2_areg;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v2_areg;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::STV_f64_v2_areg;\n          break;\n        }\n        break;\n      case NVPTXISD::StoreV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::STV_i8_v4_areg;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::STV_i16_v4_areg;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::STV_i32_v4_areg;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::STV_f32_v4_areg;\n          break;\n        }\n        break;\n      }\n    }\n    StOps.push_back(N2);\n  }\n\n  StOps.push_back(Chain);\n\n  ST = CurDAG->getMachineNode(Opcode, DL, MVT::Other, StOps);\n\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = cast<MemSDNode>(N)->getMemOperand();\n  cast<MachineSDNode>(ST)->setMemRefs(MemRefs0, MemRefs0 + 1);\n\n  ReplaceNode(N, ST);\n  return true;\n}\n\n"},"canLowerToLDG":{"range":[559,591],"code":"static bool canLowerToLDG(MemSDNode *N, const NVPTXSubtarget &Subtarget,\n                          unsigned CodeAddrSpace, MachineFunction *F) {\n  // To use non-coherent caching, the load has to be from global\n  // memory and we have to prove that the memory area is not written\n  // to anywhere for the duration of the kernel call, not even after\n  // the load.\n  //\n  // To ensure that there are no writes to the memory, we require the\n  // underlying pointer to be a noalias (__restrict) kernel parameter\n  // that is never used for a write. We can only do this for kernel\n  // functions since from within a device function, we cannot know if\n  // there were or will be writes to the memory from the caller - or we\n  // could, but then we would have to do inter-procedural analysis.\n  if (!Subtarget.hasLDG() || CodeAddrSpace != NVPTX::PTXLdStInstCode::GLOBAL ||\n      !isKernelFunction(*F->getFunction())) {\n    return false;\n  }\n\n  // We use GetUnderlyingObjects() here instead of\n  // GetUnderlyingObject() mainly because the former looks through phi\n  // nodes while the latter does not. We need to look through phi\n  // nodes to handle pointer induction variables.\n  SmallVector<Value *, 8> Objs;\n  GetUnderlyingObjects(const_cast<Value *>(N->getMemOperand()->getValue()),\n                       Objs, F->getDataLayout());\n  for (Value *Obj : Objs) {\n    auto *A = dyn_cast<const Argument>(Obj);\n    if (!A || !A->onlyReadsMemory() || !A->hasNoAliasAttr()) return false;\n  }\n\n  return true;\n}\n\n"},"SelectADDRri_imp":{"range":[5117,5147],"code":"bool NVPTXDAGToDAGISel::SelectADDRri_imp(\n    SDNode *OpNode, SDValue Addr, SDValue &Base, SDValue &Offset, MVT mvt) {\n  if (FrameIndexSDNode *FIN = dyn_cast<FrameIndexSDNode>(Addr)) {\n    Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), mvt);\n    Offset = CurDAG->getTargetConstant(0, SDLoc(OpNode), mvt);\n    return true;\n  }\n  if (Addr.getOpcode() == ISD::TargetExternalSymbol ||\n      Addr.getOpcode() == ISD::TargetGlobalAddress)\n    return false; // direct calls.\n\n  if (Addr.getOpcode() == ISD::ADD) {\n    if (SelectDirectAddr(Addr.getOperand(0), Addr)) {\n      return false;\n    }\n    if (ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1))) {\n      if (FrameIndexSDNode *FIN =\n              dyn_cast<FrameIndexSDNode>(Addr.getOperand(0)))\n        // Constant offset from frame ref.\n        Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), mvt);\n      else\n        Base = Addr.getOperand(0);\n      Offset = CurDAG->getTargetConstant(CN->getZExtValue(), SDLoc(OpNode),\n                                         mvt);\n      return true;\n    }\n  }\n  return false;\n}\n\n// register+offset\n"},"SelectAddrSpaceCast":{"range":[611,674],"code":"void NVPTXDAGToDAGISel::SelectAddrSpaceCast(SDNode *N) {\n  SDValue Src = N->getOperand(0);\n  AddrSpaceCastSDNode *CastN = cast<AddrSpaceCastSDNode>(N);\n  unsigned SrcAddrSpace = CastN->getSrcAddressSpace();\n  unsigned DstAddrSpace = CastN->getDestAddressSpace();\n\n  assert(SrcAddrSpace != DstAddrSpace &&\n         \"addrspacecast must be between different address spaces\");\n\n  if (DstAddrSpace == ADDRESS_SPACE_GENERIC) {\n    // Specific to generic\n    unsigned Opc;\n    switch (SrcAddrSpace) {\n    default: report_fatal_error(\"Bad address space in addrspacecast\");\n    case ADDRESS_SPACE_GLOBAL:\n      Opc = TM.is64Bit() ? NVPTX::cvta_global_yes_64 : NVPTX::cvta_global_yes;\n      break;\n    case ADDRESS_SPACE_SHARED:\n      Opc = TM.is64Bit() ? NVPTX::cvta_shared_yes_64 : NVPTX::cvta_shared_yes;\n      break;\n    case ADDRESS_SPACE_CONST:\n      Opc = TM.is64Bit() ? NVPTX::cvta_const_yes_64 : NVPTX::cvta_const_yes;\n      break;\n    case ADDRESS_SPACE_LOCAL:\n      Opc = TM.is64Bit() ? NVPTX::cvta_local_yes_64 : NVPTX::cvta_local_yes;\n      break;\n    }\n    ReplaceNode(N, CurDAG->getMachineNode(Opc, SDLoc(N), N->getValueType(0),\n                                          Src));\n    return;\n  } else {\n    // Generic to specific\n    if (SrcAddrSpace != 0)\n      report_fatal_error(\"Cannot cast between two non-generic address spaces\");\n    unsigned Opc;\n    switch (DstAddrSpace) {\n    default: report_fatal_error(\"Bad address space in addrspacecast\");\n    case ADDRESS_SPACE_GLOBAL:\n      Opc = TM.is64Bit() ? NVPTX::cvta_to_global_yes_64\n                         : NVPTX::cvta_to_global_yes;\n      break;\n    case ADDRESS_SPACE_SHARED:\n      Opc = TM.is64Bit() ? NVPTX::cvta_to_shared_yes_64\n                         : NVPTX::cvta_to_shared_yes;\n      break;\n    case ADDRESS_SPACE_CONST:\n      Opc =\n          TM.is64Bit() ? NVPTX::cvta_to_const_yes_64 : NVPTX::cvta_to_const_yes;\n      break;\n    case ADDRESS_SPACE_LOCAL:\n      Opc =\n          TM.is64Bit() ? NVPTX::cvta_to_local_yes_64 : NVPTX::cvta_to_local_yes;\n      break;\n    case ADDRESS_SPACE_PARAM:\n      Opc = TM.is64Bit() ? NVPTX::nvvm_ptr_gen_to_param_64\n                         : NVPTX::nvvm_ptr_gen_to_param;\n      break;\n    }\n    ReplaceNode(N, CurDAG->getMachineNode(Opc, SDLoc(N), N->getValueType(0),\n                                          Src));\n    return;\n  }\n}\n\n"},"tryLDGLDU":{"range":[1300,2117],"code":"bool NVPTXDAGToDAGISel::tryLDGLDU(SDNode *N) {\n\n  SDValue Chain = N->getOperand(0);\n  SDValue Op1;\n  MemSDNode *Mem;\n  bool IsLDG = true;\n\n  // If this is an LDG intrinsic, the address is the third operand. If its an\n  // LDG/LDU SD node (from custom vector handling), then its the second operand\n  if (N->getOpcode() == ISD::INTRINSIC_W_CHAIN) {\n    Op1 = N->getOperand(2);\n    Mem = cast<MemIntrinsicSDNode>(N);\n    unsigned IID = cast<ConstantSDNode>(N->getOperand(1))->getZExtValue();\n    switch (IID) {\n    default:\n      return false;\n    case Intrinsic::nvvm_ldg_global_f:\n    case Intrinsic::nvvm_ldg_global_i:\n    case Intrinsic::nvvm_ldg_global_p:\n      IsLDG = true;\n      break;\n    case Intrinsic::nvvm_ldu_global_f:\n    case Intrinsic::nvvm_ldu_global_i:\n    case Intrinsic::nvvm_ldu_global_p:\n      IsLDG = false;\n      break;\n    }\n  } else {\n    Op1 = N->getOperand(1);\n    Mem = cast<MemSDNode>(N);\n  }\n\n  unsigned Opcode;\n  SDLoc DL(N);\n  SDNode *LD;\n  SDValue Base, Offset, Addr;\n\n  EVT EltVT = Mem->getMemoryVT();\n  unsigned NumElts = 1;\n  if (EltVT.isVector()) {\n    NumElts = EltVT.getVectorNumElements();\n    EltVT = EltVT.getVectorElementType();\n  }\n\n  // Build the \"promoted\" result VTList for the load. If we are really loading\n  // i8s, then the return type will be promoted to i16 since we do not expose\n  // 8-bit registers in NVPTX.\n  EVT NodeVT = (EltVT == MVT::i8) ? MVT::i16 : EltVT;\n  SmallVector<EVT, 5> InstVTs;\n  for (unsigned i = 0; i != NumElts; ++i) {\n    InstVTs.push_back(NodeVT);\n  }\n  InstVTs.push_back(MVT::Other);\n  SDVTList InstVTList = CurDAG->getVTList(InstVTs);\n\n  if (SelectDirectAddr(Op1, Addr)) {\n    switch (N->getOpcode()) {\n    default:\n      return false;\n    case ISD::INTRINSIC_W_CHAIN:\n      if (IsLDG) {\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i8avar;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i16avar;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i32avar;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i64avar;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f32avar;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f64avar;\n          break;\n        }\n      } else {\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i8avar;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i16avar;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i32avar;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i64avar;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f32avar;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f64avar;\n          break;\n        }\n      }\n      break;\n    case NVPTXISD::LDGV2:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::INT_PTX_LDG_G_v2i8_ELE_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::INT_PTX_LDG_G_v2i16_ELE_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::INT_PTX_LDG_G_v2i32_ELE_avar;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::INT_PTX_LDG_G_v2i64_ELE_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::INT_PTX_LDG_G_v2f32_ELE_avar;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::INT_PTX_LDG_G_v2f64_ELE_avar;\n        break;\n      }\n      break;\n    case NVPTXISD::LDUV2:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::INT_PTX_LDU_G_v2i8_ELE_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::INT_PTX_LDU_G_v2i16_ELE_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::INT_PTX_LDU_G_v2i32_ELE_avar;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::INT_PTX_LDU_G_v2i64_ELE_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::INT_PTX_LDU_G_v2f32_ELE_avar;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::INT_PTX_LDU_G_v2f64_ELE_avar;\n        break;\n      }\n      break;\n    case NVPTXISD::LDGV4:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::INT_PTX_LDG_G_v4i8_ELE_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::INT_PTX_LDG_G_v4i16_ELE_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::INT_PTX_LDG_G_v4i32_ELE_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::INT_PTX_LDG_G_v4f32_ELE_avar;\n        break;\n      }\n      break;\n    case NVPTXISD::LDUV4:\n      switch (EltVT.getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i8:\n        Opcode = NVPTX::INT_PTX_LDU_G_v4i8_ELE_avar;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::INT_PTX_LDU_G_v4i16_ELE_avar;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::INT_PTX_LDU_G_v4i32_ELE_avar;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::INT_PTX_LDU_G_v4f32_ELE_avar;\n        break;\n      }\n      break;\n    }\n\n    SDValue Ops[] = { Addr, Chain };\n    LD = CurDAG->getMachineNode(Opcode, DL, InstVTList, Ops);\n  } else if (TM.is64Bit() ? SelectADDRri64(Op1.getNode(), Op1, Base, Offset)\n                          : SelectADDRri(Op1.getNode(), Op1, Base, Offset)) {\n    if (TM.is64Bit()) {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case ISD::LOAD:\n      case ISD::INTRINSIC_W_CHAIN:\n        if (IsLDG) {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i8ari64;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i16ari64;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i32ari64;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i64ari64;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f32ari64;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f64ari64;\n            break;\n          }\n        } else {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i8ari64;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i16ari64;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i32ari64;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i64ari64;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f32ari64;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f64ari64;\n            break;\n          }\n        }\n        break;\n      case NVPTXISD::LoadV2:\n      case NVPTXISD::LDGV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i8_ELE_ari64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i16_ELE_ari64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i32_ELE_ari64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i64_ELE_ari64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f32_ELE_ari64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f64_ELE_ari64;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i8_ELE_ari64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i16_ELE_ari64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i32_ELE_ari64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i64_ELE_ari64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f32_ELE_ari64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f64_ELE_ari64;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n      case NVPTXISD::LDGV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i8_ELE_ari64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i16_ELE_ari64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i32_ELE_ari64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4f32_ELE_ari64;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i8_ELE_ari64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i16_ELE_ari64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i32_ELE_ari64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4f32_ELE_ari64;\n          break;\n        }\n        break;\n      }\n    } else {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case ISD::LOAD:\n      case ISD::INTRINSIC_W_CHAIN:\n        if (IsLDG) {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i8ari;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i16ari;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i32ari;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i64ari;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f32ari;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f64ari;\n            break;\n          }\n        } else {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i8ari;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i16ari;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i32ari;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i64ari;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f32ari;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f64ari;\n            break;\n          }\n        }\n        break;\n      case NVPTXISD::LoadV2:\n      case NVPTXISD::LDGV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i8_ELE_ari32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i16_ELE_ari32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i32_ELE_ari32;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i64_ELE_ari32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f32_ELE_ari32;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f64_ELE_ari32;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i8_ELE_ari32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i16_ELE_ari32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i32_ELE_ari32;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i64_ELE_ari32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f32_ELE_ari32;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f64_ELE_ari32;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n      case NVPTXISD::LDGV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i8_ELE_ari32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i16_ELE_ari32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i32_ELE_ari32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4f32_ELE_ari32;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i8_ELE_ari32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i16_ELE_ari32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i32_ELE_ari32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4f32_ELE_ari32;\n          break;\n        }\n        break;\n      }\n    }\n\n    SDValue Ops[] = { Base, Offset, Chain };\n\n    LD = CurDAG->getMachineNode(Opcode, DL, InstVTList, Ops);\n  } else {\n    if (TM.is64Bit()) {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case ISD::LOAD:\n      case ISD::INTRINSIC_W_CHAIN:\n        if (IsLDG) {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i8areg64;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i16areg64;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i32areg64;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i64areg64;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f32areg64;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f64areg64;\n            break;\n          }\n        } else {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i8areg64;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i16areg64;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i32areg64;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i64areg64;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f32areg64;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f64areg64;\n            break;\n          }\n        }\n        break;\n      case NVPTXISD::LoadV2:\n      case NVPTXISD::LDGV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i8_ELE_areg64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i16_ELE_areg64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i32_ELE_areg64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i64_ELE_areg64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f32_ELE_areg64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f64_ELE_areg64;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i8_ELE_areg64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i16_ELE_areg64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i32_ELE_areg64;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i64_ELE_areg64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f32_ELE_areg64;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f64_ELE_areg64;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n      case NVPTXISD::LDGV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i8_ELE_areg64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i16_ELE_areg64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i32_ELE_areg64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4f32_ELE_areg64;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i8_ELE_areg64;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i16_ELE_areg64;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i32_ELE_areg64;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4f32_ELE_areg64;\n          break;\n        }\n        break;\n      }\n    } else {\n      switch (N->getOpcode()) {\n      default:\n        return false;\n      case ISD::LOAD:\n      case ISD::INTRINSIC_W_CHAIN:\n        if (IsLDG) {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i8areg;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i16areg;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i32areg;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_i64areg;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f32areg;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDG_GLOBAL_f64areg;\n            break;\n          }\n        } else {\n          switch (EltVT.getSimpleVT().SimpleTy) {\n          default:\n            return false;\n          case MVT::i8:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i8areg;\n            break;\n          case MVT::i16:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i16areg;\n            break;\n          case MVT::i32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i32areg;\n            break;\n          case MVT::i64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_i64areg;\n            break;\n          case MVT::f32:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f32areg;\n            break;\n          case MVT::f64:\n            Opcode = NVPTX::INT_PTX_LDU_GLOBAL_f64areg;\n            break;\n          }\n        }\n        break;\n      case NVPTXISD::LoadV2:\n      case NVPTXISD::LDGV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i8_ELE_areg32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i16_ELE_areg32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i32_ELE_areg32;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2i64_ELE_areg32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f32_ELE_areg32;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDG_G_v2f64_ELE_areg32;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV2:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i8_ELE_areg32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i16_ELE_areg32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i32_ELE_areg32;\n          break;\n        case MVT::i64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2i64_ELE_areg32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f32_ELE_areg32;\n          break;\n        case MVT::f64:\n          Opcode = NVPTX::INT_PTX_LDU_G_v2f64_ELE_areg32;\n          break;\n        }\n        break;\n      case NVPTXISD::LoadV4:\n      case NVPTXISD::LDGV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i8_ELE_areg32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i16_ELE_areg32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4i32_ELE_areg32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDG_G_v4f32_ELE_areg32;\n          break;\n        }\n        break;\n      case NVPTXISD::LDUV4:\n        switch (EltVT.getSimpleVT().SimpleTy) {\n        default:\n          return false;\n        case MVT::i8:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i8_ELE_areg32;\n          break;\n        case MVT::i16:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i16_ELE_areg32;\n          break;\n        case MVT::i32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4i32_ELE_areg32;\n          break;\n        case MVT::f32:\n          Opcode = NVPTX::INT_PTX_LDU_G_v4f32_ELE_areg32;\n          break;\n        }\n        break;\n      }\n    }\n\n    SDValue Ops[] = { Op1, Chain };\n    LD = CurDAG->getMachineNode(Opcode, DL, InstVTList, Ops);\n  }\n\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = Mem->getMemOperand();\n  cast<MachineSDNode>(LD)->setMemRefs(MemRefs0, MemRefs0 + 1);\n\n  // For automatic generation of LDG (through SelectLoad[Vector], not the\n  // intrinsics), we may have an extending load like:\n  //\n  //   i32,ch = load<LD1[%data1(addrspace=1)], zext from i8> t0, t7, undef:i64\n  //\n  // In this case, the matching logic above will select a load for the original\n  // memory type (in this case, i8) and our types will not match (the node needs\n  // to return an i32 in this case). Our LDG/LDU nodes do not support the\n  // concept of sign-/zero-extension, so emulate it here by adding an explicit\n  // CVT instruction. Ptxas should clean up any redundancies here.\n\n  EVT OrigType = N->getValueType(0);\n  LoadSDNode *LdNode = dyn_cast<LoadSDNode>(N);\n\n  if (OrigType != EltVT && LdNode) {\n    // We have an extending-load. The instruction we selected operates on the\n    // smaller type, but the SDNode we are replacing has the larger type. We\n    // need to emit a CVT to make the types match.\n    bool IsSigned = LdNode->getExtensionType() == ISD::SEXTLOAD;\n    unsigned CvtOpc = GetConvertOpcode(OrigType.getSimpleVT(),\n                                       EltVT.getSimpleVT(), IsSigned);\n\n    // For each output value, apply the manual sign/zero-extension and make sure\n    // all users of the load go through that CVT.\n    for (unsigned i = 0; i != NumElts; ++i) {\n      SDValue Res(LD, i);\n      SDValue OrigVal(N, i);\n\n      SDNode *CvtNode =\n        CurDAG->getMachineNode(CvtOpc, DL, OrigType, Res,\n                               CurDAG->getTargetConstant(NVPTX::PTXCvtMode::NONE,\n                                                         DL, MVT::i32));\n      ReplaceUses(OrigVal, SDValue(CvtNode, 0));\n    }\n  }\n\n  ReplaceNode(N, LD);\n  return true;\n}\n\n"},"tryIntrinsicChain":{"range":[524,538],"code":"bool NVPTXDAGToDAGISel::tryIntrinsicChain(SDNode *N) {\n  unsigned IID = cast<ConstantSDNode>(N->getOperand(1))->getZExtValue();\n  switch (IID) {\n  default:\n    return false;\n  case Intrinsic::nvvm_ldg_global_f:\n  case Intrinsic::nvvm_ldg_global_i:\n  case Intrinsic::nvvm_ldg_global_p:\n  case Intrinsic::nvvm_ldu_global_f:\n  case Intrinsic::nvvm_ldu_global_i:\n  case Intrinsic::nvvm_ldu_global_p:\n    return tryLDGLDU(N);\n  }\n}\n\n"},"SelectDirectAddr":{"range":[5068,5088],"code":"bool NVPTXDAGToDAGISel::SelectDirectAddr(SDValue N, SDValue &Address) {\n  // Return true if TGA or ES.\n  if (N.getOpcode() == ISD::TargetGlobalAddress ||\n      N.getOpcode() == ISD::TargetExternalSymbol) {\n    Address = N;\n    return true;\n  }\n  if (N.getOpcode() == NVPTXISD::Wrapper) {\n    Address = N.getOperand(0);\n    return true;\n  }\n  if (N.getOpcode() == ISD::INTRINSIC_WO_CHAIN) {\n    unsigned IID = cast<ConstantSDNode>(N.getOperand(0))->getZExtValue();\n    if (IID == Intrinsic::nvvm_ptr_gen_to_param)\n      if (N.getOperand(1).getOpcode() == NVPTXISD::MoveParam)\n        return (SelectDirectAddr(N.getOperand(1).getOperand(0), Address));\n  }\n  return false;\n}\n\n// symbol+offset\n"},"SelectTexSurfHandle":{"range":[603,610],"code":"void NVPTXDAGToDAGISel::SelectTexSurfHandle(SDNode *N) {\n  // Op 0 is the intrinsic ID\n  SDValue Wrapper = N->getOperand(1);\n  SDValue GlobalVal = Wrapper.getOperand(0);\n  ReplaceNode(N, CurDAG->getMachineNode(NVPTX::texsurf_handles, SDLoc(N),\n                                        MVT::i64, GlobalVal));\n}\n\n"},"SelectADDRsi_imp":{"range":[5089,5104],"code":"bool NVPTXDAGToDAGISel::SelectADDRsi_imp(\n    SDNode *OpNode, SDValue Addr, SDValue &Base, SDValue &Offset, MVT mvt) {\n  if (Addr.getOpcode() == ISD::ADD) {\n    if (ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1))) {\n      SDValue base = Addr.getOperand(0);\n      if (SelectDirectAddr(base, Base)) {\n        Offset = CurDAG->getTargetConstant(CN->getZExtValue(), SDLoc(OpNode),\n                                           mvt);\n        return true;\n      }\n    }\n  }\n  return false;\n}\n\n// symbol+offset\n"},"SelectADDRsi64":{"range":[5111,5116],"code":"bool NVPTXDAGToDAGISel::SelectADDRsi64(SDNode *OpNode, SDValue Addr,\n                                       SDValue &Base, SDValue &Offset) {\n  return SelectADDRsi_imp(OpNode, Addr, Base, Offset, MVT::i64);\n}\n\n// register+offset\n"},"tryLoad":{"range":[675,913],"code":"bool NVPTXDAGToDAGISel::tryLoad(SDNode *N) {\n  SDLoc dl(N);\n  LoadSDNode *LD = cast<LoadSDNode>(N);\n  EVT LoadedVT = LD->getMemoryVT();\n  SDNode *NVPTXLD = nullptr;\n\n  // do not support pre/post inc/dec\n  if (LD->isIndexed())\n    return false;\n\n  if (!LoadedVT.isSimple())\n    return false;\n\n  // Address Space Setting\n  unsigned int codeAddrSpace = getCodeAddrSpace(LD);\n\n  if (canLowerToLDG(LD, *Subtarget, codeAddrSpace, MF)) {\n    return tryLDGLDU(N);\n  }\n\n  // Volatile Setting\n  // - .volatile is only availalble for .global and .shared\n  bool isVolatile = LD->isVolatile();\n  if (codeAddrSpace != NVPTX::PTXLdStInstCode::GLOBAL &&\n      codeAddrSpace != NVPTX::PTXLdStInstCode::SHARED &&\n      codeAddrSpace != NVPTX::PTXLdStInstCode::GENERIC)\n    isVolatile = false;\n\n  // Vector Setting\n  MVT SimpleVT = LoadedVT.getSimpleVT();\n  unsigned vecType = NVPTX::PTXLdStInstCode::Scalar;\n  if (SimpleVT.isVector()) {\n    unsigned num = SimpleVT.getVectorNumElements();\n    if (num == 2)\n      vecType = NVPTX::PTXLdStInstCode::V2;\n    else if (num == 4)\n      vecType = NVPTX::PTXLdStInstCode::V4;\n    else\n      return false;\n  }\n\n  // Type Setting: fromType + fromTypeWidth\n  //\n  // Sign   : ISD::SEXTLOAD\n  // Unsign : ISD::ZEXTLOAD, ISD::NON_EXTLOAD or ISD::EXTLOAD and the\n  //          type is integer\n  // Float  : ISD::NON_EXTLOAD or ISD::EXTLOAD and the type is float\n  MVT ScalarVT = SimpleVT.getScalarType();\n  // Read at least 8 bits (predicates are stored as 8-bit values)\n  unsigned fromTypeWidth = std::max(8U, ScalarVT.getSizeInBits());\n  unsigned int fromType;\n  if ((LD->getExtensionType() == ISD::SEXTLOAD))\n    fromType = NVPTX::PTXLdStInstCode::Signed;\n  else if (ScalarVT.isFloatingPoint())\n    fromType = NVPTX::PTXLdStInstCode::Float;\n  else\n    fromType = NVPTX::PTXLdStInstCode::Unsigned;\n\n  // Create the machine instruction DAG\n  SDValue Chain = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n  SDValue Addr;\n  SDValue Offset, Base;\n  unsigned Opcode;\n  MVT::SimpleValueType TargetVT = LD->getSimpleValueType(0).SimpleTy;\n\n  if (SelectDirectAddr(N1, Addr)) {\n    switch (TargetVT) {\n    case MVT::i8:\n      Opcode = NVPTX::LD_i8_avar;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::LD_i16_avar;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::LD_i32_avar;\n      break;\n    case MVT::i64:\n      Opcode = NVPTX::LD_i64_avar;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::LD_f32_avar;\n      break;\n    case MVT::f64:\n      Opcode = NVPTX::LD_f64_avar;\n      break;\n    default:\n      return false;\n    }\n    SDValue Ops[] = { getI32Imm(isVolatile, dl), getI32Imm(codeAddrSpace, dl),\n                      getI32Imm(vecType, dl), getI32Imm(fromType, dl),\n                      getI32Imm(fromTypeWidth, dl), Addr, Chain };\n    NVPTXLD = CurDAG->getMachineNode(Opcode, dl, TargetVT, MVT::Other, Ops);\n  } else if (TM.is64Bit() ? SelectADDRsi64(N1.getNode(), N1, Base, Offset)\n                          : SelectADDRsi(N1.getNode(), N1, Base, Offset)) {\n    switch (TargetVT) {\n    case MVT::i8:\n      Opcode = NVPTX::LD_i8_asi;\n      break;\n    case MVT::i16:\n      Opcode = NVPTX::LD_i16_asi;\n      break;\n    case MVT::i32:\n      Opcode = NVPTX::LD_i32_asi;\n      break;\n    case MVT::i64:\n      Opcode = NVPTX::LD_i64_asi;\n      break;\n    case MVT::f32:\n      Opcode = NVPTX::LD_f32_asi;\n      break;\n    case MVT::f64:\n      Opcode = NVPTX::LD_f64_asi;\n      break;\n    default:\n      return false;\n    }\n    SDValue Ops[] = { getI32Imm(isVolatile, dl), getI32Imm(codeAddrSpace, dl),\n                      getI32Imm(vecType, dl), getI32Imm(fromType, dl),\n                      getI32Imm(fromTypeWidth, dl), Base, Offset, Chain };\n    NVPTXLD = CurDAG->getMachineNode(Opcode, dl, TargetVT, MVT::Other, Ops);\n  } else if (TM.is64Bit() ? SelectADDRri64(N1.getNode(), N1, Base, Offset)\n                          : SelectADDRri(N1.getNode(), N1, Base, Offset)) {\n    if (TM.is64Bit()) {\n      switch (TargetVT) {\n      case MVT::i8:\n        Opcode = NVPTX::LD_i8_ari_64;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LD_i16_ari_64;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LD_i32_ari_64;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::LD_i64_ari_64;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LD_f32_ari_64;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::LD_f64_ari_64;\n        break;\n      default:\n        return false;\n      }\n    } else {\n      switch (TargetVT) {\n      case MVT::i8:\n        Opcode = NVPTX::LD_i8_ari;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LD_i16_ari;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LD_i32_ari;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::LD_i64_ari;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LD_f32_ari;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::LD_f64_ari;\n        break;\n      default:\n        return false;\n      }\n    }\n    SDValue Ops[] = { getI32Imm(isVolatile, dl), getI32Imm(codeAddrSpace, dl),\n                      getI32Imm(vecType, dl), getI32Imm(fromType, dl),\n                      getI32Imm(fromTypeWidth, dl), Base, Offset, Chain };\n    NVPTXLD = CurDAG->getMachineNode(Opcode, dl, TargetVT, MVT::Other, Ops);\n  } else {\n    if (TM.is64Bit()) {\n      switch (TargetVT) {\n      case MVT::i8:\n        Opcode = NVPTX::LD_i8_areg_64;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LD_i16_areg_64;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LD_i32_areg_64;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::LD_i64_areg_64;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LD_f32_areg_64;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::LD_f64_areg_64;\n        break;\n      default:\n        return false;\n      }\n    } else {\n      switch (TargetVT) {\n      case MVT::i8:\n        Opcode = NVPTX::LD_i8_areg;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::LD_i16_areg;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::LD_i32_areg;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::LD_i64_areg;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::LD_f32_areg;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::LD_f64_areg;\n        break;\n      default:\n        return false;\n      }\n    }\n    SDValue Ops[] = { getI32Imm(isVolatile, dl), getI32Imm(codeAddrSpace, dl),\n                      getI32Imm(vecType, dl), getI32Imm(fromType, dl),\n                      getI32Imm(fromTypeWidth, dl), N1, Chain };\n    NVPTXLD = CurDAG->getMachineNode(Opcode, dl, TargetVT, MVT::Other, Ops);\n  }\n\n  if (!NVPTXLD)\n    return false;\n\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = cast<MemSDNode>(N)->getMemOperand();\n  cast<MachineSDNode>(NVPTXLD)->setMemRefs(MemRefs0, MemRefs0 + 1);\n\n  ReplaceNode(N, NVPTXLD);\n  return true;\n}\n\n"},"Select":{"range":[108,523],"code":"void NVPTXDAGToDAGISel::Select(SDNode *N) {\n\n  if (N->isMachineOpcode()) {\n    N->setNodeId(-1);\n    return; // Already selected.\n  }\n\n  switch (N->getOpcode()) {\n  case ISD::LOAD:\n    if (tryLoad(N))\n      return;\n    break;\n  case ISD::STORE:\n    if (tryStore(N))\n      return;\n    break;\n  case NVPTXISD::LoadV2:\n  case NVPTXISD::LoadV4:\n    if (tryLoadVector(N))\n      return;\n    break;\n  case NVPTXISD::LDGV2:\n  case NVPTXISD::LDGV4:\n  case NVPTXISD::LDUV2:\n  case NVPTXISD::LDUV4:\n    if (tryLDGLDU(N))\n      return;\n    break;\n  case NVPTXISD::StoreV2:\n  case NVPTXISD::StoreV4:\n    if (tryStoreVector(N))\n      return;\n    break;\n  case NVPTXISD::LoadParam:\n  case NVPTXISD::LoadParamV2:\n  case NVPTXISD::LoadParamV4:\n    if (tryLoadParam(N))\n      return;\n    break;\n  case NVPTXISD::StoreRetval:\n  case NVPTXISD::StoreRetvalV2:\n  case NVPTXISD::StoreRetvalV4:\n    if (tryStoreRetval(N))\n      return;\n    break;\n  case NVPTXISD::StoreParam:\n  case NVPTXISD::StoreParamV2:\n  case NVPTXISD::StoreParamV4:\n  case NVPTXISD::StoreParamS32:\n  case NVPTXISD::StoreParamU32:\n    if (tryStoreParam(N))\n      return;\n    break;\n  case ISD::INTRINSIC_WO_CHAIN:\n    if (tryIntrinsicNoChain(N))\n      return;\n    break;\n  case ISD::INTRINSIC_W_CHAIN:\n    if (tryIntrinsicChain(N))\n      return;\n    break;\n  case NVPTXISD::Tex1DFloatS32:\n  case NVPTXISD::Tex1DFloatFloat:\n  case NVPTXISD::Tex1DFloatFloatLevel:\n  case NVPTXISD::Tex1DFloatFloatGrad:\n  case NVPTXISD::Tex1DS32S32:\n  case NVPTXISD::Tex1DS32Float:\n  case NVPTXISD::Tex1DS32FloatLevel:\n  case NVPTXISD::Tex1DS32FloatGrad:\n  case NVPTXISD::Tex1DU32S32:\n  case NVPTXISD::Tex1DU32Float:\n  case NVPTXISD::Tex1DU32FloatLevel:\n  case NVPTXISD::Tex1DU32FloatGrad:\n  case NVPTXISD::Tex1DArrayFloatS32:\n  case NVPTXISD::Tex1DArrayFloatFloat:\n  case NVPTXISD::Tex1DArrayFloatFloatLevel:\n  case NVPTXISD::Tex1DArrayFloatFloatGrad:\n  case NVPTXISD::Tex1DArrayS32S32:\n  case NVPTXISD::Tex1DArrayS32Float:\n  case NVPTXISD::Tex1DArrayS32FloatLevel:\n  case NVPTXISD::Tex1DArrayS32FloatGrad:\n  case NVPTXISD::Tex1DArrayU32S32:\n  case NVPTXISD::Tex1DArrayU32Float:\n  case NVPTXISD::Tex1DArrayU32FloatLevel:\n  case NVPTXISD::Tex1DArrayU32FloatGrad:\n  case NVPTXISD::Tex2DFloatS32:\n  case NVPTXISD::Tex2DFloatFloat:\n  case NVPTXISD::Tex2DFloatFloatLevel:\n  case NVPTXISD::Tex2DFloatFloatGrad:\n  case NVPTXISD::Tex2DS32S32:\n  case NVPTXISD::Tex2DS32Float:\n  case NVPTXISD::Tex2DS32FloatLevel:\n  case NVPTXISD::Tex2DS32FloatGrad:\n  case NVPTXISD::Tex2DU32S32:\n  case NVPTXISD::Tex2DU32Float:\n  case NVPTXISD::Tex2DU32FloatLevel:\n  case NVPTXISD::Tex2DU32FloatGrad:\n  case NVPTXISD::Tex2DArrayFloatS32:\n  case NVPTXISD::Tex2DArrayFloatFloat:\n  case NVPTXISD::Tex2DArrayFloatFloatLevel:\n  case NVPTXISD::Tex2DArrayFloatFloatGrad:\n  case NVPTXISD::Tex2DArrayS32S32:\n  case NVPTXISD::Tex2DArrayS32Float:\n  case NVPTXISD::Tex2DArrayS32FloatLevel:\n  case NVPTXISD::Tex2DArrayS32FloatGrad:\n  case NVPTXISD::Tex2DArrayU32S32:\n  case NVPTXISD::Tex2DArrayU32Float:\n  case NVPTXISD::Tex2DArrayU32FloatLevel:\n  case NVPTXISD::Tex2DArrayU32FloatGrad:\n  case NVPTXISD::Tex3DFloatS32:\n  case NVPTXISD::Tex3DFloatFloat:\n  case NVPTXISD::Tex3DFloatFloatLevel:\n  case NVPTXISD::Tex3DFloatFloatGrad:\n  case NVPTXISD::Tex3DS32S32:\n  case NVPTXISD::Tex3DS32Float:\n  case NVPTXISD::Tex3DS32FloatLevel:\n  case NVPTXISD::Tex3DS32FloatGrad:\n  case NVPTXISD::Tex3DU32S32:\n  case NVPTXISD::Tex3DU32Float:\n  case NVPTXISD::Tex3DU32FloatLevel:\n  case NVPTXISD::Tex3DU32FloatGrad:\n  case NVPTXISD::TexCubeFloatFloat:\n  case NVPTXISD::TexCubeFloatFloatLevel:\n  case NVPTXISD::TexCubeS32Float:\n  case NVPTXISD::TexCubeS32FloatLevel:\n  case NVPTXISD::TexCubeU32Float:\n  case NVPTXISD::TexCubeU32FloatLevel:\n  case NVPTXISD::TexCubeArrayFloatFloat:\n  case NVPTXISD::TexCubeArrayFloatFloatLevel:\n  case NVPTXISD::TexCubeArrayS32Float:\n  case NVPTXISD::TexCubeArrayS32FloatLevel:\n  case NVPTXISD::TexCubeArrayU32Float:\n  case NVPTXISD::TexCubeArrayU32FloatLevel:\n  case NVPTXISD::Tld4R2DFloatFloat:\n  case NVPTXISD::Tld4G2DFloatFloat:\n  case NVPTXISD::Tld4B2DFloatFloat:\n  case NVPTXISD::Tld4A2DFloatFloat:\n  case NVPTXISD::Tld4R2DS64Float:\n  case NVPTXISD::Tld4G2DS64Float:\n  case NVPTXISD::Tld4B2DS64Float:\n  case NVPTXISD::Tld4A2DS64Float:\n  case NVPTXISD::Tld4R2DU64Float:\n  case NVPTXISD::Tld4G2DU64Float:\n  case NVPTXISD::Tld4B2DU64Float:\n  case NVPTXISD::Tld4A2DU64Float:\n  case NVPTXISD::TexUnified1DFloatS32:\n  case NVPTXISD::TexUnified1DFloatFloat:\n  case NVPTXISD::TexUnified1DFloatFloatLevel:\n  case NVPTXISD::TexUnified1DFloatFloatGrad:\n  case NVPTXISD::TexUnified1DS32S32:\n  case NVPTXISD::TexUnified1DS32Float:\n  case NVPTXISD::TexUnified1DS32FloatLevel:\n  case NVPTXISD::TexUnified1DS32FloatGrad:\n  case NVPTXISD::TexUnified1DU32S32:\n  case NVPTXISD::TexUnified1DU32Float:\n  case NVPTXISD::TexUnified1DU32FloatLevel:\n  case NVPTXISD::TexUnified1DU32FloatGrad:\n  case NVPTXISD::TexUnified1DArrayFloatS32:\n  case NVPTXISD::TexUnified1DArrayFloatFloat:\n  case NVPTXISD::TexUnified1DArrayFloatFloatLevel:\n  case NVPTXISD::TexUnified1DArrayFloatFloatGrad:\n  case NVPTXISD::TexUnified1DArrayS32S32:\n  case NVPTXISD::TexUnified1DArrayS32Float:\n  case NVPTXISD::TexUnified1DArrayS32FloatLevel:\n  case NVPTXISD::TexUnified1DArrayS32FloatGrad:\n  case NVPTXISD::TexUnified1DArrayU32S32:\n  case NVPTXISD::TexUnified1DArrayU32Float:\n  case NVPTXISD::TexUnified1DArrayU32FloatLevel:\n  case NVPTXISD::TexUnified1DArrayU32FloatGrad:\n  case NVPTXISD::TexUnified2DFloatS32:\n  case NVPTXISD::TexUnified2DFloatFloat:\n  case NVPTXISD::TexUnified2DFloatFloatLevel:\n  case NVPTXISD::TexUnified2DFloatFloatGrad:\n  case NVPTXISD::TexUnified2DS32S32:\n  case NVPTXISD::TexUnified2DS32Float:\n  case NVPTXISD::TexUnified2DS32FloatLevel:\n  case NVPTXISD::TexUnified2DS32FloatGrad:\n  case NVPTXISD::TexUnified2DU32S32:\n  case NVPTXISD::TexUnified2DU32Float:\n  case NVPTXISD::TexUnified2DU32FloatLevel:\n  case NVPTXISD::TexUnified2DU32FloatGrad:\n  case NVPTXISD::TexUnified2DArrayFloatS32:\n  case NVPTXISD::TexUnified2DArrayFloatFloat:\n  case NVPTXISD::TexUnified2DArrayFloatFloatLevel:\n  case NVPTXISD::TexUnified2DArrayFloatFloatGrad:\n  case NVPTXISD::TexUnified2DArrayS32S32:\n  case NVPTXISD::TexUnified2DArrayS32Float:\n  case NVPTXISD::TexUnified2DArrayS32FloatLevel:\n  case NVPTXISD::TexUnified2DArrayS32FloatGrad:\n  case NVPTXISD::TexUnified2DArrayU32S32:\n  case NVPTXISD::TexUnified2DArrayU32Float:\n  case NVPTXISD::TexUnified2DArrayU32FloatLevel:\n  case NVPTXISD::TexUnified2DArrayU32FloatGrad:\n  case NVPTXISD::TexUnified3DFloatS32:\n  case NVPTXISD::TexUnified3DFloatFloat:\n  case NVPTXISD::TexUnified3DFloatFloatLevel:\n  case NVPTXISD::TexUnified3DFloatFloatGrad:\n  case NVPTXISD::TexUnified3DS32S32:\n  case NVPTXISD::TexUnified3DS32Float:\n  case NVPTXISD::TexUnified3DS32FloatLevel:\n  case NVPTXISD::TexUnified3DS32FloatGrad:\n  case NVPTXISD::TexUnified3DU32S32:\n  case NVPTXISD::TexUnified3DU32Float:\n  case NVPTXISD::TexUnified3DU32FloatLevel:\n  case NVPTXISD::TexUnified3DU32FloatGrad:\n  case NVPTXISD::TexUnifiedCubeFloatFloat:\n  case NVPTXISD::TexUnifiedCubeFloatFloatLevel:\n  case NVPTXISD::TexUnifiedCubeS32Float:\n  case NVPTXISD::TexUnifiedCubeS32FloatLevel:\n  case NVPTXISD::TexUnifiedCubeU32Float:\n  case NVPTXISD::TexUnifiedCubeU32FloatLevel:\n  case NVPTXISD::TexUnifiedCubeArrayFloatFloat:\n  case NVPTXISD::TexUnifiedCubeArrayFloatFloatLevel:\n  case NVPTXISD::TexUnifiedCubeArrayS32Float:\n  case NVPTXISD::TexUnifiedCubeArrayS32FloatLevel:\n  case NVPTXISD::TexUnifiedCubeArrayU32Float:\n  case NVPTXISD::TexUnifiedCubeArrayU32FloatLevel:\n  case NVPTXISD::Tld4UnifiedR2DFloatFloat:\n  case NVPTXISD::Tld4UnifiedG2DFloatFloat:\n  case NVPTXISD::Tld4UnifiedB2DFloatFloat:\n  case NVPTXISD::Tld4UnifiedA2DFloatFloat:\n  case NVPTXISD::Tld4UnifiedR2DS64Float:\n  case NVPTXISD::Tld4UnifiedG2DS64Float:\n  case NVPTXISD::Tld4UnifiedB2DS64Float:\n  case NVPTXISD::Tld4UnifiedA2DS64Float:\n  case NVPTXISD::Tld4UnifiedR2DU64Float:\n  case NVPTXISD::Tld4UnifiedG2DU64Float:\n  case NVPTXISD::Tld4UnifiedB2DU64Float:\n  case NVPTXISD::Tld4UnifiedA2DU64Float:\n    if (tryTextureIntrinsic(N))\n      return;\n    break;\n  case NVPTXISD::Suld1DI8Clamp:\n  case NVPTXISD::Suld1DI16Clamp:\n  case NVPTXISD::Suld1DI32Clamp:\n  case NVPTXISD::Suld1DI64Clamp:\n  case NVPTXISD::Suld1DV2I8Clamp:\n  case NVPTXISD::Suld1DV2I16Clamp:\n  case NVPTXISD::Suld1DV2I32Clamp:\n  case NVPTXISD::Suld1DV2I64Clamp:\n  case NVPTXISD::Suld1DV4I8Clamp:\n  case NVPTXISD::Suld1DV4I16Clamp:\n  case NVPTXISD::Suld1DV4I32Clamp:\n  case NVPTXISD::Suld1DArrayI8Clamp:\n  case NVPTXISD::Suld1DArrayI16Clamp:\n  case NVPTXISD::Suld1DArrayI32Clamp:\n  case NVPTXISD::Suld1DArrayI64Clamp:\n  case NVPTXISD::Suld1DArrayV2I8Clamp:\n  case NVPTXISD::Suld1DArrayV2I16Clamp:\n  case NVPTXISD::Suld1DArrayV2I32Clamp:\n  case NVPTXISD::Suld1DArrayV2I64Clamp:\n  case NVPTXISD::Suld1DArrayV4I8Clamp:\n  case NVPTXISD::Suld1DArrayV4I16Clamp:\n  case NVPTXISD::Suld1DArrayV4I32Clamp:\n  case NVPTXISD::Suld2DI8Clamp:\n  case NVPTXISD::Suld2DI16Clamp:\n  case NVPTXISD::Suld2DI32Clamp:\n  case NVPTXISD::Suld2DI64Clamp:\n  case NVPTXISD::Suld2DV2I8Clamp:\n  case NVPTXISD::Suld2DV2I16Clamp:\n  case NVPTXISD::Suld2DV2I32Clamp:\n  case NVPTXISD::Suld2DV2I64Clamp:\n  case NVPTXISD::Suld2DV4I8Clamp:\n  case NVPTXISD::Suld2DV4I16Clamp:\n  case NVPTXISD::Suld2DV4I32Clamp:\n  case NVPTXISD::Suld2DArrayI8Clamp:\n  case NVPTXISD::Suld2DArrayI16Clamp:\n  case NVPTXISD::Suld2DArrayI32Clamp:\n  case NVPTXISD::Suld2DArrayI64Clamp:\n  case NVPTXISD::Suld2DArrayV2I8Clamp:\n  case NVPTXISD::Suld2DArrayV2I16Clamp:\n  case NVPTXISD::Suld2DArrayV2I32Clamp:\n  case NVPTXISD::Suld2DArrayV2I64Clamp:\n  case NVPTXISD::Suld2DArrayV4I8Clamp:\n  case NVPTXISD::Suld2DArrayV4I16Clamp:\n  case NVPTXISD::Suld2DArrayV4I32Clamp:\n  case NVPTXISD::Suld3DI8Clamp:\n  case NVPTXISD::Suld3DI16Clamp:\n  case NVPTXISD::Suld3DI32Clamp:\n  case NVPTXISD::Suld3DI64Clamp:\n  case NVPTXISD::Suld3DV2I8Clamp:\n  case NVPTXISD::Suld3DV2I16Clamp:\n  case NVPTXISD::Suld3DV2I32Clamp:\n  case NVPTXISD::Suld3DV2I64Clamp:\n  case NVPTXISD::Suld3DV4I8Clamp:\n  case NVPTXISD::Suld3DV4I16Clamp:\n  case NVPTXISD::Suld3DV4I32Clamp:\n  case NVPTXISD::Suld1DI8Trap:\n  case NVPTXISD::Suld1DI16Trap:\n  case NVPTXISD::Suld1DI32Trap:\n  case NVPTXISD::Suld1DI64Trap:\n  case NVPTXISD::Suld1DV2I8Trap:\n  case NVPTXISD::Suld1DV2I16Trap:\n  case NVPTXISD::Suld1DV2I32Trap:\n  case NVPTXISD::Suld1DV2I64Trap:\n  case NVPTXISD::Suld1DV4I8Trap:\n  case NVPTXISD::Suld1DV4I16Trap:\n  case NVPTXISD::Suld1DV4I32Trap:\n  case NVPTXISD::Suld1DArrayI8Trap:\n  case NVPTXISD::Suld1DArrayI16Trap:\n  case NVPTXISD::Suld1DArrayI32Trap:\n  case NVPTXISD::Suld1DArrayI64Trap:\n  case NVPTXISD::Suld1DArrayV2I8Trap:\n  case NVPTXISD::Suld1DArrayV2I16Trap:\n  case NVPTXISD::Suld1DArrayV2I32Trap:\n  case NVPTXISD::Suld1DArrayV2I64Trap:\n  case NVPTXISD::Suld1DArrayV4I8Trap:\n  case NVPTXISD::Suld1DArrayV4I16Trap:\n  case NVPTXISD::Suld1DArrayV4I32Trap:\n  case NVPTXISD::Suld2DI8Trap:\n  case NVPTXISD::Suld2DI16Trap:\n  case NVPTXISD::Suld2DI32Trap:\n  case NVPTXISD::Suld2DI64Trap:\n  case NVPTXISD::Suld2DV2I8Trap:\n  case NVPTXISD::Suld2DV2I16Trap:\n  case NVPTXISD::Suld2DV2I32Trap:\n  case NVPTXISD::Suld2DV2I64Trap:\n  case NVPTXISD::Suld2DV4I8Trap:\n  case NVPTXISD::Suld2DV4I16Trap:\n  case NVPTXISD::Suld2DV4I32Trap:\n  case NVPTXISD::Suld2DArrayI8Trap:\n  case NVPTXISD::Suld2DArrayI16Trap:\n  case NVPTXISD::Suld2DArrayI32Trap:\n  case NVPTXISD::Suld2DArrayI64Trap:\n  case NVPTXISD::Suld2DArrayV2I8Trap:\n  case NVPTXISD::Suld2DArrayV2I16Trap:\n  case NVPTXISD::Suld2DArrayV2I32Trap:\n  case NVPTXISD::Suld2DArrayV2I64Trap:\n  case NVPTXISD::Suld2DArrayV4I8Trap:\n  case NVPTXISD::Suld2DArrayV4I16Trap:\n  case NVPTXISD::Suld2DArrayV4I32Trap:\n  case NVPTXISD::Suld3DI8Trap:\n  case NVPTXISD::Suld3DI16Trap:\n  case NVPTXISD::Suld3DI32Trap:\n  case NVPTXISD::Suld3DI64Trap:\n  case NVPTXISD::Suld3DV2I8Trap:\n  case NVPTXISD::Suld3DV2I16Trap:\n  case NVPTXISD::Suld3DV2I32Trap:\n  case NVPTXISD::Suld3DV2I64Trap:\n  case NVPTXISD::Suld3DV4I8Trap:\n  case NVPTXISD::Suld3DV4I16Trap:\n  case NVPTXISD::Suld3DV4I32Trap:\n  case NVPTXISD::Suld1DI8Zero:\n  case NVPTXISD::Suld1DI16Zero:\n  case NVPTXISD::Suld1DI32Zero:\n  case NVPTXISD::Suld1DI64Zero:\n  case NVPTXISD::Suld1DV2I8Zero:\n  case NVPTXISD::Suld1DV2I16Zero:\n  case NVPTXISD::Suld1DV2I32Zero:\n  case NVPTXISD::Suld1DV2I64Zero:\n  case NVPTXISD::Suld1DV4I8Zero:\n  case NVPTXISD::Suld1DV4I16Zero:\n  case NVPTXISD::Suld1DV4I32Zero:\n  case NVPTXISD::Suld1DArrayI8Zero:\n  case NVPTXISD::Suld1DArrayI16Zero:\n  case NVPTXISD::Suld1DArrayI32Zero:\n  case NVPTXISD::Suld1DArrayI64Zero:\n  case NVPTXISD::Suld1DArrayV2I8Zero:\n  case NVPTXISD::Suld1DArrayV2I16Zero:\n  case NVPTXISD::Suld1DArrayV2I32Zero:\n  case NVPTXISD::Suld1DArrayV2I64Zero:\n  case NVPTXISD::Suld1DArrayV4I8Zero:\n  case NVPTXISD::Suld1DArrayV4I16Zero:\n  case NVPTXISD::Suld1DArrayV4I32Zero:\n  case NVPTXISD::Suld2DI8Zero:\n  case NVPTXISD::Suld2DI16Zero:\n  case NVPTXISD::Suld2DI32Zero:\n  case NVPTXISD::Suld2DI64Zero:\n  case NVPTXISD::Suld2DV2I8Zero:\n  case NVPTXISD::Suld2DV2I16Zero:\n  case NVPTXISD::Suld2DV2I32Zero:\n  case NVPTXISD::Suld2DV2I64Zero:\n  case NVPTXISD::Suld2DV4I8Zero:\n  case NVPTXISD::Suld2DV4I16Zero:\n  case NVPTXISD::Suld2DV4I32Zero:\n  case NVPTXISD::Suld2DArrayI8Zero:\n  case NVPTXISD::Suld2DArrayI16Zero:\n  case NVPTXISD::Suld2DArrayI32Zero:\n  case NVPTXISD::Suld2DArrayI64Zero:\n  case NVPTXISD::Suld2DArrayV2I8Zero:\n  case NVPTXISD::Suld2DArrayV2I16Zero:\n  case NVPTXISD::Suld2DArrayV2I32Zero:\n  case NVPTXISD::Suld2DArrayV2I64Zero:\n  case NVPTXISD::Suld2DArrayV4I8Zero:\n  case NVPTXISD::Suld2DArrayV4I16Zero:\n  case NVPTXISD::Suld2DArrayV4I32Zero:\n  case NVPTXISD::Suld3DI8Zero:\n  case NVPTXISD::Suld3DI16Zero:\n  case NVPTXISD::Suld3DI32Zero:\n  case NVPTXISD::Suld3DI64Zero:\n  case NVPTXISD::Suld3DV2I8Zero:\n  case NVPTXISD::Suld3DV2I16Zero:\n  case NVPTXISD::Suld3DV2I32Zero:\n  case NVPTXISD::Suld3DV2I64Zero:\n  case NVPTXISD::Suld3DV4I8Zero:\n  case NVPTXISD::Suld3DV4I16Zero:\n  case NVPTXISD::Suld3DV4I32Zero:\n    if (trySurfaceIntrinsic(N))\n      return;\n    break;\n  case ISD::AND:\n  case ISD::SRA:\n  case ISD::SRL:\n    // Try to select BFE\n    if (tryBFE(N))\n      return;\n    break;\n  case ISD::ADDRSPACECAST:\n    SelectAddrSpaceCast(N);\n    return;\n  default:\n    break;\n  }\n  SelectCode(N);\n}\n\n"},"tryStoreParam":{"range":[2978,3134],"code":"bool NVPTXDAGToDAGISel::tryStoreParam(SDNode *N) {\n  SDLoc DL(N);\n  SDValue Chain = N->getOperand(0);\n  SDValue Param = N->getOperand(1);\n  unsigned ParamVal = cast<ConstantSDNode>(Param)->getZExtValue();\n  SDValue Offset = N->getOperand(2);\n  unsigned OffsetVal = cast<ConstantSDNode>(Offset)->getZExtValue();\n  MemSDNode *Mem = cast<MemSDNode>(N);\n  SDValue Flag = N->getOperand(N->getNumOperands() - 1);\n\n  // How many elements do we have?\n  unsigned NumElts = 1;\n  switch (N->getOpcode()) {\n  default:\n    return false;\n  case NVPTXISD::StoreParamU32:\n  case NVPTXISD::StoreParamS32:\n  case NVPTXISD::StoreParam:\n    NumElts = 1;\n    break;\n  case NVPTXISD::StoreParamV2:\n    NumElts = 2;\n    break;\n  case NVPTXISD::StoreParamV4:\n    NumElts = 4;\n    break;\n  }\n\n  // Build vector of operands\n  SmallVector<SDValue, 8> Ops;\n  for (unsigned i = 0; i < NumElts; ++i)\n    Ops.push_back(N->getOperand(i + 3));\n  Ops.push_back(CurDAG->getTargetConstant(ParamVal, DL, MVT::i32));\n  Ops.push_back(CurDAG->getTargetConstant(OffsetVal, DL, MVT::i32));\n  Ops.push_back(Chain);\n  Ops.push_back(Flag);\n\n  // Determine target opcode\n  // If we have an i1, use an 8-bit store. The lowering code in\n  // NVPTXISelLowering will have already emitted an upcast.\n  unsigned Opcode = 0;\n  switch (N->getOpcode()) {\n  default:\n    switch (NumElts) {\n    default:\n      return false;\n    case 1:\n      switch (Mem->getMemoryVT().getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i1:\n        Opcode = NVPTX::StoreParamI8;\n        break;\n      case MVT::i8:\n        Opcode = NVPTX::StoreParamI8;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::StoreParamI16;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::StoreParamI32;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::StoreParamI64;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::StoreParamF32;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::StoreParamF64;\n        break;\n      }\n      break;\n    case 2:\n      switch (Mem->getMemoryVT().getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i1:\n        Opcode = NVPTX::StoreParamV2I8;\n        break;\n      case MVT::i8:\n        Opcode = NVPTX::StoreParamV2I8;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::StoreParamV2I16;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::StoreParamV2I32;\n        break;\n      case MVT::i64:\n        Opcode = NVPTX::StoreParamV2I64;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::StoreParamV2F32;\n        break;\n      case MVT::f64:\n        Opcode = NVPTX::StoreParamV2F64;\n        break;\n      }\n      break;\n    case 4:\n      switch (Mem->getMemoryVT().getSimpleVT().SimpleTy) {\n      default:\n        return false;\n      case MVT::i1:\n        Opcode = NVPTX::StoreParamV4I8;\n        break;\n      case MVT::i8:\n        Opcode = NVPTX::StoreParamV4I8;\n        break;\n      case MVT::i16:\n        Opcode = NVPTX::StoreParamV4I16;\n        break;\n      case MVT::i32:\n        Opcode = NVPTX::StoreParamV4I32;\n        break;\n      case MVT::f32:\n        Opcode = NVPTX::StoreParamV4F32;\n        break;\n      }\n      break;\n    }\n    break;\n  // Special case: if we have a sign-extend/zero-extend node, insert the\n  // conversion instruction first, and use that as the value operand to\n  // the selected StoreParam node.\n  case NVPTXISD::StoreParamU32: {\n    Opcode = NVPTX::StoreParamI32;\n    SDValue CvtNone = CurDAG->getTargetConstant(NVPTX::PTXCvtMode::NONE, DL,\n                                                MVT::i32);\n    SDNode *Cvt = CurDAG->getMachineNode(NVPTX::CVT_u32_u16, DL,\n                                         MVT::i32, Ops[0], CvtNone);\n    Ops[0] = SDValue(Cvt, 0);\n    break;\n  }\n  case NVPTXISD::StoreParamS32: {\n    Opcode = NVPTX::StoreParamI32;\n    SDValue CvtNone = CurDAG->getTargetConstant(NVPTX::PTXCvtMode::NONE, DL,\n                                                MVT::i32);\n    SDNode *Cvt = CurDAG->getMachineNode(NVPTX::CVT_s32_s16, DL,\n                                         MVT::i32, Ops[0], CvtNone);\n    Ops[0] = SDValue(Cvt, 0);\n    break;\n  }\n  }\n\n  SDVTList RetVTs = CurDAG->getVTList(MVT::Other, MVT::Glue);\n  SDNode *Ret =\n      CurDAG->getMachineNode(Opcode, DL, RetVTs, Ops);\n  MachineSDNode::mmo_iterator MemRefs0 = MF->allocateMemRefsArray(1);\n  MemRefs0[0] = cast<MemSDNode>(N)->getMemOperand();\n  cast<MachineSDNode>(Ret)->setMemRefs(MemRefs0, MemRefs0 + 1);\n\n  ReplaceNode(N, Ret);\n  return true;\n}\n\n"},"SelectADDRsi":{"range":[5105,5110],"code":"bool NVPTXDAGToDAGISel::SelectADDRsi(SDNode *OpNode, SDValue Addr,\n                                     SDValue &Base, SDValue &Offset) {\n  return SelectADDRsi_imp(OpNode, Addr, Base, Offset, MVT::i32);\n}\n\n// symbol+offset\n"}}},"NVPTXFrameLowering.cpp":{"path":"NVPTXFrameLowering.cpp","size":2958,"lines":78,"functions":{"NVPTXFrameLowering":{"range":[28,30],"code":"NVPTXFrameLowering::NVPTXFrameLowering()\n    : TargetFrameLowering(TargetFrameLowering::StackGrowsUp, 8, 0) {}\n\n"},"hasFP":{"range":[31,32],"code":"bool NVPTXFrameLowering::hasFP(const MachineFunction &MF) const { return true; }\n\n"},"emitEpilogue":{"range":[67,71],"code":"void NVPTXFrameLowering::emitEpilogue(MachineFunction &MF,\n                                      MachineBasicBlock &MBB) const {}\n\n// This function eliminates ADJCALLSTACKDOWN,\n// ADJCALLSTACKUP pseudo instructions\n"},"emitPrologue":{"range":[33,66],"code":"void NVPTXFrameLowering::emitPrologue(MachineFunction &MF,\n                                      MachineBasicBlock &MBB) const {\n  if (MF.getFrameInfo()->hasStackObjects()) {\n    assert(&MF.front() == &MBB && \"Shrink-wrapping not yet supported\");\n    MachineInstr *MI = &MBB.front();\n    MachineRegisterInfo &MR = MF.getRegInfo();\n\n    // This instruction really occurs before first instruction\n    // in the BB, so giving it no debug location.\n    DebugLoc dl = DebugLoc();\n\n    // Emits\n    //   mov %SPL, %depot;\n    //   cvta.local %SP, %SPL;\n    // for local address accesses in MF.\n    bool Is64Bit =\n        static_cast<const NVPTXTargetMachine &>(MF.getTarget()).is64Bit();\n    unsigned CvtaLocalOpcode =\n        (Is64Bit ? NVPTX::cvta_local_yes_64 : NVPTX::cvta_local_yes);\n    unsigned MovDepotOpcode =\n        (Is64Bit ? NVPTX::MOV_DEPOT_ADDR_64 : NVPTX::MOV_DEPOT_ADDR);\n    if (!MR.use_empty(NVPTX::VRFrame)) {\n      // If %SP is not used, do not bother emitting \"cvta.local %SP, %SPL\".\n      MI = BuildMI(MBB, MI, dl,\n                   MF.getSubtarget().getInstrInfo()->get(CvtaLocalOpcode),\n                   NVPTX::VRFrame)\n               .addReg(NVPTX::VRFrameLocal);\n    }\n    BuildMI(MBB, MI, dl, MF.getSubtarget().getInstrInfo()->get(MovDepotOpcode),\n            NVPTX::VRFrameLocal)\n        .addImm(MF.getFunctionNumber());\n  }\n}\n\n"},"eliminateCallFramePseudoInstr":{"range":[72,79],"code":"MachineBasicBlock::iterator NVPTXFrameLowering::eliminateCallFramePseudoInstr(\n    MachineFunction &MF, MachineBasicBlock &MBB,\n    MachineBasicBlock::iterator I) const {\n  // Simply discard ADJCALLSTACKDOWN,\n  // ADJCALLSTACKUP instructions.\n  return MBB.erase(I);\n}\n"}}},"NVPTXGenericToNVVM.cpp":{"path":"NVPTXGenericToNVVM.cpp","size":15234,"lines":391,"functions":{"GenericToNVVM":{"range":[42,68],"code":"  GenericToNVVM() : ModulePass(ID) {}\n\n  bool runOnModule(Module &M) override;\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override {}\n\nprivate:\n  Value *getOrInsertCVTA(Module *M, Function *F, GlobalVariable *GV,\n                         IRBuilder<> &Builder);\n  Value *remapConstant(Module *M, Function *F, Constant *C,\n                       IRBuilder<> &Builder);\n  Value *remapConstantVectorOrConstantAggregate(Module *M, Function *F,\n                                                Constant *C,\n                                                IRBuilder<> &Builder);\n  Value *remapConstantExpr(Module *M, Function *F, ConstantExpr *C,\n                           IRBuilder<> &Builder);\n  void remapNamedMDNode(ValueToValueMapTy &VM, NamedMDNode *N);\n\n  typedef ValueMap<GlobalVariable *, GlobalVariable *> GVMapTy;\n  typedef ValueMap<Constant *, Value *> ConstantToValueMapTy;\n  GVMapTy GVMap;\n  ConstantToValueMapTy ConstantToValueMap;\n};\n} // end namespace\n\nchar GenericToNVVM::ID = 0;\n\n"},"runOnModule":{"range":[76,166],"code":"bool GenericToNVVM::runOnModule(Module &M) {\n  // Create a clone of each global variable that has the default address space.\n  // The clone is created with the global address space  specifier, and the pair\n  // of original global variable and its clone is placed in the GVMap for later\n  // use.\n\n  for (Module::global_iterator I = M.global_begin(), E = M.global_end();\n       I != E;) {\n    GlobalVariable *GV = &*I++;\n    if (GV->getType()->getAddressSpace() == llvm::ADDRESS_SPACE_GENERIC &&\n        !llvm::isTexture(*GV) && !llvm::isSurface(*GV) &&\n        !llvm::isSampler(*GV) && !GV->getName().startswith(\"llvm.\")) {\n      GlobalVariable *NewGV = new GlobalVariable(\n          M, GV->getValueType(), GV->isConstant(),\n          GV->getLinkage(),\n          GV->hasInitializer() ? GV->getInitializer() : nullptr,\n          \"\", GV, GV->getThreadLocalMode(), llvm::ADDRESS_SPACE_GLOBAL);\n      NewGV->copyAttributesFrom(GV);\n      GVMap[GV] = NewGV;\n    }\n  }\n\n  // Return immediately, if every global variable has a specific address space\n  // specifier.\n  if (GVMap.empty()) {\n    return false;\n  }\n\n  // Walk through the instructions in function defitinions, and replace any use\n  // of original global variables in GVMap with a use of the corresponding\n  // copies in GVMap.  If necessary, promote constants to instructions.\n  for (Module::iterator I = M.begin(), E = M.end(); I != E; ++I) {\n    if (I->isDeclaration()) {\n      continue;\n    }\n    IRBuilder<> Builder(I->getEntryBlock().getFirstNonPHIOrDbg());\n    for (Function::iterator BBI = I->begin(), BBE = I->end(); BBI != BBE;\n         ++BBI) {\n      for (BasicBlock::iterator II = BBI->begin(), IE = BBI->end(); II != IE;\n           ++II) {\n        for (unsigned i = 0, e = II->getNumOperands(); i < e; ++i) {\n          Value *Operand = II->getOperand(i);\n          if (isa<Constant>(Operand)) {\n            II->setOperand(\n                i, remapConstant(&M, &*I, cast<Constant>(Operand), Builder));\n          }\n        }\n      }\n    }\n    ConstantToValueMap.clear();\n  }\n\n  // Copy GVMap over to a standard value map.\n  ValueToValueMapTy VM;\n  for (auto I = GVMap.begin(), E = GVMap.end(); I != E; ++I)\n    VM[I->first] = I->second;\n\n  // Walk through the metadata section and update the debug information\n  // associated with the global variables in the default address space.\n  for (NamedMDNode &I : M.named_metadata()) {\n    remapNamedMDNode(VM, &I);\n  }\n\n  // Walk through the global variable  initializers, and replace any use of\n  // original global variables in GVMap with a use of the corresponding copies\n  // in GVMap.  The copies need to be bitcast to the original global variable\n  // types, as we cannot use cvta in global variable initializers.\n  for (GVMapTy::iterator I = GVMap.begin(), E = GVMap.end(); I != E;) {\n    GlobalVariable *GV = I->first;\n    GlobalVariable *NewGV = I->second;\n\n    // Remove GV from the map so that it can be RAUWed.  Note that\n    // DenseMap::erase() won't invalidate any iterators but this one.\n    auto Next = std::next(I);\n    GVMap.erase(I);\n    I = Next;\n\n    Constant *BitCastNewGV = ConstantExpr::getPointerCast(NewGV, GV->getType());\n    // At this point, the remaining uses of GV should be found only in global\n    // variable initializers, as other uses have been already been removed\n    // while walking through the instructions in function definitions.\n    GV->replaceAllUsesWith(BitCastNewGV);\n    std::string Name = GV->getName();\n    GV->eraseFromParent();\n    NewGV->setName(Name);\n  }\n  assert(GVMap.empty() && \"Expected it to be empty by now\");\n\n  return true;\n}\n\n"},"createGenericToNVVMPass":{"range":[69,75],"code":"ModulePass *llvm::createGenericToNVVMPass() { return new GenericToNVVM(); }\n\nINITIALIZE_PASS(\n    GenericToNVVM, \"generic-to-nvvm\",\n    \"Ensure that the global variables are in the global address space\", false,\n    false)\n\n"},"getOrInsertCVTA":{"range":[167,206],"code":"Value *GenericToNVVM::getOrInsertCVTA(Module *M, Function *F,\n                                      GlobalVariable *GV,\n                                      IRBuilder<> &Builder) {\n  PointerType *GVType = GV->getType();\n  Value *CVTA = nullptr;\n\n  // See if the address space conversion requires the operand to be bitcast\n  // to i8 addrspace(n)* first.\n  EVT ExtendedGVType = EVT::getEVT(GV->getValueType(), true);\n  if (!ExtendedGVType.isInteger() && !ExtendedGVType.isFloatingPoint()) {\n    // A bitcast to i8 addrspace(n)* on the operand is needed.\n    LLVMContext &Context = M->getContext();\n    unsigned int AddrSpace = GVType->getAddressSpace();\n    Type *DestTy = PointerType::get(Type::getInt8Ty(Context), AddrSpace);\n    CVTA = Builder.CreateBitCast(GV, DestTy, \"cvta\");\n    // Insert the address space conversion.\n    Type *ResultType =\n        PointerType::get(Type::getInt8Ty(Context), llvm::ADDRESS_SPACE_GENERIC);\n    Function *CVTAFunction = Intrinsic::getDeclaration(\n        M, Intrinsic::nvvm_ptr_global_to_gen, {ResultType, DestTy});\n    CVTA = Builder.CreateCall(CVTAFunction, CVTA, \"cvta\");\n    // Another bitcast from i8 * to <the element type of GVType> * is\n    // required.\n    DestTy =\n        PointerType::get(GV->getValueType(), llvm::ADDRESS_SPACE_GENERIC);\n    CVTA = Builder.CreateBitCast(CVTA, DestTy, \"cvta\");\n  } else {\n    // A simple CVTA is enough.\n    SmallVector<Type *, 2> ParamTypes;\n    ParamTypes.push_back(PointerType::get(GV->getValueType(),\n                                          llvm::ADDRESS_SPACE_GENERIC));\n    ParamTypes.push_back(GVType);\n    Function *CVTAFunction = Intrinsic::getDeclaration(\n        M, Intrinsic::nvvm_ptr_global_to_gen, ParamTypes);\n    CVTA = Builder.CreateCall(CVTAFunction, GV, \"cvta\");\n  }\n\n  return CVTA;\n}\n\n"},"remapConstantExpr":{"range":[285,363],"code":"Value *GenericToNVVM::remapConstantExpr(Module *M, Function *F, ConstantExpr *C,\n                                        IRBuilder<> &Builder) {\n  bool OperandChanged = false;\n  SmallVector<Value *, 4> NewOperands;\n  unsigned NumOperands = C->getNumOperands();\n\n  // Check if any operand is or uses a global variable in  GVMap, and thus\n  // converted to another value.\n  for (unsigned i = 0; i < NumOperands; ++i) {\n    Value *Operand = C->getOperand(i);\n    Value *NewOperand = remapConstant(M, F, cast<Constant>(Operand), Builder);\n    OperandChanged |= Operand != NewOperand;\n    NewOperands.push_back(NewOperand);\n  }\n\n  // If none of the operands has been modified, return C as it is.\n  if (!OperandChanged) {\n    return C;\n  }\n\n  // If any of the operands has been modified, construct the instruction with\n  // the converted operands.\n  unsigned Opcode = C->getOpcode();\n  switch (Opcode) {\n  case Instruction::ICmp:\n    // CompareConstantExpr (icmp)\n    return Builder.CreateICmp(CmpInst::Predicate(C->getPredicate()),\n                              NewOperands[0], NewOperands[1]);\n  case Instruction::FCmp:\n    // CompareConstantExpr (fcmp)\n    llvm_unreachable(\"Address space conversion should have no effect \"\n                     \"on float point CompareConstantExpr (fcmp)!\");\n  case Instruction::ExtractElement:\n    // ExtractElementConstantExpr\n    return Builder.CreateExtractElement(NewOperands[0], NewOperands[1]);\n  case Instruction::InsertElement:\n    // InsertElementConstantExpr\n    return Builder.CreateInsertElement(NewOperands[0], NewOperands[1],\n                                       NewOperands[2]);\n  case Instruction::ShuffleVector:\n    // ShuffleVector\n    return Builder.CreateShuffleVector(NewOperands[0], NewOperands[1],\n                                       NewOperands[2]);\n  case Instruction::ExtractValue:\n    // ExtractValueConstantExpr\n    return Builder.CreateExtractValue(NewOperands[0], C->getIndices());\n  case Instruction::InsertValue:\n    // InsertValueConstantExpr\n    return Builder.CreateInsertValue(NewOperands[0], NewOperands[1],\n                                     C->getIndices());\n  case Instruction::GetElementPtr:\n    // GetElementPtrConstantExpr\n    return cast<GEPOperator>(C)->isInBounds()\n               ? Builder.CreateGEP(\n                     cast<GEPOperator>(C)->getSourceElementType(),\n                     NewOperands[0],\n                     makeArrayRef(&NewOperands[1], NumOperands - 1))\n               : Builder.CreateInBoundsGEP(\n                     cast<GEPOperator>(C)->getSourceElementType(),\n                     NewOperands[0],\n                     makeArrayRef(&NewOperands[1], NumOperands - 1));\n  case Instruction::Select:\n    // SelectConstantExpr\n    return Builder.CreateSelect(NewOperands[0], NewOperands[1], NewOperands[2]);\n  default:\n    // BinaryConstantExpr\n    if (Instruction::isBinaryOp(Opcode)) {\n      return Builder.CreateBinOp(Instruction::BinaryOps(C->getOpcode()),\n                                 NewOperands[0], NewOperands[1]);\n    }\n    // UnaryConstantExpr\n    if (Instruction::isCast(Opcode)) {\n      return Builder.CreateCast(Instruction::CastOps(C->getOpcode()),\n                                NewOperands[0], C->getType());\n    }\n    llvm_unreachable(\"GenericToNVVM encountered an unsupported ConstantExpr\");\n  }\n}\n\n"},"remapConstant":{"range":[207,245],"code":"Value *GenericToNVVM::remapConstant(Module *M, Function *F, Constant *C,\n                                    IRBuilder<> &Builder) {\n  // If the constant C has been converted already in the given function  F, just\n  // return the converted value.\n  ConstantToValueMapTy::iterator CTII = ConstantToValueMap.find(C);\n  if (CTII != ConstantToValueMap.end()) {\n    return CTII->second;\n  }\n\n  Value *NewValue = C;\n  if (isa<GlobalVariable>(C)) {\n    // If the constant C is a global variable and is found in  GVMap, generate a\n    // set set of instructions that convert the clone of C with the global\n    // address space specifier to a generic pointer.\n    // The constant C cannot be used here, as it will be erased from the\n    // module eventually.  And the clone of C with the global address space\n    // specifier cannot be used here either, as it will affect the types of\n    // other instructions in the function.  Hence, this address space conversion\n    // is required.\n    GVMapTy::iterator I = GVMap.find(cast<GlobalVariable>(C));\n    if (I != GVMap.end()) {\n      NewValue = getOrInsertCVTA(M, F, I->second, Builder);\n    }\n  } else if (isa<ConstantAggregate>(C)) {\n    // If any element in the constant vector or aggregate C is or uses a global\n    // variable in GVMap, the constant C needs to be reconstructed, using a set\n    // of instructions.\n    NewValue = remapConstantVectorOrConstantAggregate(M, F, C, Builder);\n  } else if (isa<ConstantExpr>(C)) {\n    // If any operand in the constant expression C is or uses a global variable\n    // in GVMap, the constant expression C needs to be reconstructed, using a\n    // set of instructions.\n    NewValue = remapConstantExpr(M, F, cast<ConstantExpr>(C), Builder);\n  }\n\n  ConstantToValueMap[C] = NewValue;\n  return NewValue;\n}\n\n"},"remapNamedMDNode":{"range":[364,392],"code":"void GenericToNVVM::remapNamedMDNode(ValueToValueMapTy &VM, NamedMDNode *N) {\n\n  bool OperandChanged = false;\n  SmallVector<MDNode *, 16> NewOperands;\n  unsigned NumOperands = N->getNumOperands();\n\n  // Check if any operand is or contains a global variable in  GVMap, and thus\n  // converted to another value.\n  for (unsigned i = 0; i < NumOperands; ++i) {\n    MDNode *Operand = N->getOperand(i);\n    MDNode *NewOperand = MapMetadata(Operand, VM);\n    OperandChanged |= Operand != NewOperand;\n    NewOperands.push_back(NewOperand);\n  }\n\n  // If none of the operands has been modified, return immediately.\n  if (!OperandChanged) {\n    return;\n  }\n\n  // Replace the old operands with the new operands.\n  N->dropAllReferences();\n  for (SmallVectorImpl<MDNode *>::iterator I = NewOperands.begin(),\n                                           E = NewOperands.end();\n       I != E; ++I) {\n    N->addOperand(*I);\n  }\n}\n"},"remapConstantVectorOrConstantAggregate":{"range":[246,284],"code":"Value *GenericToNVVM::remapConstantVectorOrConstantAggregate(\n    Module *M, Function *F, Constant *C, IRBuilder<> &Builder) {\n  bool OperandChanged = false;\n  SmallVector<Value *, 4> NewOperands;\n  unsigned NumOperands = C->getNumOperands();\n\n  // Check if any element is or uses a global variable in  GVMap, and thus\n  // converted to another value.\n  for (unsigned i = 0; i < NumOperands; ++i) {\n    Value *Operand = C->getOperand(i);\n    Value *NewOperand = remapConstant(M, F, cast<Constant>(Operand), Builder);\n    OperandChanged |= Operand != NewOperand;\n    NewOperands.push_back(NewOperand);\n  }\n\n  // If none of the elements has been modified, return C as it is.\n  if (!OperandChanged) {\n    return C;\n  }\n\n  // If any of the elements has been  modified, construct the equivalent\n  // vector or aggregate value with a set instructions and the converted\n  // elements.\n  Value *NewValue = UndefValue::get(C->getType());\n  if (isa<ConstantVector>(C)) {\n    for (unsigned i = 0; i < NumOperands; ++i) {\n      Value *Idx = ConstantInt::get(Type::getInt32Ty(M->getContext()), i);\n      NewValue = Builder.CreateInsertElement(NewValue, NewOperands[i], Idx);\n    }\n  } else {\n    for (unsigned i = 0; i < NumOperands; ++i) {\n      NewValue =\n          Builder.CreateInsertValue(NewValue, NewOperands[i], makeArrayRef(i));\n    }\n  }\n\n  return NewValue;\n}\n\n"}}},"NVPTXLowerKernelArgs.cpp":{"path":"NVPTXLowerKernelArgs.cpp","size":8360,"lines":234,"functions":{"runOnFunction":{"range":[195,231],"code":"bool NVPTXLowerKernelArgs::runOnFunction(Function &F) {\n  // Skip non-kernels. See the comments at the top of this file.\n  if (!isKernelFunction(F))\n    return false;\n\n  if (TM && TM->getDrvInterface() == NVPTX::CUDA) {\n    // Mark pointers in byval structs as global.\n    for (auto &B : F) {\n      for (auto &I : B) {\n        if (LoadInst *LI = dyn_cast<LoadInst>(&I)) {\n          if (LI->getType()->isPointerTy()) {\n            Value *UO = GetUnderlyingObject(LI->getPointerOperand(),\n                                            F.getParent()->getDataLayout());\n            if (Argument *Arg = dyn_cast<Argument>(UO)) {\n              if (Arg->hasByValAttr()) {\n                // LI is a load from a pointer within a byval kernel parameter.\n                markPointerAsGlobal(LI);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  for (Argument &Arg : F.args()) {\n    if (Arg.getType()->isPointerTy()) {\n      if (Arg.hasByValAttr())\n        handleByValParam(&Arg);\n      else if (TM && TM->getDrvInterface() == NVPTX::CUDA)\n        markPointerAsGlobal(&Arg);\n    }\n  }\n  return true;\n}\n\nFunctionPass *\n"},"markPointerAsGlobal":{"range":[165,194],"code":"void NVPTXLowerKernelArgs::markPointerAsGlobal(Value *Ptr) {\n  if (Ptr->getType()->getPointerAddressSpace() == ADDRESS_SPACE_GLOBAL)\n    return;\n\n  // Deciding where to emit the addrspacecast pair.\n  BasicBlock::iterator InsertPt;\n  if (Argument *Arg = dyn_cast<Argument>(Ptr)) {\n    // Insert at the functon entry if Ptr is an argument.\n    InsertPt = Arg->getParent()->getEntryBlock().begin();\n  } else {\n    // Insert right after Ptr if Ptr is an instruction.\n    InsertPt = ++cast<Instruction>(Ptr)->getIterator();\n    assert(InsertPt != InsertPt->getParent()->end() &&\n           \"We don't call this function with Ptr being a terminator.\");\n  }\n\n  Instruction *PtrInGlobal = new AddrSpaceCastInst(\n      Ptr, PointerType::get(Ptr->getType()->getPointerElementType(),\n                            ADDRESS_SPACE_GLOBAL),\n      Ptr->getName(), &*InsertPt);\n  Value *PtrInGeneric = new AddrSpaceCastInst(PtrInGlobal, Ptr->getType(),\n                                              Ptr->getName(), &*InsertPt);\n  // Replace with PtrInGeneric all uses of Ptr except PtrInGlobal.\n  Ptr->replaceAllUsesWith(PtrInGeneric);\n  PtrInGlobal->setOperand(0, Ptr);\n}\n\n// =============================================================================\n// Main function for this pass.\n// =============================================================================\n"},"handleByValParam":{"range":[143,164],"code":"void NVPTXLowerKernelArgs::handleByValParam(Argument *Arg) {\n  Function *Func = Arg->getParent();\n  Instruction *FirstInst = &(Func->getEntryBlock().front());\n  PointerType *PType = dyn_cast<PointerType>(Arg->getType());\n\n  assert(PType && \"Expecting pointer type in handleByValParam\");\n\n  Type *StructType = PType->getElementType();\n  AllocaInst *AllocA = new AllocaInst(StructType, Arg->getName(), FirstInst);\n  // Set the alignment to alignment of the byval parameter. This is because,\n  // later load/stores assume that alignment, and we are going to replace\n  // the use of the byval parameter with this alloca instruction.\n  AllocA->setAlignment(Func->getParamAlignment(Arg->getArgNo() + 1));\n  Arg->replaceAllUsesWith(AllocA);\n\n  Value *ArgInParam = new AddrSpaceCastInst(\n      Arg, PointerType::get(StructType, ADDRESS_SPACE_PARAM), Arg->getName(),\n      FirstInst);\n  LoadInst *LI = new LoadInst(ArgInParam, Arg->getName(), FirstInst);\n  new StoreInst(LI, AllocA, FirstInst);\n}\n\n"},"NVPTXLowerKernelArgs":{"range":[114,142],"code":"  NVPTXLowerKernelArgs(const NVPTXTargetMachine *TM = nullptr)\n      : FunctionPass(ID), TM(TM) {}\n  const char *getPassName() const override {\n    return \"Lower pointer arguments of CUDA kernels\";\n  }\n\nprivate:\n  const NVPTXTargetMachine *TM;\n};\n} // namespace\n\nchar NVPTXLowerKernelArgs::ID = 1;\n\nINITIALIZE_PASS(NVPTXLowerKernelArgs, \"nvptx-lower-kernel-args\",\n                \"Lower kernel arguments (NVPTX)\", false, false)\n\n// =============================================================================\n// If the function had a byval struct ptr arg, say foo(%struct.x* byval %d),\n// then add the following instructions to the first basic block:\n//\n// %temp = alloca %struct.x, align 8\n// %tempd = addrspacecast %struct.x* %d to %struct.x addrspace(101)*\n// %tv = load %struct.x addrspace(101)* %tempd\n// store %struct.x %tv, %struct.x* %temp, align 8\n//\n// The above code allocates some space in the stack and copies the incoming\n// struct from param space to local space.\n// Then replace all occurrences of %d by %temp.\n// =============================================================================\n"},"createNVPTXLowerKernelArgsPass":{"range":[232,235],"code":"llvm::createNVPTXLowerKernelArgsPass(const NVPTXTargetMachine *TM) {\n  return new NVPTXLowerKernelArgs(TM);\n}\n"}}},"NVPTXLowerAggrCopies.cpp":{"path":"NVPTXLowerAggrCopies.cpp","size":13874,"lines":351,"functions":{"convertMemMoveToLoop":{"range":[131,207],"code":"void convertMemMoveToLoop(Instruction *ConvertedInst, Value *SrcAddr,\n                          Value *DstAddr, Value *CopyLen, bool SrcIsVolatile,\n                          bool DstIsVolatile, LLVMContext &Context,\n                          Function &F) {\n  Type *TypeOfCopyLen = CopyLen->getType();\n  BasicBlock *OrigBB = ConvertedInst->getParent();\n\n  // Create the a comparison of src and dst, based on which we jump to either\n  // the forward-copy part of the function (if src >= dst) or the backwards-copy\n  // part (if src < dst).\n  // SplitBlockAndInsertIfThenElse conveniently creates the basic if-then-else\n  // structure. Its block terminators (unconditional branches) are replaced by\n  // the appropriate conditional branches when the loop is built.\n  ICmpInst *PtrCompare = new ICmpInst(ConvertedInst, ICmpInst::ICMP_ULT,\n                                      SrcAddr, DstAddr, \"compare_src_dst\");\n  TerminatorInst *ThenTerm, *ElseTerm;\n  SplitBlockAndInsertIfThenElse(PtrCompare, ConvertedInst, &ThenTerm,\n                                &ElseTerm);\n\n  // Each part of the function consists of two blocks:\n  //   copy_backwards:        used to skip the loop when n == 0\n  //   copy_backwards_loop:   the actual backwards loop BB\n  //   copy_forward:          used to skip the loop when n == 0\n  //   copy_forward_loop:     the actual forward loop BB\n  BasicBlock *CopyBackwardsBB = ThenTerm->getParent();\n  CopyBackwardsBB->setName(\"copy_backwards\");\n  BasicBlock *CopyForwardBB = ElseTerm->getParent();\n  CopyForwardBB->setName(\"copy_forward\");\n  BasicBlock *ExitBB = ConvertedInst->getParent();\n  ExitBB->setName(\"memmove_done\");\n\n  // Initial comparison of n == 0 that lets us skip the loops altogether. Shared\n  // between both backwards and forward copy clauses.\n  ICmpInst *CompareN =\n      new ICmpInst(OrigBB->getTerminator(), ICmpInst::ICMP_EQ, CopyLen,\n                   ConstantInt::get(TypeOfCopyLen, 0), \"compare_n_to_0\");\n\n  // Copying backwards.\n  BasicBlock *LoopBB =\n      BasicBlock::Create(Context, \"copy_backwards_loop\", &F, CopyForwardBB);\n  IRBuilder<> LoopBuilder(LoopBB);\n  PHINode *LoopPhi = LoopBuilder.CreatePHI(TypeOfCopyLen, 0);\n  Value *IndexPtr = LoopBuilder.CreateSub(\n      LoopPhi, ConstantInt::get(TypeOfCopyLen, 1), \"index_ptr\");\n  Value *Element = LoopBuilder.CreateLoad(\n      LoopBuilder.CreateInBoundsGEP(SrcAddr, IndexPtr), \"element\");\n  LoopBuilder.CreateStore(Element,\n                          LoopBuilder.CreateInBoundsGEP(DstAddr, IndexPtr));\n  LoopBuilder.CreateCondBr(\n      LoopBuilder.CreateICmpEQ(IndexPtr, ConstantInt::get(TypeOfCopyLen, 0)),\n      ExitBB, LoopBB);\n  LoopPhi->addIncoming(IndexPtr, LoopBB);\n  LoopPhi->addIncoming(CopyLen, CopyBackwardsBB);\n  BranchInst::Create(ExitBB, LoopBB, CompareN, ThenTerm);\n  ThenTerm->eraseFromParent();\n\n  // Copying forward.\n  BasicBlock *FwdLoopBB =\n      BasicBlock::Create(Context, \"copy_forward_loop\", &F, ExitBB);\n  IRBuilder<> FwdLoopBuilder(FwdLoopBB);\n  PHINode *FwdCopyPhi = FwdLoopBuilder.CreatePHI(TypeOfCopyLen, 0, \"index_ptr\");\n  Value *FwdElement = FwdLoopBuilder.CreateLoad(\n      FwdLoopBuilder.CreateInBoundsGEP(SrcAddr, FwdCopyPhi), \"element\");\n  FwdLoopBuilder.CreateStore(\n      FwdElement, FwdLoopBuilder.CreateInBoundsGEP(DstAddr, FwdCopyPhi));\n  Value *FwdIndexPtr = FwdLoopBuilder.CreateAdd(\n      FwdCopyPhi, ConstantInt::get(TypeOfCopyLen, 1), \"index_increment\");\n  FwdLoopBuilder.CreateCondBr(FwdLoopBuilder.CreateICmpEQ(FwdIndexPtr, CopyLen),\n                              ExitBB, FwdLoopBB);\n  FwdCopyPhi->addIncoming(FwdIndexPtr, FwdLoopBB);\n  FwdCopyPhi->addIncoming(ConstantInt::get(TypeOfCopyLen, 0), CopyForwardBB);\n\n  BranchInst::Create(ExitBB, FwdLoopBB, CompareN, ElseTerm);\n  ElseTerm->eraseFromParent();\n}\n\n// Lower memset to loop.\n"},"runOnFunction":{"range":[241,348],"code":"bool NVPTXLowerAggrCopies::runOnFunction(Function &F) {\n  SmallVector<LoadInst *, 4> AggrLoads;\n  SmallVector<MemIntrinsic *, 4> MemCalls;\n\n  const DataLayout &DL = F.getParent()->getDataLayout();\n  LLVMContext &Context = F.getParent()->getContext();\n\n  // Collect all aggregate loads and mem* calls.\n  for (Function::iterator BI = F.begin(), BE = F.end(); BI != BE; ++BI) {\n    for (BasicBlock::iterator II = BI->begin(), IE = BI->end(); II != IE;\n         ++II) {\n      if (LoadInst *LI = dyn_cast<LoadInst>(II)) {\n        if (!LI->hasOneUse())\n          continue;\n\n        if (DL.getTypeStoreSize(LI->getType()) < MaxAggrCopySize)\n          continue;\n\n        if (StoreInst *SI = dyn_cast<StoreInst>(LI->user_back())) {\n          if (SI->getOperand(0) != LI)\n            continue;\n          AggrLoads.push_back(LI);\n        }\n      } else if (MemIntrinsic *IntrCall = dyn_cast<MemIntrinsic>(II)) {\n        // Convert intrinsic calls with variable size or with constant size\n        // larger than the MaxAggrCopySize threshold.\n        if (ConstantInt *LenCI = dyn_cast<ConstantInt>(IntrCall->getLength())) {\n          if (LenCI->getZExtValue() >= MaxAggrCopySize) {\n            MemCalls.push_back(IntrCall);\n          }\n        } else {\n          MemCalls.push_back(IntrCall);\n        }\n      }\n    }\n  }\n\n  if (AggrLoads.size() == 0 && MemCalls.size() == 0) {\n    return false;\n  }\n\n  //\n  // Do the transformation of an aggr load/copy/set to a loop\n  //\n  for (LoadInst *LI : AggrLoads) {\n    StoreInst *SI = dyn_cast<StoreInst>(*LI->user_begin());\n    Value *SrcAddr = LI->getOperand(0);\n    Value *DstAddr = SI->getOperand(1);\n    unsigned NumLoads = DL.getTypeStoreSize(LI->getType());\n    Value *CopyLen = ConstantInt::get(Type::getInt32Ty(Context), NumLoads);\n\n    convertMemCpyToLoop(/* ConvertedInst */ SI,\n                        /* SrcAddr */ SrcAddr, /* DstAddr */ DstAddr,\n                        /* CopyLen */ CopyLen,\n                        /* SrcIsVolatile */ LI->isVolatile(),\n                        /* DstIsVolatile */ SI->isVolatile(),\n                        /* Context */ Context,\n                        /* Function F */ F);\n\n    SI->eraseFromParent();\n    LI->eraseFromParent();\n  }\n\n  // Transform mem* intrinsic calls.\n  for (MemIntrinsic *MemCall : MemCalls) {\n    if (MemCpyInst *Memcpy = dyn_cast<MemCpyInst>(MemCall)) {\n      convertMemCpyToLoop(/* ConvertedInst */ Memcpy,\n                          /* SrcAddr */ Memcpy->getRawSource(),\n                          /* DstAddr */ Memcpy->getRawDest(),\n                          /* CopyLen */ Memcpy->getLength(),\n                          /* SrcIsVolatile */ Memcpy->isVolatile(),\n                          /* DstIsVolatile */ Memcpy->isVolatile(),\n                          /* Context */ Context,\n                          /* Function F */ F);\n    } else if (MemMoveInst *Memmove = dyn_cast<MemMoveInst>(MemCall)) {\n      convertMemMoveToLoop(/* ConvertedInst */ Memmove,\n                           /* SrcAddr */ Memmove->getRawSource(),\n                           /* DstAddr */ Memmove->getRawDest(),\n                           /* CopyLen */ Memmove->getLength(),\n                           /* SrcIsVolatile */ Memmove->isVolatile(),\n                           /* DstIsVolatile */ Memmove->isVolatile(),\n                           /* Context */ Context,\n                           /* Function F */ F);\n\n    } else if (MemSetInst *Memset = dyn_cast<MemSetInst>(MemCall)) {\n      convertMemSetToLoop(/* ConvertedInst */ Memset,\n                          /* DstAddr */ Memset->getRawDest(),\n                          /* CopyLen */ Memset->getLength(),\n                          /* SetValue */ Memset->getValue(),\n                          /* Context */ Context,\n                          /* Function F */ F);\n    }\n    MemCall->eraseFromParent();\n  }\n\n  return true;\n}\n\n} // namespace\n\nnamespace llvm {\nvoid initializeNVPTXLowerAggrCopiesPass(PassRegistry &);\n}\n\nINITIALIZE_PASS(NVPTXLowerAggrCopies, \"nvptx-lower-aggr-copies\",\n                \"Lower aggregate copies, and llvm.mem* intrinsics into loops\",\n                false, false)\n\n"},"NVPTXLowerAggrCopies":{"range":[41,59],"code":"  NVPTXLowerAggrCopies() : FunctionPass(ID) {}\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override {\n    AU.addPreserved<MachineFunctionAnalysis>();\n    AU.addPreserved<StackProtector>();\n  }\n\n  bool runOnFunction(Function &F) override;\n\n  static const unsigned MaxAggrCopySize = 128;\n\n  const char *getPassName() const override {\n    return \"Lower aggregate copies/intrinsics into loops\";\n  }\n};\n\nchar NVPTXLowerAggrCopies::ID = 0;\n\n// Lower memcpy to loop.\n"},"convertMemCpyToLoop":{"range":[60,130],"code":"void convertMemCpyToLoop(Instruction *ConvertedInst, Value *SrcAddr,\n                         Value *DstAddr, Value *CopyLen, bool SrcIsVolatile,\n                         bool DstIsVolatile, LLVMContext &Context,\n                         Function &F) {\n  Type *TypeOfCopyLen = CopyLen->getType();\n\n  BasicBlock *OrigBB = ConvertedInst->getParent();\n  BasicBlock *NewBB =\n      ConvertedInst->getParent()->splitBasicBlock(ConvertedInst, \"split\");\n  BasicBlock *LoopBB = BasicBlock::Create(Context, \"loadstoreloop\", &F, NewBB);\n\n  OrigBB->getTerminator()->setSuccessor(0, LoopBB);\n  IRBuilder<> Builder(OrigBB->getTerminator());\n\n  // SrcAddr and DstAddr are expected to be pointer types,\n  // so no check is made here.\n  unsigned SrcAS = cast<PointerType>(SrcAddr->getType())->getAddressSpace();\n  unsigned DstAS = cast<PointerType>(DstAddr->getType())->getAddressSpace();\n\n  // Cast pointers to (char *)\n  SrcAddr = Builder.CreateBitCast(SrcAddr, Builder.getInt8PtrTy(SrcAS));\n  DstAddr = Builder.CreateBitCast(DstAddr, Builder.getInt8PtrTy(DstAS));\n\n  IRBuilder<> LoopBuilder(LoopBB);\n  PHINode *LoopIndex = LoopBuilder.CreatePHI(TypeOfCopyLen, 0);\n  LoopIndex->addIncoming(ConstantInt::get(TypeOfCopyLen, 0), OrigBB);\n\n  // load from SrcAddr+LoopIndex\n  // TODO: we can leverage the align parameter of llvm.memcpy for more efficient\n  // word-sized loads and stores.\n  Value *Element =\n      LoopBuilder.CreateLoad(LoopBuilder.CreateInBoundsGEP(\n                                 LoopBuilder.getInt8Ty(), SrcAddr, LoopIndex),\n                             SrcIsVolatile);\n  // store at DstAddr+LoopIndex\n  LoopBuilder.CreateStore(Element,\n                          LoopBuilder.CreateInBoundsGEP(LoopBuilder.getInt8Ty(),\n                                                        DstAddr, LoopIndex),\n                          DstIsVolatile);\n\n  // The value for LoopIndex coming from backedge is (LoopIndex + 1)\n  Value *NewIndex =\n      LoopBuilder.CreateAdd(LoopIndex, ConstantInt::get(TypeOfCopyLen, 1));\n  LoopIndex->addIncoming(NewIndex, LoopBB);\n\n  LoopBuilder.CreateCondBr(LoopBuilder.CreateICmpULT(NewIndex, CopyLen), LoopBB,\n                           NewBB);\n}\n\n// Lower memmove to IR. memmove is required to correctly copy overlapping memory\n// regions; therefore, it has to check the relative positions of the source and\n// destination pointers and choose the copy direction accordingly.\n//\n// The code below is an IR rendition of this C function:\n//\n// void* memmove(void* dst, const void* src, size_t n) {\n//   unsigned char* d = dst;\n//   const unsigned char* s = src;\n//   if (s < d) {\n//     // copy backwards\n//     while (n--) {\n//       d[n] = s[n];\n//     }\n//   } else {\n//     // copy forward\n//     for (size_t i = 0; i < n; ++i) {\n//       d[i] = s[i];\n//     }\n//   }\n//   return dst;\n// }\n"},"createLowerAggrCopies":{"range":[349,352],"code":"FunctionPass *llvm::createLowerAggrCopies() {\n  return new NVPTXLowerAggrCopies();\n}\n"},"convertMemSetToLoop":{"range":[208,240],"code":"void convertMemSetToLoop(Instruction *ConvertedInst, Value *DstAddr,\n                         Value *CopyLen, Value *SetValue, LLVMContext &Context,\n                         Function &F) {\n  BasicBlock *OrigBB = ConvertedInst->getParent();\n  BasicBlock *NewBB =\n      ConvertedInst->getParent()->splitBasicBlock(ConvertedInst, \"split\");\n  BasicBlock *LoopBB = BasicBlock::Create(Context, \"loadstoreloop\", &F, NewBB);\n\n  OrigBB->getTerminator()->setSuccessor(0, LoopBB);\n  IRBuilder<> Builder(OrigBB->getTerminator());\n\n  // Cast pointer to the type of value getting stored\n  unsigned dstAS = cast<PointerType>(DstAddr->getType())->getAddressSpace();\n  DstAddr = Builder.CreateBitCast(DstAddr,\n                                  PointerType::get(SetValue->getType(), dstAS));\n\n  IRBuilder<> LoopBuilder(LoopBB);\n  PHINode *LoopIndex = LoopBuilder.CreatePHI(CopyLen->getType(), 0);\n  LoopIndex->addIncoming(ConstantInt::get(CopyLen->getType(), 0), OrigBB);\n\n  LoopBuilder.CreateStore(\n      SetValue,\n      LoopBuilder.CreateInBoundsGEP(SetValue->getType(), DstAddr, LoopIndex),\n      false);\n\n  Value *NewIndex =\n      LoopBuilder.CreateAdd(LoopIndex, ConstantInt::get(CopyLen->getType(), 1));\n  LoopIndex->addIncoming(NewIndex, LoopBB);\n\n  LoopBuilder.CreateCondBr(LoopBuilder.CreateICmpULT(NewIndex, CopyLen), LoopBB,\n                           NewBB);\n}\n\n"}}},"NVPTXISelLowering.cpp":{"path":"NVPTXISelLowering.cpp","size":184128,"lines":4562,"functions":{"getParamSymbol":{"range":[2025,2038],"code":"NVPTXTargetLowering::getParamSymbol(SelectionDAG &DAG, int idx, EVT v) const {\n  std::string ParamSym;\n  raw_string_ostream ParamStr(ParamSym);\n\n  ParamStr << DAG.getMachineFunction().getName() << \"_param_\" << idx;\n  ParamStr.flush();\n\n  std::string *SavedStr =\n    nvTM->getManagedStrPool()->getManagedString(ParamSym.c_str());\n  return DAG.getTargetExternalSymbol(SavedStr->c_str(), v);\n}\n\n// Check to see if the kernel argument is image*_t or sampler_t\n\n"},"LowerSTOREi1":{"range":[2009,2024],"code":"SDValue NVPTXTargetLowering::LowerSTOREi1(SDValue Op, SelectionDAG &DAG) const {\n  SDNode *Node = Op.getNode();\n  SDLoc dl(Node);\n  StoreSDNode *ST = cast<StoreSDNode>(Node);\n  SDValue Tmp1 = ST->getChain();\n  SDValue Tmp2 = ST->getBasePtr();\n  SDValue Tmp3 = ST->getValue();\n  assert(Tmp3.getValueType() == MVT::i1 && \"Custom lowering for i1 store only\");\n  Tmp3 = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, Tmp3);\n  SDValue Result =\n      DAG.getTruncStore(Tmp1, dl, Tmp3, Tmp2, ST->getPointerInfo(), MVT::i8,\n                        ST->getAlignment(), ST->getMemOperand()->getFlags());\n  return Result;\n}\n\nSDValue\n"},"LowerShiftLeftParts":{"range":[1765,1822],"code":"SDValue NVPTXTargetLowering::LowerShiftLeftParts(SDValue Op,\n                                                 SelectionDAG &DAG) const {\n  assert(Op.getNumOperands() == 3 && \"Not a double-shift!\");\n  assert(Op.getOpcode() == ISD::SHL_PARTS);\n\n  EVT VT = Op.getValueType();\n  unsigned VTBits = VT.getSizeInBits();\n  SDLoc dl(Op);\n  SDValue ShOpLo = Op.getOperand(0);\n  SDValue ShOpHi = Op.getOperand(1);\n  SDValue ShAmt  = Op.getOperand(2);\n\n  if (VTBits == 32 && STI.getSmVersion() >= 35) {\n\n    // For 32bit and sm35, we can use the funnel shift 'shf' instruction.\n    // {dHi, dLo} = {aHi, aLo} << Amt\n    //   dHi = shf.l.clamp aLo, aHi, Amt\n    //   dLo = aLo << Amt\n\n    SDValue Hi = DAG.getNode(NVPTXISD::FUN_SHFL_CLAMP, dl, VT, ShOpLo, ShOpHi,\n                             ShAmt);\n    SDValue Lo = DAG.getNode(ISD::SHL, dl, VT, ShOpLo, ShAmt);\n\n    SDValue Ops[2] = { Lo, Hi };\n    return DAG.getMergeValues(Ops, dl);\n  }\n  else {\n\n    // {dHi, dLo} = {aHi, aLo} << Amt\n    // - if (Amt>=size) then\n    //      dLo = aLo << Amt (all 0)\n    //      dLo = aLo << (Amt-size)\n    //   else\n    //      dLo = aLo << Amt\n    //      dHi = (aHi << Amt) | (aLo >> (size-Amt))\n\n    SDValue RevShAmt = DAG.getNode(ISD::SUB, dl, MVT::i32,\n                                   DAG.getConstant(VTBits, dl, MVT::i32),\n                                   ShAmt);\n    SDValue Tmp1 = DAG.getNode(ISD::SHL, dl, VT, ShOpHi, ShAmt);\n    SDValue ExtraShAmt = DAG.getNode(ISD::SUB, dl, MVT::i32, ShAmt,\n                                     DAG.getConstant(VTBits, dl, MVT::i32));\n    SDValue Tmp2 = DAG.getNode(ISD::SRL, dl, VT, ShOpLo, RevShAmt);\n    SDValue FalseVal = DAG.getNode(ISD::OR, dl, VT, Tmp1, Tmp2);\n    SDValue TrueVal = DAG.getNode(ISD::SHL, dl, VT, ShOpLo, ExtraShAmt);\n\n    SDValue Cmp = DAG.getSetCC(dl, MVT::i1, ShAmt,\n                               DAG.getConstant(VTBits, dl, MVT::i32),\n                               ISD::SETGE);\n    SDValue Lo = DAG.getNode(ISD::SHL, dl, VT, ShOpLo, ShAmt);\n    SDValue Hi = DAG.getNode(ISD::SELECT, dl, VT, Cmp, TrueVal, FalseVal);\n\n    SDValue Ops[2] = { Lo, Hi };\n    return DAG.getMergeValues(Ops, dl);\n  }\n}\n\nSDValue\n"},"getOpcForSurfaceInstr":{"range":[2900,3241],"code":"static unsigned getOpcForSurfaceInstr(unsigned Intrinsic) {\n  switch (Intrinsic) {\n  default:\n    return 0;\n  case Intrinsic::nvvm_suld_1d_i8_clamp:\n    return NVPTXISD::Suld1DI8Clamp;\n  case Intrinsic::nvvm_suld_1d_i16_clamp:\n    return NVPTXISD::Suld1DI16Clamp;\n  case Intrinsic::nvvm_suld_1d_i32_clamp:\n    return NVPTXISD::Suld1DI32Clamp;\n  case Intrinsic::nvvm_suld_1d_i64_clamp:\n    return NVPTXISD::Suld1DI64Clamp;\n  case Intrinsic::nvvm_suld_1d_v2i8_clamp:\n    return NVPTXISD::Suld1DV2I8Clamp;\n  case Intrinsic::nvvm_suld_1d_v2i16_clamp:\n    return NVPTXISD::Suld1DV2I16Clamp;\n  case Intrinsic::nvvm_suld_1d_v2i32_clamp:\n    return NVPTXISD::Suld1DV2I32Clamp;\n  case Intrinsic::nvvm_suld_1d_v2i64_clamp:\n    return NVPTXISD::Suld1DV2I64Clamp;\n  case Intrinsic::nvvm_suld_1d_v4i8_clamp:\n    return NVPTXISD::Suld1DV4I8Clamp;\n  case Intrinsic::nvvm_suld_1d_v4i16_clamp:\n    return NVPTXISD::Suld1DV4I16Clamp;\n  case Intrinsic::nvvm_suld_1d_v4i32_clamp:\n    return NVPTXISD::Suld1DV4I32Clamp;\n  case Intrinsic::nvvm_suld_1d_array_i8_clamp:\n    return NVPTXISD::Suld1DArrayI8Clamp;\n  case Intrinsic::nvvm_suld_1d_array_i16_clamp:\n    return NVPTXISD::Suld1DArrayI16Clamp;\n  case Intrinsic::nvvm_suld_1d_array_i32_clamp:\n    return NVPTXISD::Suld1DArrayI32Clamp;\n  case Intrinsic::nvvm_suld_1d_array_i64_clamp:\n    return NVPTXISD::Suld1DArrayI64Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v2i8_clamp:\n    return NVPTXISD::Suld1DArrayV2I8Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v2i16_clamp:\n    return NVPTXISD::Suld1DArrayV2I16Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v2i32_clamp:\n    return NVPTXISD::Suld1DArrayV2I32Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v2i64_clamp:\n    return NVPTXISD::Suld1DArrayV2I64Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v4i8_clamp:\n    return NVPTXISD::Suld1DArrayV4I8Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v4i16_clamp:\n    return NVPTXISD::Suld1DArrayV4I16Clamp;\n  case Intrinsic::nvvm_suld_1d_array_v4i32_clamp:\n    return NVPTXISD::Suld1DArrayV4I32Clamp;\n  case Intrinsic::nvvm_suld_2d_i8_clamp:\n    return NVPTXISD::Suld2DI8Clamp;\n  case Intrinsic::nvvm_suld_2d_i16_clamp:\n    return NVPTXISD::Suld2DI16Clamp;\n  case Intrinsic::nvvm_suld_2d_i32_clamp:\n    return NVPTXISD::Suld2DI32Clamp;\n  case Intrinsic::nvvm_suld_2d_i64_clamp:\n    return NVPTXISD::Suld2DI64Clamp;\n  case Intrinsic::nvvm_suld_2d_v2i8_clamp:\n    return NVPTXISD::Suld2DV2I8Clamp;\n  case Intrinsic::nvvm_suld_2d_v2i16_clamp:\n    return NVPTXISD::Suld2DV2I16Clamp;\n  case Intrinsic::nvvm_suld_2d_v2i32_clamp:\n    return NVPTXISD::Suld2DV2I32Clamp;\n  case Intrinsic::nvvm_suld_2d_v2i64_clamp:\n    return NVPTXISD::Suld2DV2I64Clamp;\n  case Intrinsic::nvvm_suld_2d_v4i8_clamp:\n    return NVPTXISD::Suld2DV4I8Clamp;\n  case Intrinsic::nvvm_suld_2d_v4i16_clamp:\n    return NVPTXISD::Suld2DV4I16Clamp;\n  case Intrinsic::nvvm_suld_2d_v4i32_clamp:\n    return NVPTXISD::Suld2DV4I32Clamp;\n  case Intrinsic::nvvm_suld_2d_array_i8_clamp:\n    return NVPTXISD::Suld2DArrayI8Clamp;\n  case Intrinsic::nvvm_suld_2d_array_i16_clamp:\n    return NVPTXISD::Suld2DArrayI16Clamp;\n  case Intrinsic::nvvm_suld_2d_array_i32_clamp:\n    return NVPTXISD::Suld2DArrayI32Clamp;\n  case Intrinsic::nvvm_suld_2d_array_i64_clamp:\n    return NVPTXISD::Suld2DArrayI64Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v2i8_clamp:\n    return NVPTXISD::Suld2DArrayV2I8Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v2i16_clamp:\n    return NVPTXISD::Suld2DArrayV2I16Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v2i32_clamp:\n    return NVPTXISD::Suld2DArrayV2I32Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v2i64_clamp:\n    return NVPTXISD::Suld2DArrayV2I64Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v4i8_clamp:\n    return NVPTXISD::Suld2DArrayV4I8Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v4i16_clamp:\n    return NVPTXISD::Suld2DArrayV4I16Clamp;\n  case Intrinsic::nvvm_suld_2d_array_v4i32_clamp:\n    return NVPTXISD::Suld2DArrayV4I32Clamp;\n  case Intrinsic::nvvm_suld_3d_i8_clamp:\n    return NVPTXISD::Suld3DI8Clamp;\n  case Intrinsic::nvvm_suld_3d_i16_clamp:\n    return NVPTXISD::Suld3DI16Clamp;\n  case Intrinsic::nvvm_suld_3d_i32_clamp:\n    return NVPTXISD::Suld3DI32Clamp;\n  case Intrinsic::nvvm_suld_3d_i64_clamp:\n    return NVPTXISD::Suld3DI64Clamp;\n  case Intrinsic::nvvm_suld_3d_v2i8_clamp:\n    return NVPTXISD::Suld3DV2I8Clamp;\n  case Intrinsic::nvvm_suld_3d_v2i16_clamp:\n    return NVPTXISD::Suld3DV2I16Clamp;\n  case Intrinsic::nvvm_suld_3d_v2i32_clamp:\n    return NVPTXISD::Suld3DV2I32Clamp;\n  case Intrinsic::nvvm_suld_3d_v2i64_clamp:\n    return NVPTXISD::Suld3DV2I64Clamp;\n  case Intrinsic::nvvm_suld_3d_v4i8_clamp:\n    return NVPTXISD::Suld3DV4I8Clamp;\n  case Intrinsic::nvvm_suld_3d_v4i16_clamp:\n    return NVPTXISD::Suld3DV4I16Clamp;\n  case Intrinsic::nvvm_suld_3d_v4i32_clamp:\n    return NVPTXISD::Suld3DV4I32Clamp;\n  case Intrinsic::nvvm_suld_1d_i8_trap:\n    return NVPTXISD::Suld1DI8Trap;\n  case Intrinsic::nvvm_suld_1d_i16_trap:\n    return NVPTXISD::Suld1DI16Trap;\n  case Intrinsic::nvvm_suld_1d_i32_trap:\n    return NVPTXISD::Suld1DI32Trap;\n  case Intrinsic::nvvm_suld_1d_i64_trap:\n    return NVPTXISD::Suld1DI64Trap;\n  case Intrinsic::nvvm_suld_1d_v2i8_trap:\n    return NVPTXISD::Suld1DV2I8Trap;\n  case Intrinsic::nvvm_suld_1d_v2i16_trap:\n    return NVPTXISD::Suld1DV2I16Trap;\n  case Intrinsic::nvvm_suld_1d_v2i32_trap:\n    return NVPTXISD::Suld1DV2I32Trap;\n  case Intrinsic::nvvm_suld_1d_v2i64_trap:\n    return NVPTXISD::Suld1DV2I64Trap;\n  case Intrinsic::nvvm_suld_1d_v4i8_trap:\n    return NVPTXISD::Suld1DV4I8Trap;\n  case Intrinsic::nvvm_suld_1d_v4i16_trap:\n    return NVPTXISD::Suld1DV4I16Trap;\n  case Intrinsic::nvvm_suld_1d_v4i32_trap:\n    return NVPTXISD::Suld1DV4I32Trap;\n  case Intrinsic::nvvm_suld_1d_array_i8_trap:\n    return NVPTXISD::Suld1DArrayI8Trap;\n  case Intrinsic::nvvm_suld_1d_array_i16_trap:\n    return NVPTXISD::Suld1DArrayI16Trap;\n  case Intrinsic::nvvm_suld_1d_array_i32_trap:\n    return NVPTXISD::Suld1DArrayI32Trap;\n  case Intrinsic::nvvm_suld_1d_array_i64_trap:\n    return NVPTXISD::Suld1DArrayI64Trap;\n  case Intrinsic::nvvm_suld_1d_array_v2i8_trap:\n    return NVPTXISD::Suld1DArrayV2I8Trap;\n  case Intrinsic::nvvm_suld_1d_array_v2i16_trap:\n    return NVPTXISD::Suld1DArrayV2I16Trap;\n  case Intrinsic::nvvm_suld_1d_array_v2i32_trap:\n    return NVPTXISD::Suld1DArrayV2I32Trap;\n  case Intrinsic::nvvm_suld_1d_array_v2i64_trap:\n    return NVPTXISD::Suld1DArrayV2I64Trap;\n  case Intrinsic::nvvm_suld_1d_array_v4i8_trap:\n    return NVPTXISD::Suld1DArrayV4I8Trap;\n  case Intrinsic::nvvm_suld_1d_array_v4i16_trap:\n    return NVPTXISD::Suld1DArrayV4I16Trap;\n  case Intrinsic::nvvm_suld_1d_array_v4i32_trap:\n    return NVPTXISD::Suld1DArrayV4I32Trap;\n  case Intrinsic::nvvm_suld_2d_i8_trap:\n    return NVPTXISD::Suld2DI8Trap;\n  case Intrinsic::nvvm_suld_2d_i16_trap:\n    return NVPTXISD::Suld2DI16Trap;\n  case Intrinsic::nvvm_suld_2d_i32_trap:\n    return NVPTXISD::Suld2DI32Trap;\n  case Intrinsic::nvvm_suld_2d_i64_trap:\n    return NVPTXISD::Suld2DI64Trap;\n  case Intrinsic::nvvm_suld_2d_v2i8_trap:\n    return NVPTXISD::Suld2DV2I8Trap;\n  case Intrinsic::nvvm_suld_2d_v2i16_trap:\n    return NVPTXISD::Suld2DV2I16Trap;\n  case Intrinsic::nvvm_suld_2d_v2i32_trap:\n    return NVPTXISD::Suld2DV2I32Trap;\n  case Intrinsic::nvvm_suld_2d_v2i64_trap:\n    return NVPTXISD::Suld2DV2I64Trap;\n  case Intrinsic::nvvm_suld_2d_v4i8_trap:\n    return NVPTXISD::Suld2DV4I8Trap;\n  case Intrinsic::nvvm_suld_2d_v4i16_trap:\n    return NVPTXISD::Suld2DV4I16Trap;\n  case Intrinsic::nvvm_suld_2d_v4i32_trap:\n    return NVPTXISD::Suld2DV4I32Trap;\n  case Intrinsic::nvvm_suld_2d_array_i8_trap:\n    return NVPTXISD::Suld2DArrayI8Trap;\n  case Intrinsic::nvvm_suld_2d_array_i16_trap:\n    return NVPTXISD::Suld2DArrayI16Trap;\n  case Intrinsic::nvvm_suld_2d_array_i32_trap:\n    return NVPTXISD::Suld2DArrayI32Trap;\n  case Intrinsic::nvvm_suld_2d_array_i64_trap:\n    return NVPTXISD::Suld2DArrayI64Trap;\n  case Intrinsic::nvvm_suld_2d_array_v2i8_trap:\n    return NVPTXISD::Suld2DArrayV2I8Trap;\n  case Intrinsic::nvvm_suld_2d_array_v2i16_trap:\n    return NVPTXISD::Suld2DArrayV2I16Trap;\n  case Intrinsic::nvvm_suld_2d_array_v2i32_trap:\n    return NVPTXISD::Suld2DArrayV2I32Trap;\n  case Intrinsic::nvvm_suld_2d_array_v2i64_trap:\n    return NVPTXISD::Suld2DArrayV2I64Trap;\n  case Intrinsic::nvvm_suld_2d_array_v4i8_trap:\n    return NVPTXISD::Suld2DArrayV4I8Trap;\n  case Intrinsic::nvvm_suld_2d_array_v4i16_trap:\n    return NVPTXISD::Suld2DArrayV4I16Trap;\n  case Intrinsic::nvvm_suld_2d_array_v4i32_trap:\n    return NVPTXISD::Suld2DArrayV4I32Trap;\n  case Intrinsic::nvvm_suld_3d_i8_trap:\n    return NVPTXISD::Suld3DI8Trap;\n  case Intrinsic::nvvm_suld_3d_i16_trap:\n    return NVPTXISD::Suld3DI16Trap;\n  case Intrinsic::nvvm_suld_3d_i32_trap:\n    return NVPTXISD::Suld3DI32Trap;\n  case Intrinsic::nvvm_suld_3d_i64_trap:\n    return NVPTXISD::Suld3DI64Trap;\n  case Intrinsic::nvvm_suld_3d_v2i8_trap:\n    return NVPTXISD::Suld3DV2I8Trap;\n  case Intrinsic::nvvm_suld_3d_v2i16_trap:\n    return NVPTXISD::Suld3DV2I16Trap;\n  case Intrinsic::nvvm_suld_3d_v2i32_trap:\n    return NVPTXISD::Suld3DV2I32Trap;\n  case Intrinsic::nvvm_suld_3d_v2i64_trap:\n    return NVPTXISD::Suld3DV2I64Trap;\n  case Intrinsic::nvvm_suld_3d_v4i8_trap:\n    return NVPTXISD::Suld3DV4I8Trap;\n  case Intrinsic::nvvm_suld_3d_v4i16_trap:\n    return NVPTXISD::Suld3DV4I16Trap;\n  case Intrinsic::nvvm_suld_3d_v4i32_trap:\n    return NVPTXISD::Suld3DV4I32Trap;\n  case Intrinsic::nvvm_suld_1d_i8_zero:\n    return NVPTXISD::Suld1DI8Zero;\n  case Intrinsic::nvvm_suld_1d_i16_zero:\n    return NVPTXISD::Suld1DI16Zero;\n  case Intrinsic::nvvm_suld_1d_i32_zero:\n    return NVPTXISD::Suld1DI32Zero;\n  case Intrinsic::nvvm_suld_1d_i64_zero:\n    return NVPTXISD::Suld1DI64Zero;\n  case Intrinsic::nvvm_suld_1d_v2i8_zero:\n    return NVPTXISD::Suld1DV2I8Zero;\n  case Intrinsic::nvvm_suld_1d_v2i16_zero:\n    return NVPTXISD::Suld1DV2I16Zero;\n  case Intrinsic::nvvm_suld_1d_v2i32_zero:\n    return NVPTXISD::Suld1DV2I32Zero;\n  case Intrinsic::nvvm_suld_1d_v2i64_zero:\n    return NVPTXISD::Suld1DV2I64Zero;\n  case Intrinsic::nvvm_suld_1d_v4i8_zero:\n    return NVPTXISD::Suld1DV4I8Zero;\n  case Intrinsic::nvvm_suld_1d_v4i16_zero:\n    return NVPTXISD::Suld1DV4I16Zero;\n  case Intrinsic::nvvm_suld_1d_v4i32_zero:\n    return NVPTXISD::Suld1DV4I32Zero;\n  case Intrinsic::nvvm_suld_1d_array_i8_zero:\n    return NVPTXISD::Suld1DArrayI8Zero;\n  case Intrinsic::nvvm_suld_1d_array_i16_zero:\n    return NVPTXISD::Suld1DArrayI16Zero;\n  case Intrinsic::nvvm_suld_1d_array_i32_zero:\n    return NVPTXISD::Suld1DArrayI32Zero;\n  case Intrinsic::nvvm_suld_1d_array_i64_zero:\n    return NVPTXISD::Suld1DArrayI64Zero;\n  case Intrinsic::nvvm_suld_1d_array_v2i8_zero:\n    return NVPTXISD::Suld1DArrayV2I8Zero;\n  case Intrinsic::nvvm_suld_1d_array_v2i16_zero:\n    return NVPTXISD::Suld1DArrayV2I16Zero;\n  case Intrinsic::nvvm_suld_1d_array_v2i32_zero:\n    return NVPTXISD::Suld1DArrayV2I32Zero;\n  case Intrinsic::nvvm_suld_1d_array_v2i64_zero:\n    return NVPTXISD::Suld1DArrayV2I64Zero;\n  case Intrinsic::nvvm_suld_1d_array_v4i8_zero:\n    return NVPTXISD::Suld1DArrayV4I8Zero;\n  case Intrinsic::nvvm_suld_1d_array_v4i16_zero:\n    return NVPTXISD::Suld1DArrayV4I16Zero;\n  case Intrinsic::nvvm_suld_1d_array_v4i32_zero:\n    return NVPTXISD::Suld1DArrayV4I32Zero;\n  case Intrinsic::nvvm_suld_2d_i8_zero:\n    return NVPTXISD::Suld2DI8Zero;\n  case Intrinsic::nvvm_suld_2d_i16_zero:\n    return NVPTXISD::Suld2DI16Zero;\n  case Intrinsic::nvvm_suld_2d_i32_zero:\n    return NVPTXISD::Suld2DI32Zero;\n  case Intrinsic::nvvm_suld_2d_i64_zero:\n    return NVPTXISD::Suld2DI64Zero;\n  case Intrinsic::nvvm_suld_2d_v2i8_zero:\n    return NVPTXISD::Suld2DV2I8Zero;\n  case Intrinsic::nvvm_suld_2d_v2i16_zero:\n    return NVPTXISD::Suld2DV2I16Zero;\n  case Intrinsic::nvvm_suld_2d_v2i32_zero:\n    return NVPTXISD::Suld2DV2I32Zero;\n  case Intrinsic::nvvm_suld_2d_v2i64_zero:\n    return NVPTXISD::Suld2DV2I64Zero;\n  case Intrinsic::nvvm_suld_2d_v4i8_zero:\n    return NVPTXISD::Suld2DV4I8Zero;\n  case Intrinsic::nvvm_suld_2d_v4i16_zero:\n    return NVPTXISD::Suld2DV4I16Zero;\n  case Intrinsic::nvvm_suld_2d_v4i32_zero:\n    return NVPTXISD::Suld2DV4I32Zero;\n  case Intrinsic::nvvm_suld_2d_array_i8_zero:\n    return NVPTXISD::Suld2DArrayI8Zero;\n  case Intrinsic::nvvm_suld_2d_array_i16_zero:\n    return NVPTXISD::Suld2DArrayI16Zero;\n  case Intrinsic::nvvm_suld_2d_array_i32_zero:\n    return NVPTXISD::Suld2DArrayI32Zero;\n  case Intrinsic::nvvm_suld_2d_array_i64_zero:\n    return NVPTXISD::Suld2DArrayI64Zero;\n  case Intrinsic::nvvm_suld_2d_array_v2i8_zero:\n    return NVPTXISD::Suld2DArrayV2I8Zero;\n  case Intrinsic::nvvm_suld_2d_array_v2i16_zero:\n    return NVPTXISD::Suld2DArrayV2I16Zero;\n  case Intrinsic::nvvm_suld_2d_array_v2i32_zero:\n    return NVPTXISD::Suld2DArrayV2I32Zero;\n  case Intrinsic::nvvm_suld_2d_array_v2i64_zero:\n    return NVPTXISD::Suld2DArrayV2I64Zero;\n  case Intrinsic::nvvm_suld_2d_array_v4i8_zero:\n    return NVPTXISD::Suld2DArrayV4I8Zero;\n  case Intrinsic::nvvm_suld_2d_array_v4i16_zero:\n    return NVPTXISD::Suld2DArrayV4I16Zero;\n  case Intrinsic::nvvm_suld_2d_array_v4i32_zero:\n    return NVPTXISD::Suld2DArrayV4I32Zero;\n  case Intrinsic::nvvm_suld_3d_i8_zero:\n    return NVPTXISD::Suld3DI8Zero;\n  case Intrinsic::nvvm_suld_3d_i16_zero:\n    return NVPTXISD::Suld3DI16Zero;\n  case Intrinsic::nvvm_suld_3d_i32_zero:\n    return NVPTXISD::Suld3DI32Zero;\n  case Intrinsic::nvvm_suld_3d_i64_zero:\n    return NVPTXISD::Suld3DI64Zero;\n  case Intrinsic::nvvm_suld_3d_v2i8_zero:\n    return NVPTXISD::Suld3DV2I8Zero;\n  case Intrinsic::nvvm_suld_3d_v2i16_zero:\n    return NVPTXISD::Suld3DV2I16Zero;\n  case Intrinsic::nvvm_suld_3d_v2i32_zero:\n    return NVPTXISD::Suld3DV2I32Zero;\n  case Intrinsic::nvvm_suld_3d_v2i64_zero:\n    return NVPTXISD::Suld3DV2I64Zero;\n  case Intrinsic::nvvm_suld_3d_v4i8_zero:\n    return NVPTXISD::Suld3DV4I8Zero;\n  case Intrinsic::nvvm_suld_3d_v4i16_zero:\n    return NVPTXISD::Suld3DV4I16Zero;\n  case Intrinsic::nvvm_suld_3d_v4i32_zero:\n    return NVPTXISD::Suld3DV4I32Zero;\n  }\n}\n\n// llvm.ptx.memcpy.const and llvm.ptx.memmove.const need to be modeled as\n// TgtMemIntrinsic\n// because we need the information that is only available in the \"Value\" type\n// of destination\n// pointer. In particular, the address space information.\n"},"isImageOrSamplerVal":{"range":[2039,2060],"code":"static bool isImageOrSamplerVal(const Value *arg, const Module *context) {\n  static const char *const specialTypes[] = { \"struct._image2d_t\",\n                                              \"struct._image3d_t\",\n                                              \"struct._sampler_t\" };\n\n  Type *Ty = arg->getType();\n  auto *PTy = dyn_cast<PointerType>(Ty);\n\n  if (!PTy)\n    return false;\n\n  if (!context)\n    return false;\n\n  auto *STy = dyn_cast<StructType>(PTy->getElementType());\n  if (!STy || STy->isLiteral())\n    return false;\n\n  return std::find(std::begin(specialTypes), std::end(specialTypes),\n                   STy->getName()) != std::end(specialTypes);\n}\n\n"},"LowerShiftRightParts":{"range":[1702,1764],"code":"SDValue NVPTXTargetLowering::LowerShiftRightParts(SDValue Op,\n                                                  SelectionDAG &DAG) const {\n  assert(Op.getNumOperands() == 3 && \"Not a double-shift!\");\n  assert(Op.getOpcode() == ISD::SRA_PARTS || Op.getOpcode() == ISD::SRL_PARTS);\n\n  EVT VT = Op.getValueType();\n  unsigned VTBits = VT.getSizeInBits();\n  SDLoc dl(Op);\n  SDValue ShOpLo = Op.getOperand(0);\n  SDValue ShOpHi = Op.getOperand(1);\n  SDValue ShAmt  = Op.getOperand(2);\n  unsigned Opc = (Op.getOpcode() == ISD::SRA_PARTS) ? ISD::SRA : ISD::SRL;\n\n  if (VTBits == 32 && STI.getSmVersion() >= 35) {\n\n    // For 32bit and sm35, we can use the funnel shift 'shf' instruction.\n    // {dHi, dLo} = {aHi, aLo} >> Amt\n    //   dHi = aHi >> Amt\n    //   dLo = shf.r.clamp aLo, aHi, Amt\n\n    SDValue Hi = DAG.getNode(Opc, dl, VT, ShOpHi, ShAmt);\n    SDValue Lo = DAG.getNode(NVPTXISD::FUN_SHFR_CLAMP, dl, VT, ShOpLo, ShOpHi,\n                             ShAmt);\n\n    SDValue Ops[2] = { Lo, Hi };\n    return DAG.getMergeValues(Ops, dl);\n  }\n  else {\n\n    // {dHi, dLo} = {aHi, aLo} >> Amt\n    // - if (Amt>=size) then\n    //      dLo = aHi >> (Amt-size)\n    //      dHi = aHi >> Amt (this is either all 0 or all 1)\n    //   else\n    //      dLo = (aLo >>logic Amt) | (aHi << (size-Amt))\n    //      dHi = aHi >> Amt\n\n    SDValue RevShAmt = DAG.getNode(ISD::SUB, dl, MVT::i32,\n                                   DAG.getConstant(VTBits, dl, MVT::i32),\n                                   ShAmt);\n    SDValue Tmp1 = DAG.getNode(ISD::SRL, dl, VT, ShOpLo, ShAmt);\n    SDValue ExtraShAmt = DAG.getNode(ISD::SUB, dl, MVT::i32, ShAmt,\n                                     DAG.getConstant(VTBits, dl, MVT::i32));\n    SDValue Tmp2 = DAG.getNode(ISD::SHL, dl, VT, ShOpHi, RevShAmt);\n    SDValue FalseVal = DAG.getNode(ISD::OR, dl, VT, Tmp1, Tmp2);\n    SDValue TrueVal = DAG.getNode(Opc, dl, VT, ShOpHi, ExtraShAmt);\n\n    SDValue Cmp = DAG.getSetCC(dl, MVT::i1, ShAmt,\n                               DAG.getConstant(VTBits, dl, MVT::i32),\n                               ISD::SETGE);\n    SDValue Hi = DAG.getNode(Opc, dl, VT, ShOpHi, ShAmt);\n    SDValue Lo = DAG.getNode(ISD::SELECT, dl, VT, Cmp, TrueVal, FalseVal);\n\n    SDValue Ops[2] = { Lo, Hi };\n    return DAG.getMergeValues(Ops, dl);\n  }\n}\n\n/// LowerShiftLeftParts - Lower SHL_PARTS, which\n/// 1) returns two i32 values and take a 2 x i32 value to shift plus a shift\n///    amount, or\n/// 2) returns two i64 values and take a 2 x i64 value to shift plus a shift\n///    amount.\n"},"LowerLOAD":{"range":[1870,1880],"code":"SDValue NVPTXTargetLowering::LowerLOAD(SDValue Op, SelectionDAG &DAG) const {\n  if (Op.getValueType() == MVT::i1)\n    return LowerLOADi1(Op, DAG);\n  else\n    return SDValue();\n}\n\n// v = ld i1* addr\n//   =>\n// v1 = ld i8* addr (-> i16)\n// v = trunc i16 to i1\n"},"getPrototype":{"range":[900,1005],"code":"std::string NVPTXTargetLowering::getPrototype(\n    const DataLayout &DL, Type *retTy, const ArgListTy &Args,\n    const SmallVectorImpl<ISD::OutputArg> &Outs, unsigned retAlignment,\n    const ImmutableCallSite *CS) const {\n  auto PtrVT = getPointerTy(DL);\n\n  bool isABI = (STI.getSmVersion() >= 20);\n  assert(isABI && \"Non-ABI compilation is not supported\");\n  if (!isABI)\n    return \"\";\n\n  std::stringstream O;\n  O << \"prototype_\" << uniqueCallSite << \" : .callprototype \";\n\n  if (retTy->getTypeID() == Type::VoidTyID) {\n    O << \"()\";\n  } else {\n    O << \"(\";\n    if (retTy->isFloatingPointTy() || retTy->isIntegerTy()) {\n      unsigned size = 0;\n      if (auto *ITy = dyn_cast<IntegerType>(retTy)) {\n        size = ITy->getBitWidth();\n        if (size < 32)\n          size = 32;\n      } else {\n        assert(retTy->isFloatingPointTy() &&\n               \"Floating point type expected here\");\n        size = retTy->getPrimitiveSizeInBits();\n      }\n\n      O << \".param .b\" << size << \" _\";\n    } else if (isa<PointerType>(retTy)) {\n      O << \".param .b\" << PtrVT.getSizeInBits() << \" _\";\n    } else if ((retTy->getTypeID() == Type::StructTyID) ||\n               isa<VectorType>(retTy)) {\n      auto &DL = CS->getCalledFunction()->getParent()->getDataLayout();\n      O << \".param .align \" << retAlignment << \" .b8 _[\"\n        << DL.getTypeAllocSize(retTy) << \"]\";\n    } else {\n      llvm_unreachable(\"Unknown return type\");\n    }\n    O << \") \";\n  }\n  O << \"_ (\";\n\n  bool first = true;\n\n  unsigned OIdx = 0;\n  for (unsigned i = 0, e = Args.size(); i != e; ++i, ++OIdx) {\n    Type *Ty = Args[i].Ty;\n    if (!first) {\n      O << \", \";\n    }\n    first = false;\n\n    if (!Outs[OIdx].Flags.isByVal()) {\n      if (Ty->isAggregateType() || Ty->isVectorTy()) {\n        unsigned align = 0;\n        const CallInst *CallI = cast<CallInst>(CS->getInstruction());\n        // +1 because index 0 is reserved for return type alignment\n        if (!llvm::getAlign(*CallI, i + 1, align))\n          align = DL.getABITypeAlignment(Ty);\n        unsigned sz = DL.getTypeAllocSize(Ty);\n        O << \".param .align \" << align << \" .b8 \";\n        O << \"_\";\n        O << \"[\" << sz << \"]\";\n        // update the index for Outs\n        SmallVector<EVT, 16> vtparts;\n        ComputeValueVTs(*this, DL, Ty, vtparts);\n        if (unsigned len = vtparts.size())\n          OIdx += len - 1;\n        continue;\n      }\n       // i8 types in IR will be i16 types in SDAG\n      assert((getValueType(DL, Ty) == Outs[OIdx].VT ||\n              (getValueType(DL, Ty) == MVT::i8 && Outs[OIdx].VT == MVT::i16)) &&\n             \"type mismatch between callee prototype and arguments\");\n      // scalar type\n      unsigned sz = 0;\n      if (isa<IntegerType>(Ty)) {\n        sz = cast<IntegerType>(Ty)->getBitWidth();\n        if (sz < 32)\n          sz = 32;\n      } else if (isa<PointerType>(Ty))\n        sz = PtrVT.getSizeInBits();\n      else\n        sz = Ty->getPrimitiveSizeInBits();\n      O << \".param .b\" << sz << \" \";\n      O << \"_\";\n      continue;\n    }\n    auto *PTy = dyn_cast<PointerType>(Ty);\n    assert(PTy && \"Param with byval attribute should be a pointer type\");\n    Type *ETy = PTy->getElementType();\n\n    unsigned align = Outs[OIdx].Flags.getByValAlign();\n    unsigned sz = DL.getTypeAllocSize(ETy);\n    O << \".param .align \" << align << \" .b8 \";\n    O << \"_\";\n    O << \"[\" << sz << \"]\";\n  }\n  O << \");\";\n  return O.str();\n}\n\nunsigned\n"},"ReplaceLoadVector":{"range":[4278,4378],"code":"static void ReplaceLoadVector(SDNode *N, SelectionDAG &DAG,\n                              SmallVectorImpl<SDValue> &Results) {\n  EVT ResVT = N->getValueType(0);\n  SDLoc DL(N);\n\n  assert(ResVT.isVector() && \"Vector load must have vector type\");\n\n  // We only handle \"native\" vector sizes for now, e.g. <4 x double> is not\n  // legal.  We can (and should) split that into 2 loads of <2 x double> here\n  // but I'm leaving that as a TODO for now.\n  assert(ResVT.isSimple() && \"Can only handle simple types\");\n  switch (ResVT.getSimpleVT().SimpleTy) {\n  default:\n    return;\n  case MVT::v2i8:\n  case MVT::v2i16:\n  case MVT::v2i32:\n  case MVT::v2i64:\n  case MVT::v2f32:\n  case MVT::v2f64:\n  case MVT::v4i8:\n  case MVT::v4i16:\n  case MVT::v4i32:\n  case MVT::v4f32:\n    // This is a \"native\" vector type\n    break;\n  }\n\n  LoadSDNode *LD = cast<LoadSDNode>(N);\n\n  unsigned Align = LD->getAlignment();\n  auto &TD = DAG.getDataLayout();\n  unsigned PrefAlign =\n      TD.getPrefTypeAlignment(ResVT.getTypeForEVT(*DAG.getContext()));\n  if (Align < PrefAlign) {\n    // This load is not sufficiently aligned, so bail out and let this vector\n    // load be scalarized.  Note that we may still be able to emit smaller\n    // vector loads.  For example, if we are loading a <4 x float> with an\n    // alignment of 8, this check will fail but the legalizer will try again\n    // with 2 x <2 x float>, which will succeed with an alignment of 8.\n    return;\n  }\n\n  EVT EltVT = ResVT.getVectorElementType();\n  unsigned NumElts = ResVT.getVectorNumElements();\n\n  // Since LoadV2 is a target node, we cannot rely on DAG type legalization.\n  // Therefore, we must ensure the type is legal.  For i1 and i8, we set the\n  // loaded type to i16 and propagate the \"real\" type as the memory type.\n  bool NeedTrunc = false;\n  if (EltVT.getSizeInBits() < 16) {\n    EltVT = MVT::i16;\n    NeedTrunc = true;\n  }\n\n  unsigned Opcode = 0;\n  SDVTList LdResVTs;\n\n  switch (NumElts) {\n  default:\n    return;\n  case 2:\n    Opcode = NVPTXISD::LoadV2;\n    LdResVTs = DAG.getVTList(EltVT, EltVT, MVT::Other);\n    break;\n  case 4: {\n    Opcode = NVPTXISD::LoadV4;\n    EVT ListVTs[] = { EltVT, EltVT, EltVT, EltVT, MVT::Other };\n    LdResVTs = DAG.getVTList(ListVTs);\n    break;\n  }\n  }\n\n  // Copy regular operands\n  SmallVector<SDValue, 8> OtherOps(N->op_begin(), N->op_end());\n\n  // The select routine does not have access to the LoadSDNode instance, so\n  // pass along the extension information\n  OtherOps.push_back(DAG.getIntPtrConstant(LD->getExtensionType(), DL));\n\n  SDValue NewLD = DAG.getMemIntrinsicNode(Opcode, DL, LdResVTs, OtherOps,\n                                          LD->getMemoryVT(),\n                                          LD->getMemOperand());\n\n  SmallVector<SDValue, 4> ScalarRes;\n\n  for (unsigned i = 0; i < NumElts; ++i) {\n    SDValue Res = NewLD.getValue(i);\n    if (NeedTrunc)\n      Res = DAG.getNode(ISD::TRUNCATE, DL, ResVT.getVectorElementType(), Res);\n    ScalarRes.push_back(Res);\n  }\n\n  SDValue LoadChain = NewLD.getValue(NumElts);\n\n  SDValue BuildVec = DAG.getBuildVector(ResVT, DL, ScalarRes);\n\n  Results.push_back(BuildVec);\n  Results.push_back(LoadChain);\n}\n\n"},"LowerGlobalAddress":{"range":[892,899],"code":"NVPTXTargetLowering::LowerGlobalAddress(SDValue Op, SelectionDAG &DAG) const {\n  SDLoc dl(Op);\n  const GlobalValue *GV = cast<GlobalAddressSDNode>(Op)->getGlobal();\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n  Op = DAG.getTargetGlobalAddress(GV, dl, PtrVT);\n  return DAG.getNode(NVPTXISD::Wrapper, dl, PtrVT, Op);\n}\n\n"},"LowerSTORE":{"range":[1899,1909],"code":"SDValue NVPTXTargetLowering::LowerSTORE(SDValue Op, SelectionDAG &DAG) const {\n  EVT ValVT = Op.getOperand(1).getValueType();\n  if (ValVT == MVT::i1)\n    return LowerSTOREi1(Op, DAG);\n  else if (ValVT.isVector())\n    return LowerSTOREVector(Op, DAG);\n  else\n    return SDValue();\n}\n\nSDValue\n"},"LowerOperation":{"range":[1823,1853],"code":"NVPTXTargetLowering::LowerOperation(SDValue Op, SelectionDAG &DAG) const {\n  switch (Op.getOpcode()) {\n  case ISD::RETURNADDR:\n    return SDValue();\n  case ISD::FRAMEADDR:\n    return SDValue();\n  case ISD::GlobalAddress:\n    return LowerGlobalAddress(Op, DAG);\n  case ISD::INTRINSIC_W_CHAIN:\n    return Op;\n  case ISD::BUILD_VECTOR:\n  case ISD::EXTRACT_SUBVECTOR:\n    return Op;\n  case ISD::CONCAT_VECTORS:\n    return LowerCONCAT_VECTORS(Op, DAG);\n  case ISD::STORE:\n    return LowerSTORE(Op, DAG);\n  case ISD::LOAD:\n    return LowerLOAD(Op, DAG);\n  case ISD::SHL_PARTS:\n    return LowerShiftLeftParts(Op, DAG);\n  case ISD::SRA_PARTS:\n  case ISD::SRL_PARTS:\n    return LowerShiftRightParts(Op, DAG);\n  case ISD::SELECT:\n    return LowerSelect(Op, DAG);\n  default:\n    llvm_unreachable(\"Custom lowering not defined for operation\");\n  }\n}\n\n"},"LowerLOADi1":{"range":[1881,1898],"code":"SDValue NVPTXTargetLowering::LowerLOADi1(SDValue Op, SelectionDAG &DAG) const {\n  SDNode *Node = Op.getNode();\n  LoadSDNode *LD = cast<LoadSDNode>(Node);\n  SDLoc dl(Node);\n  assert(LD->getExtensionType() == ISD::NON_EXTLOAD);\n  assert(Node->getValueType(0) == MVT::i1 &&\n         \"Custom lowering for i1 load only\");\n  SDValue newLD = DAG.getLoad(MVT::i16, dl, LD->getChain(), LD->getBasePtr(),\n                              LD->getPointerInfo(), LD->getAlignment(),\n                              LD->getMemOperand()->getFlags());\n  SDValue result = DAG.getNode(ISD::TRUNCATE, dl, MVT::i1, newLD);\n  // The legalizer (the caller) is expecting two values from the legalized\n  // load, so we build a MergeValues node for it. See ExpandUnalignedLoad()\n  // in LegalizeDAG.cpp which also uses MergeValues.\n  SDValue Ops[] = { result, LD->getChain() };\n  return DAG.getMergeValues(Ops, dl);\n}\n\n"},"ReplaceINTRINSIC_W_CHAIN":{"range":[4379,4516],"code":"static void ReplaceINTRINSIC_W_CHAIN(SDNode *N, SelectionDAG &DAG,\n                                     SmallVectorImpl<SDValue> &Results) {\n  SDValue Chain = N->getOperand(0);\n  SDValue Intrin = N->getOperand(1);\n  SDLoc DL(N);\n\n  // Get the intrinsic ID\n  unsigned IntrinNo = cast<ConstantSDNode>(Intrin.getNode())->getZExtValue();\n  switch (IntrinNo) {\n  default:\n    return;\n  case Intrinsic::nvvm_ldg_global_i:\n  case Intrinsic::nvvm_ldg_global_f:\n  case Intrinsic::nvvm_ldg_global_p:\n  case Intrinsic::nvvm_ldu_global_i:\n  case Intrinsic::nvvm_ldu_global_f:\n  case Intrinsic::nvvm_ldu_global_p: {\n    EVT ResVT = N->getValueType(0);\n\n    if (ResVT.isVector()) {\n      // Vector LDG/LDU\n\n      unsigned NumElts = ResVT.getVectorNumElements();\n      EVT EltVT = ResVT.getVectorElementType();\n\n      // Since LDU/LDG are target nodes, we cannot rely on DAG type\n      // legalization.\n      // Therefore, we must ensure the type is legal.  For i1 and i8, we set the\n      // loaded type to i16 and propagate the \"real\" type as the memory type.\n      bool NeedTrunc = false;\n      if (EltVT.getSizeInBits() < 16) {\n        EltVT = MVT::i16;\n        NeedTrunc = true;\n      }\n\n      unsigned Opcode = 0;\n      SDVTList LdResVTs;\n\n      switch (NumElts) {\n      default:\n        return;\n      case 2:\n        switch (IntrinNo) {\n        default:\n          return;\n        case Intrinsic::nvvm_ldg_global_i:\n        case Intrinsic::nvvm_ldg_global_f:\n        case Intrinsic::nvvm_ldg_global_p:\n          Opcode = NVPTXISD::LDGV2;\n          break;\n        case Intrinsic::nvvm_ldu_global_i:\n        case Intrinsic::nvvm_ldu_global_f:\n        case Intrinsic::nvvm_ldu_global_p:\n          Opcode = NVPTXISD::LDUV2;\n          break;\n        }\n        LdResVTs = DAG.getVTList(EltVT, EltVT, MVT::Other);\n        break;\n      case 4: {\n        switch (IntrinNo) {\n        default:\n          return;\n        case Intrinsic::nvvm_ldg_global_i:\n        case Intrinsic::nvvm_ldg_global_f:\n        case Intrinsic::nvvm_ldg_global_p:\n          Opcode = NVPTXISD::LDGV4;\n          break;\n        case Intrinsic::nvvm_ldu_global_i:\n        case Intrinsic::nvvm_ldu_global_f:\n        case Intrinsic::nvvm_ldu_global_p:\n          Opcode = NVPTXISD::LDUV4;\n          break;\n        }\n        EVT ListVTs[] = { EltVT, EltVT, EltVT, EltVT, MVT::Other };\n        LdResVTs = DAG.getVTList(ListVTs);\n        break;\n      }\n      }\n\n      SmallVector<SDValue, 8> OtherOps;\n\n      // Copy regular operands\n\n      OtherOps.push_back(Chain); // Chain\n                                 // Skip operand 1 (intrinsic ID)\n      // Others\n      OtherOps.append(N->op_begin() + 2, N->op_end());\n\n      MemIntrinsicSDNode *MemSD = cast<MemIntrinsicSDNode>(N);\n\n      SDValue NewLD = DAG.getMemIntrinsicNode(Opcode, DL, LdResVTs, OtherOps,\n                                              MemSD->getMemoryVT(),\n                                              MemSD->getMemOperand());\n\n      SmallVector<SDValue, 4> ScalarRes;\n\n      for (unsigned i = 0; i < NumElts; ++i) {\n        SDValue Res = NewLD.getValue(i);\n        if (NeedTrunc)\n          Res =\n              DAG.getNode(ISD::TRUNCATE, DL, ResVT.getVectorElementType(), Res);\n        ScalarRes.push_back(Res);\n      }\n\n      SDValue LoadChain = NewLD.getValue(NumElts);\n\n      SDValue BuildVec =\n          DAG.getBuildVector(ResVT, DL, ScalarRes);\n\n      Results.push_back(BuildVec);\n      Results.push_back(LoadChain);\n    } else {\n      // i8 LDG/LDU\n      assert(ResVT.isSimple() && ResVT.getSimpleVT().SimpleTy == MVT::i8 &&\n             \"Custom handling of non-i8 ldu/ldg?\");\n\n      // Just copy all operands as-is\n      SmallVector<SDValue, 4> Ops(N->op_begin(), N->op_end());\n\n      // Force output to i16\n      SDVTList LdResVTs = DAG.getVTList(MVT::i16, MVT::Other);\n\n      MemIntrinsicSDNode *MemSD = cast<MemIntrinsicSDNode>(N);\n\n      // We make sure the memory type is i8, which will be used during isel\n      // to select the proper instruction.\n      SDValue NewLD =\n          DAG.getMemIntrinsicNode(ISD::INTRINSIC_W_CHAIN, DL, LdResVTs, Ops,\n                                  MVT::i8, MemSD->getMemOperand());\n\n      Results.push_back(DAG.getNode(ISD::TRUNCATE, DL, MVT::i8,\n                                    NewLD.getValue(0)));\n      Results.push_back(NewLD.getValue(1));\n    }\n  }\n  }\n}\n\n"},"getPreferredVectorAction":{"range":[884,891],"code":"NVPTXTargetLowering::getPreferredVectorAction(EVT VT) const {\n  if (VT.getVectorNumElements() != 1 && VT.getScalarType() == MVT::i1)\n    return TypeSplitVector;\n\n  return TargetLoweringBase::getPreferredVectorAction(VT);\n}\n\nSDValue\n"},"PerformANDCombine":{"range":[3956,4030],"code":"static SDValue PerformANDCombine(SDNode *N,\n                                 TargetLowering::DAGCombinerInfo &DCI) {\n  // The type legalizer turns a vector load of i8 values into a zextload to i16\n  // registers, optionally ANY_EXTENDs it (if target type is integer),\n  // and ANDs off the high 8 bits. Since we turn this load into a\n  // target-specific DAG node, the DAG combiner fails to eliminate these AND\n  // nodes. Do that here.\n  SDValue Val = N->getOperand(0);\n  SDValue Mask = N->getOperand(1);\n\n  if (isa<ConstantSDNode>(Val)) {\n    std::swap(Val, Mask);\n  }\n\n  SDValue AExt;\n  // Generally, we will see zextload -> IMOV16rr -> ANY_EXTEND -> and\n  if (Val.getOpcode() == ISD::ANY_EXTEND) {\n    AExt = Val;\n    Val = Val->getOperand(0);\n  }\n\n  if (Val->isMachineOpcode() && Val->getMachineOpcode() == NVPTX::IMOV16rr) {\n    Val = Val->getOperand(0);\n  }\n\n  if (Val->getOpcode() == NVPTXISD::LoadV2 ||\n      Val->getOpcode() == NVPTXISD::LoadV4) {\n    ConstantSDNode *MaskCnst = dyn_cast<ConstantSDNode>(Mask);\n    if (!MaskCnst) {\n      // Not an AND with a constant\n      return SDValue();\n    }\n\n    uint64_t MaskVal = MaskCnst->getZExtValue();\n    if (MaskVal != 0xff) {\n      // Not an AND that chops off top 8 bits\n      return SDValue();\n    }\n\n    MemSDNode *Mem = dyn_cast<MemSDNode>(Val);\n    if (!Mem) {\n      // Not a MemSDNode?!?\n      return SDValue();\n    }\n\n    EVT MemVT = Mem->getMemoryVT();\n    if (MemVT != MVT::v2i8 && MemVT != MVT::v4i8) {\n      // We only handle the i8 case\n      return SDValue();\n    }\n\n    unsigned ExtType =\n      cast<ConstantSDNode>(Val->getOperand(Val->getNumOperands()-1))->\n        getZExtValue();\n    if (ExtType == ISD::SEXTLOAD) {\n      // If for some reason the load is a sextload, the and is needed to zero\n      // out the high 8 bits\n      return SDValue();\n    }\n\n    bool AddTo = false;\n    if (AExt.getNode() != 0) {\n      // Re-insert the ext as a zext.\n      Val = DCI.DAG.getNode(ISD::ZERO_EXTEND, SDLoc(N),\n                            AExt.getValueType(), Val);\n      AddTo = true;\n    }\n\n    // If we get here, the AND is unnecessary.  Just replace it with the load\n    DCI.CombineTo(N, Val, AddTo);\n  }\n\n  return SDValue();\n}\n\n"},"TryMULWIDECombine":{"range":[4165,4231],"code":"static SDValue TryMULWIDECombine(SDNode *N,\n                                 TargetLowering::DAGCombinerInfo &DCI) {\n  EVT MulType = N->getValueType(0);\n  if (MulType != MVT::i32 && MulType != MVT::i64) {\n    return SDValue();\n  }\n\n  SDLoc DL(N);\n  unsigned OptSize = MulType.getSizeInBits() >> 1;\n  SDValue LHS = N->getOperand(0);\n  SDValue RHS = N->getOperand(1);\n\n  // Canonicalize the multiply so the constant (if any) is on the right\n  if (N->getOpcode() == ISD::MUL) {\n    if (isa<ConstantSDNode>(LHS)) {\n      std::swap(LHS, RHS);\n    }\n  }\n\n  // If we have a SHL, determine the actual multiply amount\n  if (N->getOpcode() == ISD::SHL) {\n    ConstantSDNode *ShlRHS = dyn_cast<ConstantSDNode>(RHS);\n    if (!ShlRHS) {\n      return SDValue();\n    }\n\n    APInt ShiftAmt = ShlRHS->getAPIntValue();\n    unsigned BitWidth = MulType.getSizeInBits();\n    if (ShiftAmt.sge(0) && ShiftAmt.slt(BitWidth)) {\n      APInt MulVal = APInt(BitWidth, 1) << ShiftAmt;\n      RHS = DCI.DAG.getConstant(MulVal, DL, MulType);\n    } else {\n      return SDValue();\n    }\n  }\n\n  bool Signed;\n  // Verify that our operands are demotable\n  if (!AreMulWideOperandsDemotable(LHS, RHS, OptSize, Signed)) {\n    return SDValue();\n  }\n\n  EVT DemotedVT;\n  if (MulType == MVT::i32) {\n    DemotedVT = MVT::i16;\n  } else {\n    DemotedVT = MVT::i32;\n  }\n\n  // Truncate the operands to the correct size. Note that these are just for\n  // type consistency and will (likely) be eliminated in later phases.\n  SDValue TruncLHS =\n    DCI.DAG.getNode(ISD::TRUNCATE, DL, DemotedVT, LHS);\n  SDValue TruncRHS =\n    DCI.DAG.getNode(ISD::TRUNCATE, DL, DemotedVT, RHS);\n\n  unsigned Opc;\n  if (Signed) {\n    Opc = NVPTXISD::MUL_WIDE_SIGNED;\n  } else {\n    Opc = NVPTXISD::MUL_WIDE_UNSIGNED;\n  }\n\n  return DCI.DAG.getNode(Opc, DL, MulType, TruncLHS, TruncRHS);\n}\n\n/// PerformMULCombine - Runs PTX-specific DAG combine patterns on MUL nodes.\n"},"LowerSTOREVector":{"range":[1910,2008],"code":"NVPTXTargetLowering::LowerSTOREVector(SDValue Op, SelectionDAG &DAG) const {\n  SDNode *N = Op.getNode();\n  SDValue Val = N->getOperand(1);\n  SDLoc DL(N);\n  EVT ValVT = Val.getValueType();\n\n  if (ValVT.isVector()) {\n    // We only handle \"native\" vector sizes for now, e.g. <4 x double> is not\n    // legal.  We can (and should) split that into 2 stores of <2 x double> here\n    // but I'm leaving that as a TODO for now.\n    if (!ValVT.isSimple())\n      return SDValue();\n    switch (ValVT.getSimpleVT().SimpleTy) {\n    default:\n      return SDValue();\n    case MVT::v2i8:\n    case MVT::v2i16:\n    case MVT::v2i32:\n    case MVT::v2i64:\n    case MVT::v2f32:\n    case MVT::v2f64:\n    case MVT::v4i8:\n    case MVT::v4i16:\n    case MVT::v4i32:\n    case MVT::v4f32:\n      // This is a \"native\" vector type\n      break;\n    }\n\n    MemSDNode *MemSD = cast<MemSDNode>(N);\n    const DataLayout &TD = DAG.getDataLayout();\n\n    unsigned Align = MemSD->getAlignment();\n    unsigned PrefAlign =\n        TD.getPrefTypeAlignment(ValVT.getTypeForEVT(*DAG.getContext()));\n    if (Align < PrefAlign) {\n      // This store is not sufficiently aligned, so bail out and let this vector\n      // store be scalarized.  Note that we may still be able to emit smaller\n      // vector stores.  For example, if we are storing a <4 x float> with an\n      // alignment of 8, this check will fail but the legalizer will try again\n      // with 2 x <2 x float>, which will succeed with an alignment of 8.\n      return SDValue();\n    }\n\n    unsigned Opcode = 0;\n    EVT EltVT = ValVT.getVectorElementType();\n    unsigned NumElts = ValVT.getVectorNumElements();\n\n    // Since StoreV2 is a target node, we cannot rely on DAG type legalization.\n    // Therefore, we must ensure the type is legal.  For i1 and i8, we set the\n    // stored type to i16 and propagate the \"real\" type as the memory type.\n    bool NeedExt = false;\n    if (EltVT.getSizeInBits() < 16)\n      NeedExt = true;\n\n    switch (NumElts) {\n    default:\n      return SDValue();\n    case 2:\n      Opcode = NVPTXISD::StoreV2;\n      break;\n    case 4: {\n      Opcode = NVPTXISD::StoreV4;\n      break;\n    }\n    }\n\n    SmallVector<SDValue, 8> Ops;\n\n    // First is the chain\n    Ops.push_back(N->getOperand(0));\n\n    // Then the split values\n    for (unsigned i = 0; i < NumElts; ++i) {\n      SDValue ExtVal = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, DL, EltVT, Val,\n                                   DAG.getIntPtrConstant(i, DL));\n      if (NeedExt)\n        ExtVal = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i16, ExtVal);\n      Ops.push_back(ExtVal);\n    }\n\n    // Then any remaining arguments\n    Ops.append(N->op_begin() + 2, N->op_end());\n\n    SDValue NewSt = DAG.getMemIntrinsicNode(\n        Opcode, DL, DAG.getVTList(MVT::Other), Ops,\n        MemSD->getMemoryVT(), MemSD->getMemOperand());\n\n    //return DCI.CombineTo(N, NewSt, true);\n    return NewSt;\n  }\n\n  return SDValue();\n}\n\n// st i1 v, addr\n//    =>\n// v1 = zxt v to i16\n// st.u8 i16, addr\n"},"NVPTXTargetLowering":{"range":[109,286],"code":"NVPTXTargetLowering::NVPTXTargetLowering(const NVPTXTargetMachine &TM,\n                                         const NVPTXSubtarget &STI)\n    : TargetLowering(TM), nvTM(&TM), STI(STI) {\n\n  // always lower memset, memcpy, and memmove intrinsics to load/store\n  // instructions, rather\n  // then generating calls to memset, mempcy or memmove.\n  MaxStoresPerMemset = (unsigned) 0xFFFFFFFF;\n  MaxStoresPerMemcpy = (unsigned) 0xFFFFFFFF;\n  MaxStoresPerMemmove = (unsigned) 0xFFFFFFFF;\n\n  setBooleanContents(ZeroOrNegativeOneBooleanContent);\n  setBooleanVectorContents(ZeroOrNegativeOneBooleanContent);\n\n  // Jump is Expensive. Don't create extra control flow for 'and', 'or'\n  // condition branches.\n  setJumpIsExpensive(true);\n\n  // Wide divides are _very_ slow. Try to reduce the width of the divide if\n  // possible.\n  addBypassSlowDiv(64, 32);\n\n  // By default, use the Source scheduling\n  if (sched4reg)\n    setSchedulingPreference(Sched::RegPressure);\n  else\n    setSchedulingPreference(Sched::Source);\n\n  addRegisterClass(MVT::i1, &NVPTX::Int1RegsRegClass);\n  addRegisterClass(MVT::i16, &NVPTX::Int16RegsRegClass);\n  addRegisterClass(MVT::i32, &NVPTX::Int32RegsRegClass);\n  addRegisterClass(MVT::i64, &NVPTX::Int64RegsRegClass);\n  addRegisterClass(MVT::f32, &NVPTX::Float32RegsRegClass);\n  addRegisterClass(MVT::f64, &NVPTX::Float64RegsRegClass);\n\n  // Operations not directly supported by NVPTX.\n  setOperationAction(ISD::SELECT_CC, MVT::f32, Expand);\n  setOperationAction(ISD::SELECT_CC, MVT::f64, Expand);\n  setOperationAction(ISD::SELECT_CC, MVT::i1, Expand);\n  setOperationAction(ISD::SELECT_CC, MVT::i8, Expand);\n  setOperationAction(ISD::SELECT_CC, MVT::i16, Expand);\n  setOperationAction(ISD::SELECT_CC, MVT::i32, Expand);\n  setOperationAction(ISD::SELECT_CC, MVT::i64, Expand);\n  setOperationAction(ISD::BR_CC, MVT::f32, Expand);\n  setOperationAction(ISD::BR_CC, MVT::f64, Expand);\n  setOperationAction(ISD::BR_CC, MVT::i1, Expand);\n  setOperationAction(ISD::BR_CC, MVT::i8, Expand);\n  setOperationAction(ISD::BR_CC, MVT::i16, Expand);\n  setOperationAction(ISD::BR_CC, MVT::i32, Expand);\n  setOperationAction(ISD::BR_CC, MVT::i64, Expand);\n  // Some SIGN_EXTEND_INREG can be done using cvt instruction.\n  // For others we will expand to a SHL/SRA pair.\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i64, Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i32, Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i16, Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i8 , Legal);\n  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i1, Expand);\n\n  setOperationAction(ISD::SHL_PARTS, MVT::i32  , Custom);\n  setOperationAction(ISD::SRA_PARTS, MVT::i32  , Custom);\n  setOperationAction(ISD::SRL_PARTS, MVT::i32  , Custom);\n  setOperationAction(ISD::SHL_PARTS, MVT::i64  , Custom);\n  setOperationAction(ISD::SRA_PARTS, MVT::i64  , Custom);\n  setOperationAction(ISD::SRL_PARTS, MVT::i64  , Custom);\n\n  if (STI.hasROT64()) {\n    setOperationAction(ISD::ROTL, MVT::i64, Legal);\n    setOperationAction(ISD::ROTR, MVT::i64, Legal);\n  } else {\n    setOperationAction(ISD::ROTL, MVT::i64, Expand);\n    setOperationAction(ISD::ROTR, MVT::i64, Expand);\n  }\n  if (STI.hasROT32()) {\n    setOperationAction(ISD::ROTL, MVT::i32, Legal);\n    setOperationAction(ISD::ROTR, MVT::i32, Legal);\n  } else {\n    setOperationAction(ISD::ROTL, MVT::i32, Expand);\n    setOperationAction(ISD::ROTR, MVT::i32, Expand);\n  }\n\n  setOperationAction(ISD::ROTL, MVT::i16, Expand);\n  setOperationAction(ISD::ROTR, MVT::i16, Expand);\n  setOperationAction(ISD::ROTL, MVT::i8, Expand);\n  setOperationAction(ISD::ROTR, MVT::i8, Expand);\n  setOperationAction(ISD::BSWAP, MVT::i16, Expand);\n  setOperationAction(ISD::BSWAP, MVT::i32, Expand);\n  setOperationAction(ISD::BSWAP, MVT::i64, Expand);\n\n  // Indirect branch is not supported.\n  // This also disables Jump Table creation.\n  setOperationAction(ISD::BR_JT, MVT::Other, Expand);\n  setOperationAction(ISD::BRIND, MVT::Other, Expand);\n\n  setOperationAction(ISD::GlobalAddress, MVT::i32, Custom);\n  setOperationAction(ISD::GlobalAddress, MVT::i64, Custom);\n\n  // We want to legalize constant related memmove and memcopy\n  // intrinsics.\n  setOperationAction(ISD::INTRINSIC_W_CHAIN, MVT::Other, Custom);\n\n  // Turn FP extload into load/fextend\n  setLoadExtAction(ISD::EXTLOAD, MVT::f32, MVT::f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f32, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::v2f32, MVT::v2f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::v2f64, MVT::v2f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::v2f64, MVT::v2f32, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::v4f32, MVT::v4f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::v4f64, MVT::v4f16, Expand);\n  setLoadExtAction(ISD::EXTLOAD, MVT::v4f64, MVT::v4f32, Expand);\n  // Turn FP truncstore into trunc + store.\n  // FIXME: vector types should also be expanded\n  setTruncStoreAction(MVT::f32, MVT::f16, Expand);\n  setTruncStoreAction(MVT::f64, MVT::f16, Expand);\n  setTruncStoreAction(MVT::f64, MVT::f32, Expand);\n\n  // PTX does not support load / store predicate registers\n  setOperationAction(ISD::LOAD, MVT::i1, Custom);\n  setOperationAction(ISD::STORE, MVT::i1, Custom);\n\n  for (MVT VT : MVT::integer_valuetypes()) {\n    setLoadExtAction(ISD::SEXTLOAD, VT, MVT::i1, Promote);\n    setLoadExtAction(ISD::ZEXTLOAD, VT, MVT::i1, Promote);\n    setTruncStoreAction(VT, MVT::i1, Expand);\n  }\n\n  // This is legal in NVPTX\n  setOperationAction(ISD::ConstantFP, MVT::f64, Legal);\n  setOperationAction(ISD::ConstantFP, MVT::f32, Legal);\n\n  // TRAP can be lowered to PTX trap\n  setOperationAction(ISD::TRAP, MVT::Other, Legal);\n\n  setOperationAction(ISD::ADDC, MVT::i64, Expand);\n  setOperationAction(ISD::ADDE, MVT::i64, Expand);\n\n  // Register custom handling for vector loads/stores\n  for (MVT VT : MVT::vector_valuetypes()) {\n    if (IsPTXVectorType(VT)) {\n      setOperationAction(ISD::LOAD, VT, Custom);\n      setOperationAction(ISD::STORE, VT, Custom);\n      setOperationAction(ISD::INTRINSIC_W_CHAIN, VT, Custom);\n    }\n  }\n\n  // Custom handling for i8 intrinsics\n  setOperationAction(ISD::INTRINSIC_W_CHAIN, MVT::i8, Custom);\n\n  setOperationAction(ISD::CTLZ, MVT::i16, Legal);\n  setOperationAction(ISD::CTLZ, MVT::i32, Legal);\n  setOperationAction(ISD::CTLZ, MVT::i64, Legal);\n  setOperationAction(ISD::CTTZ, MVT::i16, Expand);\n  setOperationAction(ISD::CTTZ, MVT::i32, Expand);\n  setOperationAction(ISD::CTTZ, MVT::i64, Expand);\n  setOperationAction(ISD::CTPOP, MVT::i16, Legal);\n  setOperationAction(ISD::CTPOP, MVT::i32, Legal);\n  setOperationAction(ISD::CTPOP, MVT::i64, Legal);\n\n  // PTX does not directly support SELP of i1, so promote to i32 first\n  setOperationAction(ISD::SELECT, MVT::i1, Custom);\n\n  // PTX cannot multiply two i64s in a single instruction.\n  setOperationAction(ISD::SMUL_LOHI, MVT::i64, Expand);\n  setOperationAction(ISD::UMUL_LOHI, MVT::i64, Expand);\n\n  // We have some custom DAG combine patterns for these nodes\n  setTargetDAGCombine(ISD::ADD);\n  setTargetDAGCombine(ISD::AND);\n  setTargetDAGCombine(ISD::FADD);\n  setTargetDAGCombine(ISD::MUL);\n  setTargetDAGCombine(ISD::SHL);\n  setTargetDAGCombine(ISD::SELECT);\n\n  // Now deduce the information based on the above mentioned\n  // actions\n  computeRegisterProperties(STI.getRegisterInfo());\n}\n\n"},"LowerCall":{"range":[1054,1678],"code":"SDValue NVPTXTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,\n                                       SmallVectorImpl<SDValue> &InVals) const {\n  SelectionDAG &DAG = CLI.DAG;\n  SDLoc dl = CLI.DL;\n  SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;\n  SmallVectorImpl<SDValue> &OutVals = CLI.OutVals;\n  SmallVectorImpl<ISD::InputArg> &Ins = CLI.Ins;\n  SDValue Chain = CLI.Chain;\n  SDValue Callee = CLI.Callee;\n  bool &isTailCall = CLI.IsTailCall;\n  ArgListTy &Args = CLI.getArgs();\n  Type *retTy = CLI.RetTy;\n  ImmutableCallSite *CS = CLI.CS;\n\n  bool isABI = (STI.getSmVersion() >= 20);\n  assert(isABI && \"Non-ABI compilation is not supported\");\n  if (!isABI)\n    return Chain;\n  MachineFunction &MF = DAG.getMachineFunction();\n  const Function *F = MF.getFunction();\n  auto &DL = MF.getDataLayout();\n\n  SDValue tempChain = Chain;\n  Chain = DAG.getCALLSEQ_START(Chain,\n                               DAG.getIntPtrConstant(uniqueCallSite, dl, true),\n                               dl);\n  SDValue InFlag = Chain.getValue(1);\n\n  unsigned paramCount = 0;\n  // Args.size() and Outs.size() need not match.\n  // Outs.size() will be larger\n  //   * if there is an aggregate argument with multiple fields (each field\n  //     showing up separately in Outs)\n  //   * if there is a vector argument with more than typical vector-length\n  //     elements (generally if more than 4) where each vector element is\n  //     individually present in Outs.\n  // So a different index should be used for indexing into Outs/OutVals.\n  // See similar issue in LowerFormalArguments.\n  unsigned OIdx = 0;\n  // Declare the .params or .reg need to pass values\n  // to the function\n  for (unsigned i = 0, e = Args.size(); i != e; ++i, ++OIdx) {\n    EVT VT = Outs[OIdx].VT;\n    Type *Ty = Args[i].Ty;\n\n    if (!Outs[OIdx].Flags.isByVal()) {\n      if (Ty->isAggregateType()) {\n        // aggregate\n        SmallVector<EVT, 16> vtparts;\n        SmallVector<uint64_t, 16> Offsets;\n        ComputePTXValueVTs(*this, DAG.getDataLayout(), Ty, vtparts, &Offsets,\n                           0);\n\n        unsigned align = getArgumentAlignment(Callee, CS, Ty, paramCount + 1);\n        // declare .param .align <align> .b8 .param<n>[<size>];\n        unsigned sz = DL.getTypeAllocSize(Ty);\n        SDVTList DeclareParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n        SDValue DeclareParamOps[] = { Chain, DAG.getConstant(align, dl,\n                                                             MVT::i32),\n                                      DAG.getConstant(paramCount, dl, MVT::i32),\n                                      DAG.getConstant(sz, dl, MVT::i32),\n                                      InFlag };\n        Chain = DAG.getNode(NVPTXISD::DeclareParam, dl, DeclareParamVTs,\n                            DeclareParamOps);\n        InFlag = Chain.getValue(1);\n        for (unsigned j = 0, je = vtparts.size(); j != je; ++j) {\n          EVT elemtype = vtparts[j];\n          unsigned ArgAlign = GreatestCommonDivisor64(align, Offsets[j]);\n          if (elemtype.isInteger() && (sz < 8))\n            sz = 8;\n          SDValue StVal = OutVals[OIdx];\n          if (elemtype.getSizeInBits() < 16) {\n            StVal = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i16, StVal);\n          }\n          SDVTList CopyParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n          SDValue CopyParamOps[] = { Chain,\n                                     DAG.getConstant(paramCount, dl, MVT::i32),\n                                     DAG.getConstant(Offsets[j], dl, MVT::i32),\n                                     StVal, InFlag };\n          Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreParam, dl,\n                                          CopyParamVTs, CopyParamOps,\n                                          elemtype, MachinePointerInfo(),\n                                          ArgAlign);\n          InFlag = Chain.getValue(1);\n          ++OIdx;\n        }\n        if (vtparts.size() > 0)\n          --OIdx;\n        ++paramCount;\n        continue;\n      }\n      if (Ty->isVectorTy()) {\n        EVT ObjectVT = getValueType(DL, Ty);\n        unsigned align = getArgumentAlignment(Callee, CS, Ty, paramCount + 1);\n        // declare .param .align <align> .b8 .param<n>[<size>];\n        unsigned sz = DL.getTypeAllocSize(Ty);\n        SDVTList DeclareParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n        SDValue DeclareParamOps[] = { Chain,\n                                      DAG.getConstant(align, dl, MVT::i32),\n                                      DAG.getConstant(paramCount, dl, MVT::i32),\n                                      DAG.getConstant(sz, dl, MVT::i32),\n                                      InFlag };\n        Chain = DAG.getNode(NVPTXISD::DeclareParam, dl, DeclareParamVTs,\n                            DeclareParamOps);\n        InFlag = Chain.getValue(1);\n        unsigned NumElts = ObjectVT.getVectorNumElements();\n        EVT EltVT = ObjectVT.getVectorElementType();\n        EVT MemVT = EltVT;\n        bool NeedExtend = false;\n        if (EltVT.getSizeInBits() < 16) {\n          NeedExtend = true;\n          EltVT = MVT::i16;\n        }\n\n        // V1 store\n        if (NumElts == 1) {\n          SDValue Elt = OutVals[OIdx++];\n          if (NeedExtend)\n            Elt = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, Elt);\n\n          SDVTList CopyParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n          SDValue CopyParamOps[] = { Chain,\n                                     DAG.getConstant(paramCount, dl, MVT::i32),\n                                     DAG.getConstant(0, dl, MVT::i32), Elt,\n                                     InFlag };\n          Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreParam, dl,\n                                          CopyParamVTs, CopyParamOps,\n                                          MemVT, MachinePointerInfo());\n          InFlag = Chain.getValue(1);\n        } else if (NumElts == 2) {\n          SDValue Elt0 = OutVals[OIdx++];\n          SDValue Elt1 = OutVals[OIdx++];\n          if (NeedExtend) {\n            Elt0 = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, Elt0);\n            Elt1 = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, Elt1);\n          }\n\n          SDVTList CopyParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n          SDValue CopyParamOps[] = { Chain,\n                                     DAG.getConstant(paramCount, dl, MVT::i32),\n                                     DAG.getConstant(0, dl, MVT::i32), Elt0,\n                                     Elt1, InFlag };\n          Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreParamV2, dl,\n                                          CopyParamVTs, CopyParamOps,\n                                          MemVT, MachinePointerInfo());\n          InFlag = Chain.getValue(1);\n        } else {\n          unsigned curOffset = 0;\n          // V4 stores\n          // We have at least 4 elements (<3 x Ty> expands to 4 elements) and\n          // the\n          // vector will be expanded to a power of 2 elements, so we know we can\n          // always round up to the next multiple of 4 when creating the vector\n          // stores.\n          // e.g.  4 elem => 1 st.v4\n          //       6 elem => 2 st.v4\n          //       8 elem => 2 st.v4\n          //      11 elem => 3 st.v4\n          unsigned VecSize = 4;\n          if (EltVT.getSizeInBits() == 64)\n            VecSize = 2;\n\n          // This is potentially only part of a vector, so assume all elements\n          // are packed together.\n          unsigned PerStoreOffset = MemVT.getStoreSizeInBits() / 8 * VecSize;\n\n          for (unsigned i = 0; i < NumElts; i += VecSize) {\n            // Get values\n            SDValue StoreVal;\n            SmallVector<SDValue, 8> Ops;\n            Ops.push_back(Chain);\n            Ops.push_back(DAG.getConstant(paramCount, dl, MVT::i32));\n            Ops.push_back(DAG.getConstant(curOffset, dl, MVT::i32));\n\n            unsigned Opc = NVPTXISD::StoreParamV2;\n\n            StoreVal = OutVals[OIdx++];\n            if (NeedExtend)\n              StoreVal = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal);\n            Ops.push_back(StoreVal);\n\n            if (i + 1 < NumElts) {\n              StoreVal = OutVals[OIdx++];\n              if (NeedExtend)\n                StoreVal =\n                    DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal);\n            } else {\n              StoreVal = DAG.getUNDEF(EltVT);\n            }\n            Ops.push_back(StoreVal);\n\n            if (VecSize == 4) {\n              Opc = NVPTXISD::StoreParamV4;\n              if (i + 2 < NumElts) {\n                StoreVal = OutVals[OIdx++];\n                if (NeedExtend)\n                  StoreVal =\n                      DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal);\n              } else {\n                StoreVal = DAG.getUNDEF(EltVT);\n              }\n              Ops.push_back(StoreVal);\n\n              if (i + 3 < NumElts) {\n                StoreVal = OutVals[OIdx++];\n                if (NeedExtend)\n                  StoreVal =\n                      DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal);\n              } else {\n                StoreVal = DAG.getUNDEF(EltVT);\n              }\n              Ops.push_back(StoreVal);\n            }\n\n            Ops.push_back(InFlag);\n\n            SDVTList CopyParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n            Chain = DAG.getMemIntrinsicNode(Opc, dl, CopyParamVTs, Ops,\n                                            MemVT, MachinePointerInfo());\n            InFlag = Chain.getValue(1);\n            curOffset += PerStoreOffset;\n          }\n        }\n        ++paramCount;\n        --OIdx;\n        continue;\n      }\n      // Plain scalar\n      // for ABI,    declare .param .b<size> .param<n>;\n      unsigned sz = VT.getSizeInBits();\n      bool needExtend = false;\n      if (VT.isInteger()) {\n        if (sz < 16)\n          needExtend = true;\n        if (sz < 32)\n          sz = 32;\n      }\n      SDVTList DeclareParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n      SDValue DeclareParamOps[] = { Chain,\n                                    DAG.getConstant(paramCount, dl, MVT::i32),\n                                    DAG.getConstant(sz, dl, MVT::i32),\n                                    DAG.getConstant(0, dl, MVT::i32), InFlag };\n      Chain = DAG.getNode(NVPTXISD::DeclareScalarParam, dl, DeclareParamVTs,\n                          DeclareParamOps);\n      InFlag = Chain.getValue(1);\n      SDValue OutV = OutVals[OIdx];\n      if (needExtend) {\n        // zext/sext i1 to i16\n        unsigned opc = ISD::ZERO_EXTEND;\n        if (Outs[OIdx].Flags.isSExt())\n          opc = ISD::SIGN_EXTEND;\n        OutV = DAG.getNode(opc, dl, MVT::i16, OutV);\n      }\n      SDVTList CopyParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n      SDValue CopyParamOps[] = { Chain,\n                                 DAG.getConstant(paramCount, dl, MVT::i32),\n                                 DAG.getConstant(0, dl, MVT::i32), OutV,\n                                 InFlag };\n\n      unsigned opcode = NVPTXISD::StoreParam;\n      if (Outs[OIdx].Flags.isZExt() && VT.getSizeInBits() < 32)\n        opcode = NVPTXISD::StoreParamU32;\n      else if (Outs[OIdx].Flags.isSExt() && VT.getSizeInBits() < 32)\n        opcode = NVPTXISD::StoreParamS32;\n      Chain = DAG.getMemIntrinsicNode(opcode, dl, CopyParamVTs, CopyParamOps,\n                                      VT, MachinePointerInfo());\n\n      InFlag = Chain.getValue(1);\n      ++paramCount;\n      continue;\n    }\n    // struct or vector\n    SmallVector<EVT, 16> vtparts;\n    SmallVector<uint64_t, 16> Offsets;\n    auto *PTy = dyn_cast<PointerType>(Args[i].Ty);\n    assert(PTy && \"Type of a byval parameter should be pointer\");\n    ComputePTXValueVTs(*this, DAG.getDataLayout(), PTy->getElementType(),\n                       vtparts, &Offsets, 0);\n\n    // declare .param .align <align> .b8 .param<n>[<size>];\n    unsigned sz = Outs[OIdx].Flags.getByValSize();\n    SDVTList DeclareParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n    unsigned ArgAlign = Outs[OIdx].Flags.getByValAlign();\n    // The ByValAlign in the Outs[OIdx].Flags is alway set at this point,\n    // so we don't need to worry about natural alignment or not.\n    // See TargetLowering::LowerCallTo().\n    SDValue DeclareParamOps[] = {\n      Chain, DAG.getConstant(Outs[OIdx].Flags.getByValAlign(), dl, MVT::i32),\n      DAG.getConstant(paramCount, dl, MVT::i32),\n      DAG.getConstant(sz, dl, MVT::i32), InFlag\n    };\n    Chain = DAG.getNode(NVPTXISD::DeclareParam, dl, DeclareParamVTs,\n                        DeclareParamOps);\n    InFlag = Chain.getValue(1);\n    for (unsigned j = 0, je = vtparts.size(); j != je; ++j) {\n      EVT elemtype = vtparts[j];\n      int curOffset = Offsets[j];\n      unsigned PartAlign = GreatestCommonDivisor64(ArgAlign, curOffset);\n      auto PtrVT = getPointerTy(DAG.getDataLayout());\n      SDValue srcAddr = DAG.getNode(ISD::ADD, dl, PtrVT, OutVals[OIdx],\n                                    DAG.getConstant(curOffset, dl, PtrVT));\n      SDValue theVal = DAG.getLoad(elemtype, dl, tempChain, srcAddr,\n                                   MachinePointerInfo(), PartAlign);\n      if (elemtype.getSizeInBits() < 16) {\n        theVal = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i16, theVal);\n      }\n      SDVTList CopyParamVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n      SDValue CopyParamOps[] = { Chain,\n                                 DAG.getConstant(paramCount, dl, MVT::i32),\n                                 DAG.getConstant(curOffset, dl, MVT::i32),\n                                 theVal, InFlag };\n      Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreParam, dl, CopyParamVTs,\n                                      CopyParamOps, elemtype,\n                                      MachinePointerInfo());\n\n      InFlag = Chain.getValue(1);\n    }\n    ++paramCount;\n  }\n\n  GlobalAddressSDNode *Func = dyn_cast<GlobalAddressSDNode>(Callee.getNode());\n  unsigned retAlignment = 0;\n\n  // Handle Result\n  if (Ins.size() > 0) {\n    SmallVector<EVT, 16> resvtparts;\n    ComputeValueVTs(*this, DL, retTy, resvtparts);\n\n    // Declare\n    //  .param .align 16 .b8 retval0[<size-in-bytes>], or\n    //  .param .b<size-in-bits> retval0\n    unsigned resultsz = DL.getTypeAllocSizeInBits(retTy);\n    // Emit \".param .b<size-in-bits> retval0\" instead of byte arrays only for\n    // these three types to match the logic in\n    // NVPTXAsmPrinter::printReturnValStr and NVPTXTargetLowering::getPrototype.\n    // Plus, this behavior is consistent with nvcc's.\n    if (retTy->isFloatingPointTy() || retTy->isIntegerTy() ||\n        retTy->isPointerTy()) {\n      // Scalar needs to be at least 32bit wide\n      if (resultsz < 32)\n        resultsz = 32;\n      SDVTList DeclareRetVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n      SDValue DeclareRetOps[] = { Chain, DAG.getConstant(1, dl, MVT::i32),\n                                  DAG.getConstant(resultsz, dl, MVT::i32),\n                                  DAG.getConstant(0, dl, MVT::i32), InFlag };\n      Chain = DAG.getNode(NVPTXISD::DeclareRet, dl, DeclareRetVTs,\n                          DeclareRetOps);\n      InFlag = Chain.getValue(1);\n    } else {\n      retAlignment = getArgumentAlignment(Callee, CS, retTy, 0);\n      SDVTList DeclareRetVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n      SDValue DeclareRetOps[] = { Chain,\n                                  DAG.getConstant(retAlignment, dl, MVT::i32),\n                                  DAG.getConstant(resultsz / 8, dl, MVT::i32),\n                                  DAG.getConstant(0, dl, MVT::i32), InFlag };\n      Chain = DAG.getNode(NVPTXISD::DeclareRetParam, dl, DeclareRetVTs,\n                          DeclareRetOps);\n      InFlag = Chain.getValue(1);\n    }\n  }\n\n  if (!Func) {\n    // This is indirect function call case : PTX requires a prototype of the\n    // form\n    // proto_0 : .callprototype(.param .b32 _) _ (.param .b32 _);\n    // to be emitted, and the label has to used as the last arg of call\n    // instruction.\n    // The prototype is embedded in a string and put as the operand for a\n    // CallPrototype SDNode which will print out to the value of the string.\n    SDVTList ProtoVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n    std::string Proto =\n        getPrototype(DAG.getDataLayout(), retTy, Args, Outs, retAlignment, CS);\n    const char *ProtoStr =\n      nvTM->getManagedStrPool()->getManagedString(Proto.c_str())->c_str();\n    SDValue ProtoOps[] = {\n      Chain, DAG.getTargetExternalSymbol(ProtoStr, MVT::i32), InFlag,\n    };\n    Chain = DAG.getNode(NVPTXISD::CallPrototype, dl, ProtoVTs, ProtoOps);\n    InFlag = Chain.getValue(1);\n  }\n  // Op to just print \"call\"\n  SDVTList PrintCallVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDValue PrintCallOps[] = {\n    Chain, DAG.getConstant((Ins.size() == 0) ? 0 : 1, dl, MVT::i32), InFlag\n  };\n  // We model convergent calls as separate opcodes.\n  unsigned Opcode = Func ? NVPTXISD::PrintCallUni : NVPTXISD::PrintCall;\n  if (CLI.IsConvergent)\n    Opcode = Opcode == NVPTXISD::PrintCallUni ? NVPTXISD::PrintConvergentCallUni\n                                              : NVPTXISD::PrintConvergentCall;\n  Chain = DAG.getNode(Opcode, dl, PrintCallVTs, PrintCallOps);\n  InFlag = Chain.getValue(1);\n\n  // Ops to print out the function name\n  SDVTList CallVoidVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDValue CallVoidOps[] = { Chain, Callee, InFlag };\n  Chain = DAG.getNode(NVPTXISD::CallVoid, dl, CallVoidVTs, CallVoidOps);\n  InFlag = Chain.getValue(1);\n\n  // Ops to print out the param list\n  SDVTList CallArgBeginVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDValue CallArgBeginOps[] = { Chain, InFlag };\n  Chain = DAG.getNode(NVPTXISD::CallArgBegin, dl, CallArgBeginVTs,\n                      CallArgBeginOps);\n  InFlag = Chain.getValue(1);\n\n  for (unsigned i = 0, e = paramCount; i != e; ++i) {\n    unsigned opcode;\n    if (i == (e - 1))\n      opcode = NVPTXISD::LastCallArg;\n    else\n      opcode = NVPTXISD::CallArg;\n    SDVTList CallArgVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n    SDValue CallArgOps[] = { Chain, DAG.getConstant(1, dl, MVT::i32),\n                             DAG.getConstant(i, dl, MVT::i32), InFlag };\n    Chain = DAG.getNode(opcode, dl, CallArgVTs, CallArgOps);\n    InFlag = Chain.getValue(1);\n  }\n  SDVTList CallArgEndVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n  SDValue CallArgEndOps[] = { Chain,\n                              DAG.getConstant(Func ? 1 : 0, dl, MVT::i32),\n                              InFlag };\n  Chain = DAG.getNode(NVPTXISD::CallArgEnd, dl, CallArgEndVTs, CallArgEndOps);\n  InFlag = Chain.getValue(1);\n\n  if (!Func) {\n    SDVTList PrototypeVTs = DAG.getVTList(MVT::Other, MVT::Glue);\n    SDValue PrototypeOps[] = { Chain,\n                               DAG.getConstant(uniqueCallSite, dl, MVT::i32),\n                               InFlag };\n    Chain = DAG.getNode(NVPTXISD::Prototype, dl, PrototypeVTs, PrototypeOps);\n    InFlag = Chain.getValue(1);\n  }\n\n  // Generate loads from param memory/moves from registers for result\n  if (Ins.size() > 0) {\n    if (retTy && retTy->isVectorTy()) {\n      EVT ObjectVT = getValueType(DL, retTy);\n      unsigned NumElts = ObjectVT.getVectorNumElements();\n      EVT EltVT = ObjectVT.getVectorElementType();\n      assert(STI.getTargetLowering()->getNumRegisters(F->getContext(),\n                                                      ObjectVT) == NumElts &&\n             \"Vector was not scalarized\");\n      unsigned sz = EltVT.getSizeInBits();\n      bool needTruncate = sz < 8;\n\n      if (NumElts == 1) {\n        // Just a simple load\n        SmallVector<EVT, 4> LoadRetVTs;\n        if (EltVT == MVT::i1 || EltVT == MVT::i8) {\n          // If loading i1/i8 result, generate\n          //   load.b8 i16\n          //   if i1\n          //   trunc i16 to i1\n          LoadRetVTs.push_back(MVT::i16);\n        } else\n          LoadRetVTs.push_back(EltVT);\n        LoadRetVTs.push_back(MVT::Other);\n        LoadRetVTs.push_back(MVT::Glue);\n        SDValue LoadRetOps[] = {Chain, DAG.getConstant(1, dl, MVT::i32),\n                                DAG.getConstant(0, dl, MVT::i32), InFlag};\n        SDValue retval = DAG.getMemIntrinsicNode(\n            NVPTXISD::LoadParam, dl,\n            DAG.getVTList(LoadRetVTs), LoadRetOps, EltVT, MachinePointerInfo());\n        Chain = retval.getValue(1);\n        InFlag = retval.getValue(2);\n        SDValue Ret0 = retval;\n        if (needTruncate)\n          Ret0 = DAG.getNode(ISD::TRUNCATE, dl, EltVT, Ret0);\n        InVals.push_back(Ret0);\n      } else if (NumElts == 2) {\n        // LoadV2\n        SmallVector<EVT, 4> LoadRetVTs;\n        if (EltVT == MVT::i1 || EltVT == MVT::i8) {\n          // If loading i1/i8 result, generate\n          //   load.b8 i16\n          //   if i1\n          //   trunc i16 to i1\n          LoadRetVTs.push_back(MVT::i16);\n          LoadRetVTs.push_back(MVT::i16);\n        } else {\n          LoadRetVTs.push_back(EltVT);\n          LoadRetVTs.push_back(EltVT);\n        }\n        LoadRetVTs.push_back(MVT::Other);\n        LoadRetVTs.push_back(MVT::Glue);\n        SDValue LoadRetOps[] = {Chain, DAG.getConstant(1, dl, MVT::i32),\n                                DAG.getConstant(0, dl, MVT::i32), InFlag};\n        SDValue retval = DAG.getMemIntrinsicNode(\n            NVPTXISD::LoadParamV2, dl,\n            DAG.getVTList(LoadRetVTs), LoadRetOps, EltVT, MachinePointerInfo());\n        Chain = retval.getValue(2);\n        InFlag = retval.getValue(3);\n        SDValue Ret0 = retval.getValue(0);\n        SDValue Ret1 = retval.getValue(1);\n        if (needTruncate) {\n          Ret0 = DAG.getNode(ISD::TRUNCATE, dl, MVT::i1, Ret0);\n          InVals.push_back(Ret0);\n          Ret1 = DAG.getNode(ISD::TRUNCATE, dl, MVT::i1, Ret1);\n          InVals.push_back(Ret1);\n        } else {\n          InVals.push_back(Ret0);\n          InVals.push_back(Ret1);\n        }\n      } else {\n        // Split into N LoadV4\n        unsigned Ofst = 0;\n        unsigned VecSize = 4;\n        unsigned Opc = NVPTXISD::LoadParamV4;\n        if (EltVT.getSizeInBits() == 64) {\n          VecSize = 2;\n          Opc = NVPTXISD::LoadParamV2;\n        }\n        EVT VecVT = EVT::getVectorVT(F->getContext(), EltVT, VecSize);\n        for (unsigned i = 0; i < NumElts; i += VecSize) {\n          SmallVector<EVT, 8> LoadRetVTs;\n          if (EltVT == MVT::i1 || EltVT == MVT::i8) {\n            // If loading i1/i8 result, generate\n            //   load.b8 i16\n            //   if i1\n            //   trunc i16 to i1\n            for (unsigned j = 0; j < VecSize; ++j)\n              LoadRetVTs.push_back(MVT::i16);\n          } else {\n            for (unsigned j = 0; j < VecSize; ++j)\n              LoadRetVTs.push_back(EltVT);\n          }\n          LoadRetVTs.push_back(MVT::Other);\n          LoadRetVTs.push_back(MVT::Glue);\n          SDValue LoadRetOps[] = {Chain, DAG.getConstant(1, dl, MVT::i32),\n                                  DAG.getConstant(Ofst, dl, MVT::i32), InFlag};\n          SDValue retval = DAG.getMemIntrinsicNode(\n              Opc, dl, DAG.getVTList(LoadRetVTs),\n              LoadRetOps, EltVT, MachinePointerInfo());\n          if (VecSize == 2) {\n            Chain = retval.getValue(2);\n            InFlag = retval.getValue(3);\n          } else {\n            Chain = retval.getValue(4);\n            InFlag = retval.getValue(5);\n          }\n\n          for (unsigned j = 0; j < VecSize; ++j) {\n            if (i + j >= NumElts)\n              break;\n            SDValue Elt = retval.getValue(j);\n            if (needTruncate)\n              Elt = DAG.getNode(ISD::TRUNCATE, dl, EltVT, Elt);\n            InVals.push_back(Elt);\n          }\n          Ofst += DL.getTypeAllocSize(VecVT.getTypeForEVT(F->getContext()));\n        }\n      }\n    } else {\n      SmallVector<EVT, 16> VTs;\n      SmallVector<uint64_t, 16> Offsets;\n      ComputePTXValueVTs(*this, DAG.getDataLayout(), retTy, VTs, &Offsets, 0);\n      assert(VTs.size() == Ins.size() && \"Bad value decomposition\");\n      unsigned RetAlign = getArgumentAlignment(Callee, CS, retTy, 0);\n      for (unsigned i = 0, e = Ins.size(); i != e; ++i) {\n        unsigned sz = VTs[i].getSizeInBits();\n        unsigned AlignI = GreatestCommonDivisor64(RetAlign, Offsets[i]);\n        bool needTruncate = false;\n        if (VTs[i].isInteger() && sz < 8) {\n          sz = 8;\n          needTruncate = true;\n        }\n\n        SmallVector<EVT, 4> LoadRetVTs;\n        EVT TheLoadType = VTs[i];\n        if (retTy->isIntegerTy() && DL.getTypeAllocSizeInBits(retTy) < 32) {\n          // This is for integer types only, and specifically not for\n          // aggregates.\n          LoadRetVTs.push_back(MVT::i32);\n          TheLoadType = MVT::i32;\n          needTruncate = true;\n        } else if (sz < 16) {\n          // If loading i1/i8 result, generate\n          //   load i8 (-> i16)\n          //   trunc i16 to i1/i8\n\n          // FIXME: Do we need to set needTruncate to true here, too?  We could\n          // not figure out what this branch is for in D17872, so we left it\n          // alone.  The comment above about loading i1/i8 may be wrong, as the\n          // branch above seems to cover integers of size < 32.\n          LoadRetVTs.push_back(MVT::i16);\n        } else\n          LoadRetVTs.push_back(Ins[i].VT);\n        LoadRetVTs.push_back(MVT::Other);\n        LoadRetVTs.push_back(MVT::Glue);\n\n        SDValue LoadRetOps[] = {Chain, DAG.getConstant(1, dl, MVT::i32),\n                                DAG.getConstant(Offsets[i], dl, MVT::i32),\n                                InFlag};\n        SDValue retval = DAG.getMemIntrinsicNode(\n            NVPTXISD::LoadParam, dl,\n            DAG.getVTList(LoadRetVTs), LoadRetOps,\n            TheLoadType, MachinePointerInfo(), AlignI);\n        Chain = retval.getValue(1);\n        InFlag = retval.getValue(2);\n        SDValue Ret0 = retval.getValue(0);\n        if (needTruncate)\n          Ret0 = DAG.getNode(ISD::TRUNCATE, dl, Ins[i].VT, Ret0);\n        InVals.push_back(Ret0);\n      }\n    }\n  }\n\n  Chain = DAG.getCALLSEQ_END(Chain,\n                             DAG.getIntPtrConstant(uniqueCallSite, dl, true),\n                             DAG.getIntPtrConstant(uniqueCallSite + 1, dl,\n                                                   true),\n                             InFlag, dl);\n  uniqueCallSite++;\n\n  // set isTailCall to false for now, until we figure out how to express\n  // tail call optimization in PTX\n  isTailCall = false;\n  return Chain;\n}\n\n// By default CONCAT_VECTORS is lowered by ExpandVectorBuildThroughStack()\n// (see LegalizeDAG.cpp). This is slow and uses local memory.\n// We use extract/insert/build vector just as what LegalizeOp() does in llvm 2.5\nSDValue\n"},"ComputePTXValueVTs":{"range":[83,108],"code":"static void ComputePTXValueVTs(const TargetLowering &TLI, const DataLayout &DL,\n                               Type *Ty, SmallVectorImpl<EVT> &ValueVTs,\n                               SmallVectorImpl<uint64_t> *Offsets = nullptr,\n                               uint64_t StartingOffset = 0) {\n  SmallVector<EVT, 16> TempVTs;\n  SmallVector<uint64_t, 16> TempOffsets;\n\n  ComputeValueVTs(TLI, DL, Ty, TempVTs, &TempOffsets, StartingOffset);\n  for (unsigned i = 0, e = TempVTs.size(); i != e; ++i) {\n    EVT VT = TempVTs[i];\n    uint64_t Off = TempOffsets[i];\n    if (VT.isVector())\n      for (unsigned j = 0, je = VT.getVectorNumElements(); j != je; ++j) {\n        ValueVTs.push_back(VT.getVectorElementType());\n        if (Offsets)\n          Offsets->push_back(Off+j*VT.getVectorElementType().getStoreSize());\n      }\n    else {\n      ValueVTs.push_back(VT);\n      if (Offsets)\n        Offsets->push_back(Off);\n    }\n  }\n}\n\n// NVPTXTargetLowering Constructor.\n"},"getTargetNodeName":{"range":[287,883],"code":"const char *NVPTXTargetLowering::getTargetNodeName(unsigned Opcode) const {\n  switch ((NVPTXISD::NodeType)Opcode) {\n  case NVPTXISD::FIRST_NUMBER:\n    break;\n  case NVPTXISD::CALL:\n    return \"NVPTXISD::CALL\";\n  case NVPTXISD::RET_FLAG:\n    return \"NVPTXISD::RET_FLAG\";\n  case NVPTXISD::LOAD_PARAM:\n    return \"NVPTXISD::LOAD_PARAM\";\n  case NVPTXISD::Wrapper:\n    return \"NVPTXISD::Wrapper\";\n  case NVPTXISD::DeclareParam:\n    return \"NVPTXISD::DeclareParam\";\n  case NVPTXISD::DeclareScalarParam:\n    return \"NVPTXISD::DeclareScalarParam\";\n  case NVPTXISD::DeclareRet:\n    return \"NVPTXISD::DeclareRet\";\n  case NVPTXISD::DeclareScalarRet:\n    return \"NVPTXISD::DeclareScalarRet\";\n  case NVPTXISD::DeclareRetParam:\n    return \"NVPTXISD::DeclareRetParam\";\n  case NVPTXISD::PrintCall:\n    return \"NVPTXISD::PrintCall\";\n  case NVPTXISD::PrintConvergentCall:\n    return \"NVPTXISD::PrintConvergentCall\";\n  case NVPTXISD::PrintCallUni:\n    return \"NVPTXISD::PrintCallUni\";\n  case NVPTXISD::PrintConvergentCallUni:\n    return \"NVPTXISD::PrintConvergentCallUni\";\n  case NVPTXISD::LoadParam:\n    return \"NVPTXISD::LoadParam\";\n  case NVPTXISD::LoadParamV2:\n    return \"NVPTXISD::LoadParamV2\";\n  case NVPTXISD::LoadParamV4:\n    return \"NVPTXISD::LoadParamV4\";\n  case NVPTXISD::StoreParam:\n    return \"NVPTXISD::StoreParam\";\n  case NVPTXISD::StoreParamV2:\n    return \"NVPTXISD::StoreParamV2\";\n  case NVPTXISD::StoreParamV4:\n    return \"NVPTXISD::StoreParamV4\";\n  case NVPTXISD::StoreParamS32:\n    return \"NVPTXISD::StoreParamS32\";\n  case NVPTXISD::StoreParamU32:\n    return \"NVPTXISD::StoreParamU32\";\n  case NVPTXISD::CallArgBegin:\n    return \"NVPTXISD::CallArgBegin\";\n  case NVPTXISD::CallArg:\n    return \"NVPTXISD::CallArg\";\n  case NVPTXISD::LastCallArg:\n    return \"NVPTXISD::LastCallArg\";\n  case NVPTXISD::CallArgEnd:\n    return \"NVPTXISD::CallArgEnd\";\n  case NVPTXISD::CallVoid:\n    return \"NVPTXISD::CallVoid\";\n  case NVPTXISD::CallVal:\n    return \"NVPTXISD::CallVal\";\n  case NVPTXISD::CallSymbol:\n    return \"NVPTXISD::CallSymbol\";\n  case NVPTXISD::Prototype:\n    return \"NVPTXISD::Prototype\";\n  case NVPTXISD::MoveParam:\n    return \"NVPTXISD::MoveParam\";\n  case NVPTXISD::StoreRetval:\n    return \"NVPTXISD::StoreRetval\";\n  case NVPTXISD::StoreRetvalV2:\n    return \"NVPTXISD::StoreRetvalV2\";\n  case NVPTXISD::StoreRetvalV4:\n    return \"NVPTXISD::StoreRetvalV4\";\n  case NVPTXISD::PseudoUseParam:\n    return \"NVPTXISD::PseudoUseParam\";\n  case NVPTXISD::RETURN:\n    return \"NVPTXISD::RETURN\";\n  case NVPTXISD::CallSeqBegin:\n    return \"NVPTXISD::CallSeqBegin\";\n  case NVPTXISD::CallSeqEnd:\n    return \"NVPTXISD::CallSeqEnd\";\n  case NVPTXISD::CallPrototype:\n    return \"NVPTXISD::CallPrototype\";\n  case NVPTXISD::LoadV2:\n    return \"NVPTXISD::LoadV2\";\n  case NVPTXISD::LoadV4:\n    return \"NVPTXISD::LoadV4\";\n  case NVPTXISD::LDGV2:\n    return \"NVPTXISD::LDGV2\";\n  case NVPTXISD::LDGV4:\n    return \"NVPTXISD::LDGV4\";\n  case NVPTXISD::LDUV2:\n    return \"NVPTXISD::LDUV2\";\n  case NVPTXISD::LDUV4:\n    return \"NVPTXISD::LDUV4\";\n  case NVPTXISD::StoreV2:\n    return \"NVPTXISD::StoreV2\";\n  case NVPTXISD::StoreV4:\n    return \"NVPTXISD::StoreV4\";\n  case NVPTXISD::FUN_SHFL_CLAMP:\n    return \"NVPTXISD::FUN_SHFL_CLAMP\";\n  case NVPTXISD::FUN_SHFR_CLAMP:\n    return \"NVPTXISD::FUN_SHFR_CLAMP\";\n  case NVPTXISD::IMAD:\n    return \"NVPTXISD::IMAD\";\n  case NVPTXISD::Dummy:\n    return \"NVPTXISD::Dummy\";\n  case NVPTXISD::MUL_WIDE_SIGNED:\n    return \"NVPTXISD::MUL_WIDE_SIGNED\";\n  case NVPTXISD::MUL_WIDE_UNSIGNED:\n    return \"NVPTXISD::MUL_WIDE_UNSIGNED\";\n  case NVPTXISD::Tex1DFloatS32:        return \"NVPTXISD::Tex1DFloatS32\";\n  case NVPTXISD::Tex1DFloatFloat:      return \"NVPTXISD::Tex1DFloatFloat\";\n  case NVPTXISD::Tex1DFloatFloatLevel:\n    return \"NVPTXISD::Tex1DFloatFloatLevel\";\n  case NVPTXISD::Tex1DFloatFloatGrad:\n    return \"NVPTXISD::Tex1DFloatFloatGrad\";\n  case NVPTXISD::Tex1DS32S32:          return \"NVPTXISD::Tex1DS32S32\";\n  case NVPTXISD::Tex1DS32Float:        return \"NVPTXISD::Tex1DS32Float\";\n  case NVPTXISD::Tex1DS32FloatLevel:\n    return \"NVPTXISD::Tex1DS32FloatLevel\";\n  case NVPTXISD::Tex1DS32FloatGrad:\n    return \"NVPTXISD::Tex1DS32FloatGrad\";\n  case NVPTXISD::Tex1DU32S32:          return \"NVPTXISD::Tex1DU32S32\";\n  case NVPTXISD::Tex1DU32Float:        return \"NVPTXISD::Tex1DU32Float\";\n  case NVPTXISD::Tex1DU32FloatLevel:\n    return \"NVPTXISD::Tex1DU32FloatLevel\";\n  case NVPTXISD::Tex1DU32FloatGrad:\n    return \"NVPTXISD::Tex1DU32FloatGrad\";\n  case NVPTXISD::Tex1DArrayFloatS32:   return \"NVPTXISD::Tex1DArrayFloatS32\";\n  case NVPTXISD::Tex1DArrayFloatFloat: return \"NVPTXISD::Tex1DArrayFloatFloat\";\n  case NVPTXISD::Tex1DArrayFloatFloatLevel:\n    return \"NVPTXISD::Tex1DArrayFloatFloatLevel\";\n  case NVPTXISD::Tex1DArrayFloatFloatGrad:\n    return \"NVPTXISD::Tex1DArrayFloatFloatGrad\";\n  case NVPTXISD::Tex1DArrayS32S32:     return \"NVPTXISD::Tex1DArrayS32S32\";\n  case NVPTXISD::Tex1DArrayS32Float:   return \"NVPTXISD::Tex1DArrayS32Float\";\n  case NVPTXISD::Tex1DArrayS32FloatLevel:\n    return \"NVPTXISD::Tex1DArrayS32FloatLevel\";\n  case NVPTXISD::Tex1DArrayS32FloatGrad:\n    return \"NVPTXISD::Tex1DArrayS32FloatGrad\";\n  case NVPTXISD::Tex1DArrayU32S32:     return \"NVPTXISD::Tex1DArrayU32S32\";\n  case NVPTXISD::Tex1DArrayU32Float:   return \"NVPTXISD::Tex1DArrayU32Float\";\n  case NVPTXISD::Tex1DArrayU32FloatLevel:\n    return \"NVPTXISD::Tex1DArrayU32FloatLevel\";\n  case NVPTXISD::Tex1DArrayU32FloatGrad:\n    return \"NVPTXISD::Tex1DArrayU32FloatGrad\";\n  case NVPTXISD::Tex2DFloatS32:        return \"NVPTXISD::Tex2DFloatS32\";\n  case NVPTXISD::Tex2DFloatFloat:      return \"NVPTXISD::Tex2DFloatFloat\";\n  case NVPTXISD::Tex2DFloatFloatLevel:\n    return \"NVPTXISD::Tex2DFloatFloatLevel\";\n  case NVPTXISD::Tex2DFloatFloatGrad:\n    return \"NVPTXISD::Tex2DFloatFloatGrad\";\n  case NVPTXISD::Tex2DS32S32:          return \"NVPTXISD::Tex2DS32S32\";\n  case NVPTXISD::Tex2DS32Float:        return \"NVPTXISD::Tex2DS32Float\";\n  case NVPTXISD::Tex2DS32FloatLevel:\n    return \"NVPTXISD::Tex2DS32FloatLevel\";\n  case NVPTXISD::Tex2DS32FloatGrad:\n    return \"NVPTXISD::Tex2DS32FloatGrad\";\n  case NVPTXISD::Tex2DU32S32:          return \"NVPTXISD::Tex2DU32S32\";\n  case NVPTXISD::Tex2DU32Float:        return \"NVPTXISD::Tex2DU32Float\";\n  case NVPTXISD::Tex2DU32FloatLevel:\n    return \"NVPTXISD::Tex2DU32FloatLevel\";\n  case NVPTXISD::Tex2DU32FloatGrad:\n    return \"NVPTXISD::Tex2DU32FloatGrad\";\n  case NVPTXISD::Tex2DArrayFloatS32:   return \"NVPTXISD::Tex2DArrayFloatS32\";\n  case NVPTXISD::Tex2DArrayFloatFloat: return \"NVPTXISD::Tex2DArrayFloatFloat\";\n  case NVPTXISD::Tex2DArrayFloatFloatLevel:\n    return \"NVPTXISD::Tex2DArrayFloatFloatLevel\";\n  case NVPTXISD::Tex2DArrayFloatFloatGrad:\n    return \"NVPTXISD::Tex2DArrayFloatFloatGrad\";\n  case NVPTXISD::Tex2DArrayS32S32:     return \"NVPTXISD::Tex2DArrayS32S32\";\n  case NVPTXISD::Tex2DArrayS32Float:   return \"NVPTXISD::Tex2DArrayS32Float\";\n  case NVPTXISD::Tex2DArrayS32FloatLevel:\n    return \"NVPTXISD::Tex2DArrayS32FloatLevel\";\n  case NVPTXISD::Tex2DArrayS32FloatGrad:\n    return \"NVPTXISD::Tex2DArrayS32FloatGrad\";\n  case NVPTXISD::Tex2DArrayU32S32:     return \"NVPTXISD::Tex2DArrayU32S32\";\n  case NVPTXISD::Tex2DArrayU32Float:   return \"NVPTXISD::Tex2DArrayU32Float\";\n  case NVPTXISD::Tex2DArrayU32FloatLevel:\n    return \"NVPTXISD::Tex2DArrayU32FloatLevel\";\n  case NVPTXISD::Tex2DArrayU32FloatGrad:\n    return \"NVPTXISD::Tex2DArrayU32FloatGrad\";\n  case NVPTXISD::Tex3DFloatS32:        return \"NVPTXISD::Tex3DFloatS32\";\n  case NVPTXISD::Tex3DFloatFloat:      return \"NVPTXISD::Tex3DFloatFloat\";\n  case NVPTXISD::Tex3DFloatFloatLevel:\n    return \"NVPTXISD::Tex3DFloatFloatLevel\";\n  case NVPTXISD::Tex3DFloatFloatGrad:\n    return \"NVPTXISD::Tex3DFloatFloatGrad\";\n  case NVPTXISD::Tex3DS32S32:          return \"NVPTXISD::Tex3DS32S32\";\n  case NVPTXISD::Tex3DS32Float:        return \"NVPTXISD::Tex3DS32Float\";\n  case NVPTXISD::Tex3DS32FloatLevel:\n    return \"NVPTXISD::Tex3DS32FloatLevel\";\n  case NVPTXISD::Tex3DS32FloatGrad:\n    return \"NVPTXISD::Tex3DS32FloatGrad\";\n  case NVPTXISD::Tex3DU32S32:          return \"NVPTXISD::Tex3DU32S32\";\n  case NVPTXISD::Tex3DU32Float:        return \"NVPTXISD::Tex3DU32Float\";\n  case NVPTXISD::Tex3DU32FloatLevel:\n    return \"NVPTXISD::Tex3DU32FloatLevel\";\n  case NVPTXISD::Tex3DU32FloatGrad:\n    return \"NVPTXISD::Tex3DU32FloatGrad\";\n  case NVPTXISD::TexCubeFloatFloat:      return \"NVPTXISD::TexCubeFloatFloat\";\n  case NVPTXISD::TexCubeFloatFloatLevel:\n    return \"NVPTXISD::TexCubeFloatFloatLevel\";\n  case NVPTXISD::TexCubeS32Float:        return \"NVPTXISD::TexCubeS32Float\";\n  case NVPTXISD::TexCubeS32FloatLevel:\n    return \"NVPTXISD::TexCubeS32FloatLevel\";\n  case NVPTXISD::TexCubeU32Float:        return \"NVPTXISD::TexCubeU32Float\";\n  case NVPTXISD::TexCubeU32FloatLevel:\n    return \"NVPTXISD::TexCubeU32FloatLevel\";\n  case NVPTXISD::TexCubeArrayFloatFloat:\n    return \"NVPTXISD::TexCubeArrayFloatFloat\";\n  case NVPTXISD::TexCubeArrayFloatFloatLevel:\n    return \"NVPTXISD::TexCubeArrayFloatFloatLevel\";\n  case NVPTXISD::TexCubeArrayS32Float:\n    return \"NVPTXISD::TexCubeArrayS32Float\";\n  case NVPTXISD::TexCubeArrayS32FloatLevel:\n    return \"NVPTXISD::TexCubeArrayS32FloatLevel\";\n  case NVPTXISD::TexCubeArrayU32Float:\n    return \"NVPTXISD::TexCubeArrayU32Float\";\n  case NVPTXISD::TexCubeArrayU32FloatLevel:\n    return \"NVPTXISD::TexCubeArrayU32FloatLevel\";\n  case NVPTXISD::Tld4R2DFloatFloat:\n    return \"NVPTXISD::Tld4R2DFloatFloat\";\n  case NVPTXISD::Tld4G2DFloatFloat:\n    return \"NVPTXISD::Tld4G2DFloatFloat\";\n  case NVPTXISD::Tld4B2DFloatFloat:\n    return \"NVPTXISD::Tld4B2DFloatFloat\";\n  case NVPTXISD::Tld4A2DFloatFloat:\n    return \"NVPTXISD::Tld4A2DFloatFloat\";\n  case NVPTXISD::Tld4R2DS64Float:\n    return \"NVPTXISD::Tld4R2DS64Float\";\n  case NVPTXISD::Tld4G2DS64Float:\n    return \"NVPTXISD::Tld4G2DS64Float\";\n  case NVPTXISD::Tld4B2DS64Float:\n    return \"NVPTXISD::Tld4B2DS64Float\";\n  case NVPTXISD::Tld4A2DS64Float:\n    return \"NVPTXISD::Tld4A2DS64Float\";\n  case NVPTXISD::Tld4R2DU64Float:\n    return \"NVPTXISD::Tld4R2DU64Float\";\n  case NVPTXISD::Tld4G2DU64Float:\n    return \"NVPTXISD::Tld4G2DU64Float\";\n  case NVPTXISD::Tld4B2DU64Float:\n    return \"NVPTXISD::Tld4B2DU64Float\";\n  case NVPTXISD::Tld4A2DU64Float:\n    return \"NVPTXISD::Tld4A2DU64Float\";\n\n  case NVPTXISD::TexUnified1DFloatS32:\n    return \"NVPTXISD::TexUnified1DFloatS32\";\n  case NVPTXISD::TexUnified1DFloatFloat:\n    return \"NVPTXISD::TexUnified1DFloatFloat\";\n  case NVPTXISD::TexUnified1DFloatFloatLevel:\n    return \"NVPTXISD::TexUnified1DFloatFloatLevel\";\n  case NVPTXISD::TexUnified1DFloatFloatGrad:\n    return \"NVPTXISD::TexUnified1DFloatFloatGrad\";\n  case NVPTXISD::TexUnified1DS32S32:\n    return \"NVPTXISD::TexUnified1DS32S32\";\n  case NVPTXISD::TexUnified1DS32Float:\n    return \"NVPTXISD::TexUnified1DS32Float\";\n  case NVPTXISD::TexUnified1DS32FloatLevel:\n    return \"NVPTXISD::TexUnified1DS32FloatLevel\";\n  case NVPTXISD::TexUnified1DS32FloatGrad:\n    return \"NVPTXISD::TexUnified1DS32FloatGrad\";\n  case NVPTXISD::TexUnified1DU32S32:\n    return \"NVPTXISD::TexUnified1DU32S32\";\n  case NVPTXISD::TexUnified1DU32Float:\n    return \"NVPTXISD::TexUnified1DU32Float\";\n  case NVPTXISD::TexUnified1DU32FloatLevel:\n    return \"NVPTXISD::TexUnified1DU32FloatLevel\";\n  case NVPTXISD::TexUnified1DU32FloatGrad:\n    return \"NVPTXISD::TexUnified1DU32FloatGrad\";\n  case NVPTXISD::TexUnified1DArrayFloatS32:\n    return \"NVPTXISD::TexUnified1DArrayFloatS32\";\n  case NVPTXISD::TexUnified1DArrayFloatFloat:\n    return \"NVPTXISD::TexUnified1DArrayFloatFloat\";\n  case NVPTXISD::TexUnified1DArrayFloatFloatLevel:\n    return \"NVPTXISD::TexUnified1DArrayFloatFloatLevel\";\n  case NVPTXISD::TexUnified1DArrayFloatFloatGrad:\n    return \"NVPTXISD::TexUnified1DArrayFloatFloatGrad\";\n  case NVPTXISD::TexUnified1DArrayS32S32:\n    return \"NVPTXISD::TexUnified1DArrayS32S32\";\n  case NVPTXISD::TexUnified1DArrayS32Float:\n    return \"NVPTXISD::TexUnified1DArrayS32Float\";\n  case NVPTXISD::TexUnified1DArrayS32FloatLevel:\n    return \"NVPTXISD::TexUnified1DArrayS32FloatLevel\";\n  case NVPTXISD::TexUnified1DArrayS32FloatGrad:\n    return \"NVPTXISD::TexUnified1DArrayS32FloatGrad\";\n  case NVPTXISD::TexUnified1DArrayU32S32:\n    return \"NVPTXISD::TexUnified1DArrayU32S32\";\n  case NVPTXISD::TexUnified1DArrayU32Float:\n    return \"NVPTXISD::TexUnified1DArrayU32Float\";\n  case NVPTXISD::TexUnified1DArrayU32FloatLevel:\n    return \"NVPTXISD::TexUnified1DArrayU32FloatLevel\";\n  case NVPTXISD::TexUnified1DArrayU32FloatGrad:\n    return \"NVPTXISD::TexUnified1DArrayU32FloatGrad\";\n  case NVPTXISD::TexUnified2DFloatS32:\n    return \"NVPTXISD::TexUnified2DFloatS32\";\n  case NVPTXISD::TexUnified2DFloatFloat:\n    return \"NVPTXISD::TexUnified2DFloatFloat\";\n  case NVPTXISD::TexUnified2DFloatFloatLevel:\n    return \"NVPTXISD::TexUnified2DFloatFloatLevel\";\n  case NVPTXISD::TexUnified2DFloatFloatGrad:\n    return \"NVPTXISD::TexUnified2DFloatFloatGrad\";\n  case NVPTXISD::TexUnified2DS32S32:\n    return \"NVPTXISD::TexUnified2DS32S32\";\n  case NVPTXISD::TexUnified2DS32Float:\n    return \"NVPTXISD::TexUnified2DS32Float\";\n  case NVPTXISD::TexUnified2DS32FloatLevel:\n    return \"NVPTXISD::TexUnified2DS32FloatLevel\";\n  case NVPTXISD::TexUnified2DS32FloatGrad:\n    return \"NVPTXISD::TexUnified2DS32FloatGrad\";\n  case NVPTXISD::TexUnified2DU32S32:\n    return \"NVPTXISD::TexUnified2DU32S32\";\n  case NVPTXISD::TexUnified2DU32Float:\n    return \"NVPTXISD::TexUnified2DU32Float\";\n  case NVPTXISD::TexUnified2DU32FloatLevel:\n    return \"NVPTXISD::TexUnified2DU32FloatLevel\";\n  case NVPTXISD::TexUnified2DU32FloatGrad:\n    return \"NVPTXISD::TexUnified2DU32FloatGrad\";\n  case NVPTXISD::TexUnified2DArrayFloatS32:\n    return \"NVPTXISD::TexUnified2DArrayFloatS32\";\n  case NVPTXISD::TexUnified2DArrayFloatFloat:\n    return \"NVPTXISD::TexUnified2DArrayFloatFloat\";\n  case NVPTXISD::TexUnified2DArrayFloatFloatLevel:\n    return \"NVPTXISD::TexUnified2DArrayFloatFloatLevel\";\n  case NVPTXISD::TexUnified2DArrayFloatFloatGrad:\n    return \"NVPTXISD::TexUnified2DArrayFloatFloatGrad\";\n  case NVPTXISD::TexUnified2DArrayS32S32:\n    return \"NVPTXISD::TexUnified2DArrayS32S32\";\n  case NVPTXISD::TexUnified2DArrayS32Float:\n    return \"NVPTXISD::TexUnified2DArrayS32Float\";\n  case NVPTXISD::TexUnified2DArrayS32FloatLevel:\n    return \"NVPTXISD::TexUnified2DArrayS32FloatLevel\";\n  case NVPTXISD::TexUnified2DArrayS32FloatGrad:\n    return \"NVPTXISD::TexUnified2DArrayS32FloatGrad\";\n  case NVPTXISD::TexUnified2DArrayU32S32:\n    return \"NVPTXISD::TexUnified2DArrayU32S32\";\n  case NVPTXISD::TexUnified2DArrayU32Float:\n    return \"NVPTXISD::TexUnified2DArrayU32Float\";\n  case NVPTXISD::TexUnified2DArrayU32FloatLevel:\n    return \"NVPTXISD::TexUnified2DArrayU32FloatLevel\";\n  case NVPTXISD::TexUnified2DArrayU32FloatGrad:\n    return \"NVPTXISD::TexUnified2DArrayU32FloatGrad\";\n  case NVPTXISD::TexUnified3DFloatS32:\n    return \"NVPTXISD::TexUnified3DFloatS32\";\n  case NVPTXISD::TexUnified3DFloatFloat:\n    return \"NVPTXISD::TexUnified3DFloatFloat\";\n  case NVPTXISD::TexUnified3DFloatFloatLevel:\n    return \"NVPTXISD::TexUnified3DFloatFloatLevel\";\n  case NVPTXISD::TexUnified3DFloatFloatGrad:\n    return \"NVPTXISD::TexUnified3DFloatFloatGrad\";\n  case NVPTXISD::TexUnified3DS32S32:\n    return \"NVPTXISD::TexUnified3DS32S32\";\n  case NVPTXISD::TexUnified3DS32Float:\n    return \"NVPTXISD::TexUnified3DS32Float\";\n  case NVPTXISD::TexUnified3DS32FloatLevel:\n    return \"NVPTXISD::TexUnified3DS32FloatLevel\";\n  case NVPTXISD::TexUnified3DS32FloatGrad:\n    return \"NVPTXISD::TexUnified3DS32FloatGrad\";\n  case NVPTXISD::TexUnified3DU32S32:\n    return \"NVPTXISD::TexUnified3DU32S32\";\n  case NVPTXISD::TexUnified3DU32Float:\n    return \"NVPTXISD::TexUnified3DU32Float\";\n  case NVPTXISD::TexUnified3DU32FloatLevel:\n    return \"NVPTXISD::TexUnified3DU32FloatLevel\";\n  case NVPTXISD::TexUnified3DU32FloatGrad:\n    return \"NVPTXISD::TexUnified3DU32FloatGrad\";\n  case NVPTXISD::TexUnifiedCubeFloatFloat:\n    return \"NVPTXISD::TexUnifiedCubeFloatFloat\";\n  case NVPTXISD::TexUnifiedCubeFloatFloatLevel:\n    return \"NVPTXISD::TexUnifiedCubeFloatFloatLevel\";\n  case NVPTXISD::TexUnifiedCubeS32Float:\n    return \"NVPTXISD::TexUnifiedCubeS32Float\";\n  case NVPTXISD::TexUnifiedCubeS32FloatLevel:\n    return \"NVPTXISD::TexUnifiedCubeS32FloatLevel\";\n  case NVPTXISD::TexUnifiedCubeU32Float:\n    return \"NVPTXISD::TexUnifiedCubeU32Float\";\n  case NVPTXISD::TexUnifiedCubeU32FloatLevel:\n    return \"NVPTXISD::TexUnifiedCubeU32FloatLevel\";\n  case NVPTXISD::TexUnifiedCubeArrayFloatFloat:\n    return \"NVPTXISD::TexUnifiedCubeArrayFloatFloat\";\n  case NVPTXISD::TexUnifiedCubeArrayFloatFloatLevel:\n    return \"NVPTXISD::TexUnifiedCubeArrayFloatFloatLevel\";\n  case NVPTXISD::TexUnifiedCubeArrayS32Float:\n    return \"NVPTXISD::TexUnifiedCubeArrayS32Float\";\n  case NVPTXISD::TexUnifiedCubeArrayS32FloatLevel:\n    return \"NVPTXISD::TexUnifiedCubeArrayS32FloatLevel\";\n  case NVPTXISD::TexUnifiedCubeArrayU32Float:\n    return \"NVPTXISD::TexUnifiedCubeArrayU32Float\";\n  case NVPTXISD::TexUnifiedCubeArrayU32FloatLevel:\n    return \"NVPTXISD::TexUnifiedCubeArrayU32FloatLevel\";\n  case NVPTXISD::Tld4UnifiedR2DFloatFloat:\n    return \"NVPTXISD::Tld4UnifiedR2DFloatFloat\";\n  case NVPTXISD::Tld4UnifiedG2DFloatFloat:\n    return \"NVPTXISD::Tld4UnifiedG2DFloatFloat\";\n  case NVPTXISD::Tld4UnifiedB2DFloatFloat:\n    return \"NVPTXISD::Tld4UnifiedB2DFloatFloat\";\n  case NVPTXISD::Tld4UnifiedA2DFloatFloat:\n    return \"NVPTXISD::Tld4UnifiedA2DFloatFloat\";\n  case NVPTXISD::Tld4UnifiedR2DS64Float:\n    return \"NVPTXISD::Tld4UnifiedR2DS64Float\";\n  case NVPTXISD::Tld4UnifiedG2DS64Float:\n    return \"NVPTXISD::Tld4UnifiedG2DS64Float\";\n  case NVPTXISD::Tld4UnifiedB2DS64Float:\n    return \"NVPTXISD::Tld4UnifiedB2DS64Float\";\n  case NVPTXISD::Tld4UnifiedA2DS64Float:\n    return \"NVPTXISD::Tld4UnifiedA2DS64Float\";\n  case NVPTXISD::Tld4UnifiedR2DU64Float:\n    return \"NVPTXISD::Tld4UnifiedR2DU64Float\";\n  case NVPTXISD::Tld4UnifiedG2DU64Float:\n    return \"NVPTXISD::Tld4UnifiedG2DU64Float\";\n  case NVPTXISD::Tld4UnifiedB2DU64Float:\n    return \"NVPTXISD::Tld4UnifiedB2DU64Float\";\n  case NVPTXISD::Tld4UnifiedA2DU64Float:\n    return \"NVPTXISD::Tld4UnifiedA2DU64Float\";\n\n  case NVPTXISD::Suld1DI8Clamp:          return \"NVPTXISD::Suld1DI8Clamp\";\n  case NVPTXISD::Suld1DI16Clamp:         return \"NVPTXISD::Suld1DI16Clamp\";\n  case NVPTXISD::Suld1DI32Clamp:         return \"NVPTXISD::Suld1DI32Clamp\";\n  case NVPTXISD::Suld1DI64Clamp:         return \"NVPTXISD::Suld1DI64Clamp\";\n  case NVPTXISD::Suld1DV2I8Clamp:        return \"NVPTXISD::Suld1DV2I8Clamp\";\n  case NVPTXISD::Suld1DV2I16Clamp:       return \"NVPTXISD::Suld1DV2I16Clamp\";\n  case NVPTXISD::Suld1DV2I32Clamp:       return \"NVPTXISD::Suld1DV2I32Clamp\";\n  case NVPTXISD::Suld1DV2I64Clamp:       return \"NVPTXISD::Suld1DV2I64Clamp\";\n  case NVPTXISD::Suld1DV4I8Clamp:        return \"NVPTXISD::Suld1DV4I8Clamp\";\n  case NVPTXISD::Suld1DV4I16Clamp:       return \"NVPTXISD::Suld1DV4I16Clamp\";\n  case NVPTXISD::Suld1DV4I32Clamp:       return \"NVPTXISD::Suld1DV4I32Clamp\";\n\n  case NVPTXISD::Suld1DArrayI8Clamp:   return \"NVPTXISD::Suld1DArrayI8Clamp\";\n  case NVPTXISD::Suld1DArrayI16Clamp:  return \"NVPTXISD::Suld1DArrayI16Clamp\";\n  case NVPTXISD::Suld1DArrayI32Clamp:  return \"NVPTXISD::Suld1DArrayI32Clamp\";\n  case NVPTXISD::Suld1DArrayI64Clamp:  return \"NVPTXISD::Suld1DArrayI64Clamp\";\n  case NVPTXISD::Suld1DArrayV2I8Clamp: return \"NVPTXISD::Suld1DArrayV2I8Clamp\";\n  case NVPTXISD::Suld1DArrayV2I16Clamp:return \"NVPTXISD::Suld1DArrayV2I16Clamp\";\n  case NVPTXISD::Suld1DArrayV2I32Clamp:return \"NVPTXISD::Suld1DArrayV2I32Clamp\";\n  case NVPTXISD::Suld1DArrayV2I64Clamp:return \"NVPTXISD::Suld1DArrayV2I64Clamp\";\n  case NVPTXISD::Suld1DArrayV4I8Clamp: return \"NVPTXISD::Suld1DArrayV4I8Clamp\";\n  case NVPTXISD::Suld1DArrayV4I16Clamp:return \"NVPTXISD::Suld1DArrayV4I16Clamp\";\n  case NVPTXISD::Suld1DArrayV4I32Clamp:return \"NVPTXISD::Suld1DArrayV4I32Clamp\";\n\n  case NVPTXISD::Suld2DI8Clamp:          return \"NVPTXISD::Suld2DI8Clamp\";\n  case NVPTXISD::Suld2DI16Clamp:         return \"NVPTXISD::Suld2DI16Clamp\";\n  case NVPTXISD::Suld2DI32Clamp:         return \"NVPTXISD::Suld2DI32Clamp\";\n  case NVPTXISD::Suld2DI64Clamp:         return \"NVPTXISD::Suld2DI64Clamp\";\n  case NVPTXISD::Suld2DV2I8Clamp:        return \"NVPTXISD::Suld2DV2I8Clamp\";\n  case NVPTXISD::Suld2DV2I16Clamp:       return \"NVPTXISD::Suld2DV2I16Clamp\";\n  case NVPTXISD::Suld2DV2I32Clamp:       return \"NVPTXISD::Suld2DV2I32Clamp\";\n  case NVPTXISD::Suld2DV2I64Clamp:       return \"NVPTXISD::Suld2DV2I64Clamp\";\n  case NVPTXISD::Suld2DV4I8Clamp:        return \"NVPTXISD::Suld2DV4I8Clamp\";\n  case NVPTXISD::Suld2DV4I16Clamp:       return \"NVPTXISD::Suld2DV4I16Clamp\";\n  case NVPTXISD::Suld2DV4I32Clamp:       return \"NVPTXISD::Suld2DV4I32Clamp\";\n\n  case NVPTXISD::Suld2DArrayI8Clamp:   return \"NVPTXISD::Suld2DArrayI8Clamp\";\n  case NVPTXISD::Suld2DArrayI16Clamp:  return \"NVPTXISD::Suld2DArrayI16Clamp\";\n  case NVPTXISD::Suld2DArrayI32Clamp:  return \"NVPTXISD::Suld2DArrayI32Clamp\";\n  case NVPTXISD::Suld2DArrayI64Clamp:  return \"NVPTXISD::Suld2DArrayI64Clamp\";\n  case NVPTXISD::Suld2DArrayV2I8Clamp: return \"NVPTXISD::Suld2DArrayV2I8Clamp\";\n  case NVPTXISD::Suld2DArrayV2I16Clamp:return \"NVPTXISD::Suld2DArrayV2I16Clamp\";\n  case NVPTXISD::Suld2DArrayV2I32Clamp:return \"NVPTXISD::Suld2DArrayV2I32Clamp\";\n  case NVPTXISD::Suld2DArrayV2I64Clamp:return \"NVPTXISD::Suld2DArrayV2I64Clamp\";\n  case NVPTXISD::Suld2DArrayV4I8Clamp: return \"NVPTXISD::Suld2DArrayV4I8Clamp\";\n  case NVPTXISD::Suld2DArrayV4I16Clamp:return \"NVPTXISD::Suld2DArrayV4I16Clamp\";\n  case NVPTXISD::Suld2DArrayV4I32Clamp:return \"NVPTXISD::Suld2DArrayV4I32Clamp\";\n\n  case NVPTXISD::Suld3DI8Clamp:          return \"NVPTXISD::Suld3DI8Clamp\";\n  case NVPTXISD::Suld3DI16Clamp:         return \"NVPTXISD::Suld3DI16Clamp\";\n  case NVPTXISD::Suld3DI32Clamp:         return \"NVPTXISD::Suld3DI32Clamp\";\n  case NVPTXISD::Suld3DI64Clamp:         return \"NVPTXISD::Suld3DI64Clamp\";\n  case NVPTXISD::Suld3DV2I8Clamp:        return \"NVPTXISD::Suld3DV2I8Clamp\";\n  case NVPTXISD::Suld3DV2I16Clamp:       return \"NVPTXISD::Suld3DV2I16Clamp\";\n  case NVPTXISD::Suld3DV2I32Clamp:       return \"NVPTXISD::Suld3DV2I32Clamp\";\n  case NVPTXISD::Suld3DV2I64Clamp:       return \"NVPTXISD::Suld3DV2I64Clamp\";\n  case NVPTXISD::Suld3DV4I8Clamp:        return \"NVPTXISD::Suld3DV4I8Clamp\";\n  case NVPTXISD::Suld3DV4I16Clamp:       return \"NVPTXISD::Suld3DV4I16Clamp\";\n  case NVPTXISD::Suld3DV4I32Clamp:       return \"NVPTXISD::Suld3DV4I32Clamp\";\n\n  case NVPTXISD::Suld1DI8Trap:          return \"NVPTXISD::Suld1DI8Trap\";\n  case NVPTXISD::Suld1DI16Trap:         return \"NVPTXISD::Suld1DI16Trap\";\n  case NVPTXISD::Suld1DI32Trap:         return \"NVPTXISD::Suld1DI32Trap\";\n  case NVPTXISD::Suld1DI64Trap:         return \"NVPTXISD::Suld1DI64Trap\";\n  case NVPTXISD::Suld1DV2I8Trap:        return \"NVPTXISD::Suld1DV2I8Trap\";\n  case NVPTXISD::Suld1DV2I16Trap:       return \"NVPTXISD::Suld1DV2I16Trap\";\n  case NVPTXISD::Suld1DV2I32Trap:       return \"NVPTXISD::Suld1DV2I32Trap\";\n  case NVPTXISD::Suld1DV2I64Trap:       return \"NVPTXISD::Suld1DV2I64Trap\";\n  case NVPTXISD::Suld1DV4I8Trap:        return \"NVPTXISD::Suld1DV4I8Trap\";\n  case NVPTXISD::Suld1DV4I16Trap:       return \"NVPTXISD::Suld1DV4I16Trap\";\n  case NVPTXISD::Suld1DV4I32Trap:       return \"NVPTXISD::Suld1DV4I32Trap\";\n\n  case NVPTXISD::Suld1DArrayI8Trap:     return \"NVPTXISD::Suld1DArrayI8Trap\";\n  case NVPTXISD::Suld1DArrayI16Trap:    return \"NVPTXISD::Suld1DArrayI16Trap\";\n  case NVPTXISD::Suld1DArrayI32Trap:    return \"NVPTXISD::Suld1DArrayI32Trap\";\n  case NVPTXISD::Suld1DArrayI64Trap:    return \"NVPTXISD::Suld1DArrayI64Trap\";\n  case NVPTXISD::Suld1DArrayV2I8Trap:   return \"NVPTXISD::Suld1DArrayV2I8Trap\";\n  case NVPTXISD::Suld1DArrayV2I16Trap:  return \"NVPTXISD::Suld1DArrayV2I16Trap\";\n  case NVPTXISD::Suld1DArrayV2I32Trap:  return \"NVPTXISD::Suld1DArrayV2I32Trap\";\n  case NVPTXISD::Suld1DArrayV2I64Trap:  return \"NVPTXISD::Suld1DArrayV2I64Trap\";\n  case NVPTXISD::Suld1DArrayV4I8Trap:   return \"NVPTXISD::Suld1DArrayV4I8Trap\";\n  case NVPTXISD::Suld1DArrayV4I16Trap:  return \"NVPTXISD::Suld1DArrayV4I16Trap\";\n  case NVPTXISD::Suld1DArrayV4I32Trap:  return \"NVPTXISD::Suld1DArrayV4I32Trap\";\n\n  case NVPTXISD::Suld2DI8Trap:          return \"NVPTXISD::Suld2DI8Trap\";\n  case NVPTXISD::Suld2DI16Trap:         return \"NVPTXISD::Suld2DI16Trap\";\n  case NVPTXISD::Suld2DI32Trap:         return \"NVPTXISD::Suld2DI32Trap\";\n  case NVPTXISD::Suld2DI64Trap:         return \"NVPTXISD::Suld2DI64Trap\";\n  case NVPTXISD::Suld2DV2I8Trap:        return \"NVPTXISD::Suld2DV2I8Trap\";\n  case NVPTXISD::Suld2DV2I16Trap:       return \"NVPTXISD::Suld2DV2I16Trap\";\n  case NVPTXISD::Suld2DV2I32Trap:       return \"NVPTXISD::Suld2DV2I32Trap\";\n  case NVPTXISD::Suld2DV2I64Trap:       return \"NVPTXISD::Suld2DV2I64Trap\";\n  case NVPTXISD::Suld2DV4I8Trap:        return \"NVPTXISD::Suld2DV4I8Trap\";\n  case NVPTXISD::Suld2DV4I16Trap:       return \"NVPTXISD::Suld2DV4I16Trap\";\n  case NVPTXISD::Suld2DV4I32Trap:       return \"NVPTXISD::Suld2DV4I32Trap\";\n\n  case NVPTXISD::Suld2DArrayI8Trap:     return \"NVPTXISD::Suld2DArrayI8Trap\";\n  case NVPTXISD::Suld2DArrayI16Trap:    return \"NVPTXISD::Suld2DArrayI16Trap\";\n  case NVPTXISD::Suld2DArrayI32Trap:    return \"NVPTXISD::Suld2DArrayI32Trap\";\n  case NVPTXISD::Suld2DArrayI64Trap:    return \"NVPTXISD::Suld2DArrayI64Trap\";\n  case NVPTXISD::Suld2DArrayV2I8Trap:   return \"NVPTXISD::Suld2DArrayV2I8Trap\";\n  case NVPTXISD::Suld2DArrayV2I16Trap:  return \"NVPTXISD::Suld2DArrayV2I16Trap\";\n  case NVPTXISD::Suld2DArrayV2I32Trap:  return \"NVPTXISD::Suld2DArrayV2I32Trap\";\n  case NVPTXISD::Suld2DArrayV2I64Trap:  return \"NVPTXISD::Suld2DArrayV2I64Trap\";\n  case NVPTXISD::Suld2DArrayV4I8Trap:   return \"NVPTXISD::Suld2DArrayV4I8Trap\";\n  case NVPTXISD::Suld2DArrayV4I16Trap:  return \"NVPTXISD::Suld2DArrayV4I16Trap\";\n  case NVPTXISD::Suld2DArrayV4I32Trap:  return \"NVPTXISD::Suld2DArrayV4I32Trap\";\n\n  case NVPTXISD::Suld3DI8Trap:          return \"NVPTXISD::Suld3DI8Trap\";\n  case NVPTXISD::Suld3DI16Trap:         return \"NVPTXISD::Suld3DI16Trap\";\n  case NVPTXISD::Suld3DI32Trap:         return \"NVPTXISD::Suld3DI32Trap\";\n  case NVPTXISD::Suld3DI64Trap:         return \"NVPTXISD::Suld3DI64Trap\";\n  case NVPTXISD::Suld3DV2I8Trap:        return \"NVPTXISD::Suld3DV2I8Trap\";\n  case NVPTXISD::Suld3DV2I16Trap:       return \"NVPTXISD::Suld3DV2I16Trap\";\n  case NVPTXISD::Suld3DV2I32Trap:       return \"NVPTXISD::Suld3DV2I32Trap\";\n  case NVPTXISD::Suld3DV2I64Trap:       return \"NVPTXISD::Suld3DV2I64Trap\";\n  case NVPTXISD::Suld3DV4I8Trap:        return \"NVPTXISD::Suld3DV4I8Trap\";\n  case NVPTXISD::Suld3DV4I16Trap:       return \"NVPTXISD::Suld3DV4I16Trap\";\n  case NVPTXISD::Suld3DV4I32Trap:       return \"NVPTXISD::Suld3DV4I32Trap\";\n\n  case NVPTXISD::Suld1DI8Zero:          return \"NVPTXISD::Suld1DI8Zero\";\n  case NVPTXISD::Suld1DI16Zero:         return \"NVPTXISD::Suld1DI16Zero\";\n  case NVPTXISD::Suld1DI32Zero:         return \"NVPTXISD::Suld1DI32Zero\";\n  case NVPTXISD::Suld1DI64Zero:         return \"NVPTXISD::Suld1DI64Zero\";\n  case NVPTXISD::Suld1DV2I8Zero:        return \"NVPTXISD::Suld1DV2I8Zero\";\n  case NVPTXISD::Suld1DV2I16Zero:       return \"NVPTXISD::Suld1DV2I16Zero\";\n  case NVPTXISD::Suld1DV2I32Zero:       return \"NVPTXISD::Suld1DV2I32Zero\";\n  case NVPTXISD::Suld1DV2I64Zero:       return \"NVPTXISD::Suld1DV2I64Zero\";\n  case NVPTXISD::Suld1DV4I8Zero:        return \"NVPTXISD::Suld1DV4I8Zero\";\n  case NVPTXISD::Suld1DV4I16Zero:       return \"NVPTXISD::Suld1DV4I16Zero\";\n  case NVPTXISD::Suld1DV4I32Zero:       return \"NVPTXISD::Suld1DV4I32Zero\";\n\n  case NVPTXISD::Suld1DArrayI8Zero:     return \"NVPTXISD::Suld1DArrayI8Zero\";\n  case NVPTXISD::Suld1DArrayI16Zero:    return \"NVPTXISD::Suld1DArrayI16Zero\";\n  case NVPTXISD::Suld1DArrayI32Zero:    return \"NVPTXISD::Suld1DArrayI32Zero\";\n  case NVPTXISD::Suld1DArrayI64Zero:    return \"NVPTXISD::Suld1DArrayI64Zero\";\n  case NVPTXISD::Suld1DArrayV2I8Zero:   return \"NVPTXISD::Suld1DArrayV2I8Zero\";\n  case NVPTXISD::Suld1DArrayV2I16Zero:  return \"NVPTXISD::Suld1DArrayV2I16Zero\";\n  case NVPTXISD::Suld1DArrayV2I32Zero:  return \"NVPTXISD::Suld1DArrayV2I32Zero\";\n  case NVPTXISD::Suld1DArrayV2I64Zero:  return \"NVPTXISD::Suld1DArrayV2I64Zero\";\n  case NVPTXISD::Suld1DArrayV4I8Zero:   return \"NVPTXISD::Suld1DArrayV4I8Zero\";\n  case NVPTXISD::Suld1DArrayV4I16Zero:  return \"NVPTXISD::Suld1DArrayV4I16Zero\";\n  case NVPTXISD::Suld1DArrayV4I32Zero:  return \"NVPTXISD::Suld1DArrayV4I32Zero\";\n\n  case NVPTXISD::Suld2DI8Zero:          return \"NVPTXISD::Suld2DI8Zero\";\n  case NVPTXISD::Suld2DI16Zero:         return \"NVPTXISD::Suld2DI16Zero\";\n  case NVPTXISD::Suld2DI32Zero:         return \"NVPTXISD::Suld2DI32Zero\";\n  case NVPTXISD::Suld2DI64Zero:         return \"NVPTXISD::Suld2DI64Zero\";\n  case NVPTXISD::Suld2DV2I8Zero:        return \"NVPTXISD::Suld2DV2I8Zero\";\n  case NVPTXISD::Suld2DV2I16Zero:       return \"NVPTXISD::Suld2DV2I16Zero\";\n  case NVPTXISD::Suld2DV2I32Zero:       return \"NVPTXISD::Suld2DV2I32Zero\";\n  case NVPTXISD::Suld2DV2I64Zero:       return \"NVPTXISD::Suld2DV2I64Zero\";\n  case NVPTXISD::Suld2DV4I8Zero:        return \"NVPTXISD::Suld2DV4I8Zero\";\n  case NVPTXISD::Suld2DV4I16Zero:       return \"NVPTXISD::Suld2DV4I16Zero\";\n  case NVPTXISD::Suld2DV4I32Zero:       return \"NVPTXISD::Suld2DV4I32Zero\";\n\n  case NVPTXISD::Suld2DArrayI8Zero:     return \"NVPTXISD::Suld2DArrayI8Zero\";\n  case NVPTXISD::Suld2DArrayI16Zero:    return \"NVPTXISD::Suld2DArrayI16Zero\";\n  case NVPTXISD::Suld2DArrayI32Zero:    return \"NVPTXISD::Suld2DArrayI32Zero\";\n  case NVPTXISD::Suld2DArrayI64Zero:    return \"NVPTXISD::Suld2DArrayI64Zero\";\n  case NVPTXISD::Suld2DArrayV2I8Zero:   return \"NVPTXISD::Suld2DArrayV2I8Zero\";\n  case NVPTXISD::Suld2DArrayV2I16Zero:  return \"NVPTXISD::Suld2DArrayV2I16Zero\";\n  case NVPTXISD::Suld2DArrayV2I32Zero:  return \"NVPTXISD::Suld2DArrayV2I32Zero\";\n  case NVPTXISD::Suld2DArrayV2I64Zero:  return \"NVPTXISD::Suld2DArrayV2I64Zero\";\n  case NVPTXISD::Suld2DArrayV4I8Zero:   return \"NVPTXISD::Suld2DArrayV4I8Zero\";\n  case NVPTXISD::Suld2DArrayV4I16Zero:  return \"NVPTXISD::Suld2DArrayV4I16Zero\";\n  case NVPTXISD::Suld2DArrayV4I32Zero:  return \"NVPTXISD::Suld2DArrayV4I32Zero\";\n\n  case NVPTXISD::Suld3DI8Zero:          return \"NVPTXISD::Suld3DI8Zero\";\n  case NVPTXISD::Suld3DI16Zero:         return \"NVPTXISD::Suld3DI16Zero\";\n  case NVPTXISD::Suld3DI32Zero:         return \"NVPTXISD::Suld3DI32Zero\";\n  case NVPTXISD::Suld3DI64Zero:         return \"NVPTXISD::Suld3DI64Zero\";\n  case NVPTXISD::Suld3DV2I8Zero:        return \"NVPTXISD::Suld3DV2I8Zero\";\n  case NVPTXISD::Suld3DV2I16Zero:       return \"NVPTXISD::Suld3DV2I16Zero\";\n  case NVPTXISD::Suld3DV2I32Zero:       return \"NVPTXISD::Suld3DV2I32Zero\";\n  case NVPTXISD::Suld3DV2I64Zero:       return \"NVPTXISD::Suld3DV2I64Zero\";\n  case NVPTXISD::Suld3DV4I8Zero:        return \"NVPTXISD::Suld3DV4I8Zero\";\n  case NVPTXISD::Suld3DV4I16Zero:       return \"NVPTXISD::Suld3DV4I16Zero\";\n  case NVPTXISD::Suld3DV4I32Zero:       return \"NVPTXISD::Suld3DV4I32Zero\";\n  }\n  return nullptr;\n}\n\nTargetLoweringBase::LegalizeTypeAction\n"},"LowerCONCAT_VECTORS":{"range":[1679,1701],"code":"NVPTXTargetLowering::LowerCONCAT_VECTORS(SDValue Op, SelectionDAG &DAG) const {\n  SDNode *Node = Op.getNode();\n  SDLoc dl(Node);\n  SmallVector<SDValue, 8> Ops;\n  unsigned NumOperands = Node->getNumOperands();\n  for (unsigned i = 0; i < NumOperands; ++i) {\n    SDValue SubOp = Node->getOperand(i);\n    EVT VVT = SubOp.getNode()->getValueType(0);\n    EVT EltVT = VVT.getVectorElementType();\n    unsigned NumSubElem = VVT.getVectorNumElements();\n    for (unsigned j = 0; j < NumSubElem; ++j) {\n      Ops.push_back(DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, EltVT, SubOp,\n                                DAG.getIntPtrConstant(j, dl)));\n    }\n  }\n  return DAG.getBuildVector(Node->getValueType(0), dl, Ops);\n}\n\n/// LowerShiftRightParts - Lower SRL_PARTS, SRA_PARTS, which\n/// 1) returns two i32 values and take a 2 x i32 value to shift plus a shift\n///    amount, or\n/// 2) returns two i64 values and take a 2 x i64 value to shift plus a shift\n///    amount.\n"},"getConstraintType":{"range":[3754,3774],"code":"NVPTXTargetLowering::getConstraintType(StringRef Constraint) const {\n  if (Constraint.size() == 1) {\n    switch (Constraint[0]) {\n    default:\n      break;\n    case 'b':\n    case 'r':\n    case 'h':\n    case 'c':\n    case 'l':\n    case 'f':\n    case 'd':\n    case '0':\n    case 'N':\n      return C_RegisterClass;\n    }\n  }\n  return TargetLowering::getConstraintType(Constraint);\n}\n\nstd::pair<unsigned, const TargetRegisterClass *>\n"},"AreMulWideOperandsDemotable":{"range":[4128,4164],"code":"static bool AreMulWideOperandsDemotable(SDValue LHS, SDValue RHS,\n                                        unsigned OptSize,\n                                        bool &IsSigned) {\n\n  OperandSignedness LHSSign;\n\n  // The LHS operand must be a demotable op\n  if (!IsMulWideOperandDemotable(LHS, OptSize, LHSSign))\n    return false;\n\n  // We should have been able to determine the signedness from the LHS\n  if (LHSSign == Unknown)\n    return false;\n\n  IsSigned = (LHSSign == Signed);\n\n  // The RHS can be a demotable op or a constant\n  if (ConstantSDNode *CI = dyn_cast<ConstantSDNode>(RHS)) {\n    const APInt &Val = CI->getAPIntValue();\n    if (LHSSign == Unsigned) {\n      return Val.isIntN(OptSize);\n    } else {\n      return Val.isSignedIntN(OptSize);\n    }\n  } else {\n    OperandSignedness RHSSign;\n    if (!IsMulWideOperandDemotable(RHS, OptSize, RHSSign))\n      return false;\n\n    return LHSSign == RHSSign;\n  }\n}\n\n/// TryMULWIDECombine - Attempt to replace a multiply of M bits with a multiply\n/// of M/2 bits that produces an M-bit result (i.e. mul.wide). This transform\n/// works on both multiply DAG nodes and SHL DAG nodes with a constant shift\n/// amount.\n"},"PerformDAGCombine":{"range":[4257,4277],"code":"SDValue NVPTXTargetLowering::PerformDAGCombine(SDNode *N,\n                                               DAGCombinerInfo &DCI) const {\n  CodeGenOpt::Level OptLevel = getTargetMachine().getOptLevel();\n  switch (N->getOpcode()) {\n    default: break;\n    case ISD::ADD:\n    case ISD::FADD:\n      return PerformADDCombine(N, DCI, STI, OptLevel);\n    case ISD::MUL:\n      return PerformMULCombine(N, DCI, OptLevel);\n    case ISD::SHL:\n      return PerformSHLCombine(N, DCI, OptLevel);\n    case ISD::AND:\n      return PerformANDCombine(N, DCI);\n    case ISD::SELECT:\n      return PerformSELECTCombine(N, DCI);\n  }\n  return SDValue();\n}\n\n/// ReplaceVectorLoad - Convert vector loads into multi-output scalar loads.\n"},"allowFMA":{"range":[3804,3833],"code":"bool NVPTXTargetLowering::allowFMA(MachineFunction &MF,\n                                   CodeGenOpt::Level OptLevel) const {\n  const Function *F = MF.getFunction();\n  const TargetOptions &TO = MF.getTarget().Options;\n\n  // Always honor command-line argument\n  if (FMAContractLevelOpt.getNumOccurrences() > 0) {\n    return FMAContractLevelOpt > 0;\n  } else if (OptLevel == 0) {\n    // Do not contract if we're not optimizing the code\n    return false;\n  } else if (TO.AllowFPOpFusion == FPOpFusion::Fast || TO.UnsafeFPMath) {\n    // Honor TargetOptions flags that explicitly say fusion is okay\n    return true;\n  } else if (F->hasFnAttribute(\"unsafe-fp-math\")) {\n    // Check for unsafe-fp-math=true coming from Clang\n    Attribute Attr = F->getFnAttribute(\"unsafe-fp-math\");\n    StringRef Val = Attr.getValueAsString();\n    if (Val == \"true\")\n      return true;\n  }\n\n  // We did not have a clear indication that fusion is allowed, so assume not\n  return false;\n}\n\n/// PerformADDCombineWithOperands - Try DAG combinations for an ADD with\n/// operands N0 and N1.  This is a helper for PerformADDCombine that is\n/// called with the default operands, and if that fails, with commuted\n/// operands.\n"},"PerformMULCombine":{"range":[4232,4244],"code":"static SDValue PerformMULCombine(SDNode *N,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 CodeGenOpt::Level OptLevel) {\n  if (OptLevel > 0) {\n    // Try mul.wide combining at OptLevel > 0\n    if (SDValue Ret = TryMULWIDECombine(N, DCI))\n      return Ret;\n  }\n\n  return SDValue();\n}\n\n/// PerformSHLCombine - Runs PTX-specific DAG combine patterns on SHL nodes.\n"},"SelectSectionForGlobal":{"range":[4558,4563],"code":"NVPTXTargetObjectFile::SelectSectionForGlobal(const GlobalValue *GV,\n                                              SectionKind Kind, Mangler &Mang,\n                                              const TargetMachine &TM) const {\n  return getDataSection();\n}\n"},"getTgtMemIntrinsic":{"range":[3242,3714],"code":"bool NVPTXTargetLowering::getTgtMemIntrinsic(\n    IntrinsicInfo &Info, const CallInst &I, unsigned Intrinsic) const {\n  switch (Intrinsic) {\n  default:\n    return false;\n\n  case Intrinsic::nvvm_atomic_load_add_f32:\n    Info.opc = ISD::INTRINSIC_W_CHAIN;\n    Info.memVT = MVT::f32;\n    Info.ptrVal = I.getArgOperand(0);\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = true;\n    Info.align = 0;\n    return true;\n\n  case Intrinsic::nvvm_atomic_load_inc_32:\n  case Intrinsic::nvvm_atomic_load_dec_32:\n    Info.opc = ISD::INTRINSIC_W_CHAIN;\n    Info.memVT = MVT::i32;\n    Info.ptrVal = I.getArgOperand(0);\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = true;\n    Info.align = 0;\n    return true;\n\n  case Intrinsic::nvvm_ldu_global_i:\n  case Intrinsic::nvvm_ldu_global_f:\n  case Intrinsic::nvvm_ldu_global_p: {\n    auto &DL = I.getModule()->getDataLayout();\n    Info.opc = ISD::INTRINSIC_W_CHAIN;\n    if (Intrinsic == Intrinsic::nvvm_ldu_global_i)\n      Info.memVT = getValueType(DL, I.getType());\n    else if(Intrinsic == Intrinsic::nvvm_ldu_global_p)\n      Info.memVT = getPointerTy(DL);\n    else\n      Info.memVT = getValueType(DL, I.getType());\n    Info.ptrVal = I.getArgOperand(0);\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = cast<ConstantInt>(I.getArgOperand(1))->getZExtValue();\n\n    return true;\n  }\n  case Intrinsic::nvvm_ldg_global_i:\n  case Intrinsic::nvvm_ldg_global_f:\n  case Intrinsic::nvvm_ldg_global_p: {\n    auto &DL = I.getModule()->getDataLayout();\n\n    Info.opc = ISD::INTRINSIC_W_CHAIN;\n    if (Intrinsic == Intrinsic::nvvm_ldg_global_i)\n      Info.memVT = getValueType(DL, I.getType());\n    else if(Intrinsic == Intrinsic::nvvm_ldg_global_p)\n      Info.memVT = getPointerTy(DL);\n    else\n      Info.memVT = getValueType(DL, I.getType());\n    Info.ptrVal = I.getArgOperand(0);\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = cast<ConstantInt>(I.getArgOperand(1))->getZExtValue();\n\n    return true;\n  }\n\n  case Intrinsic::nvvm_tex_1d_v4f32_s32:\n  case Intrinsic::nvvm_tex_1d_v4f32_f32:\n  case Intrinsic::nvvm_tex_1d_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_1d_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_1d_array_v4f32_s32:\n  case Intrinsic::nvvm_tex_1d_array_v4f32_f32:\n  case Intrinsic::nvvm_tex_1d_array_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_1d_array_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_2d_v4f32_s32:\n  case Intrinsic::nvvm_tex_2d_v4f32_f32:\n  case Intrinsic::nvvm_tex_2d_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_2d_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_2d_array_v4f32_s32:\n  case Intrinsic::nvvm_tex_2d_array_v4f32_f32:\n  case Intrinsic::nvvm_tex_2d_array_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_2d_array_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_3d_v4f32_s32:\n  case Intrinsic::nvvm_tex_3d_v4f32_f32:\n  case Intrinsic::nvvm_tex_3d_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_3d_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_cube_v4f32_f32:\n  case Intrinsic::nvvm_tex_cube_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_cube_array_v4f32_f32:\n  case Intrinsic::nvvm_tex_cube_array_level_v4f32_f32:\n  case Intrinsic::nvvm_tld4_r_2d_v4f32_f32:\n  case Intrinsic::nvvm_tld4_g_2d_v4f32_f32:\n  case Intrinsic::nvvm_tld4_b_2d_v4f32_f32:\n  case Intrinsic::nvvm_tld4_a_2d_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_v4f32_s32:\n  case Intrinsic::nvvm_tex_unified_1d_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_v4f32_s32:\n  case Intrinsic::nvvm_tex_unified_1d_array_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_v4f32_s32:\n  case Intrinsic::nvvm_tex_unified_2d_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_v4f32_s32:\n  case Intrinsic::nvvm_tex_unified_2d_array_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_v4f32_s32:\n  case Intrinsic::nvvm_tex_unified_3d_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_grad_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_level_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_array_v4f32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_array_level_v4f32_f32:\n  case Intrinsic::nvvm_tld4_unified_r_2d_v4f32_f32:\n  case Intrinsic::nvvm_tld4_unified_g_2d_v4f32_f32:\n  case Intrinsic::nvvm_tld4_unified_b_2d_v4f32_f32:\n  case Intrinsic::nvvm_tld4_unified_a_2d_v4f32_f32: {\n    Info.opc = getOpcForTextureInstr(Intrinsic);\n    Info.memVT = MVT::v4f32;\n    Info.ptrVal = nullptr;\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = 16;\n    return true;\n  }\n  case Intrinsic::nvvm_tex_1d_v4s32_s32:\n  case Intrinsic::nvvm_tex_1d_v4s32_f32:\n  case Intrinsic::nvvm_tex_1d_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_1d_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_1d_array_v4s32_s32:\n  case Intrinsic::nvvm_tex_1d_array_v4s32_f32:\n  case Intrinsic::nvvm_tex_1d_array_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_1d_array_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_2d_v4s32_s32:\n  case Intrinsic::nvvm_tex_2d_v4s32_f32:\n  case Intrinsic::nvvm_tex_2d_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_2d_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_2d_array_v4s32_s32:\n  case Intrinsic::nvvm_tex_2d_array_v4s32_f32:\n  case Intrinsic::nvvm_tex_2d_array_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_2d_array_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_3d_v4s32_s32:\n  case Intrinsic::nvvm_tex_3d_v4s32_f32:\n  case Intrinsic::nvvm_tex_3d_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_3d_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_cube_v4s32_f32:\n  case Intrinsic::nvvm_tex_cube_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_cube_array_v4s32_f32:\n  case Intrinsic::nvvm_tex_cube_array_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_cube_v4u32_f32:\n  case Intrinsic::nvvm_tex_cube_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_cube_array_v4u32_f32:\n  case Intrinsic::nvvm_tex_cube_array_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_1d_v4u32_s32:\n  case Intrinsic::nvvm_tex_1d_v4u32_f32:\n  case Intrinsic::nvvm_tex_1d_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_1d_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_1d_array_v4u32_s32:\n  case Intrinsic::nvvm_tex_1d_array_v4u32_f32:\n  case Intrinsic::nvvm_tex_1d_array_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_1d_array_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_2d_v4u32_s32:\n  case Intrinsic::nvvm_tex_2d_v4u32_f32:\n  case Intrinsic::nvvm_tex_2d_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_2d_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_2d_array_v4u32_s32:\n  case Intrinsic::nvvm_tex_2d_array_v4u32_f32:\n  case Intrinsic::nvvm_tex_2d_array_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_2d_array_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_3d_v4u32_s32:\n  case Intrinsic::nvvm_tex_3d_v4u32_f32:\n  case Intrinsic::nvvm_tex_3d_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_3d_grad_v4u32_f32:\n  case Intrinsic::nvvm_tld4_r_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_g_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_b_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_a_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_r_2d_v4u32_f32:\n  case Intrinsic::nvvm_tld4_g_2d_v4u32_f32:\n  case Intrinsic::nvvm_tld4_b_2d_v4u32_f32:\n  case Intrinsic::nvvm_tld4_a_2d_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_v4s32_s32:\n  case Intrinsic::nvvm_tex_unified_1d_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_v4s32_s32:\n  case Intrinsic::nvvm_tex_unified_1d_array_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_v4s32_s32:\n  case Intrinsic::nvvm_tex_unified_2d_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_v4s32_s32:\n  case Intrinsic::nvvm_tex_unified_2d_array_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_v4s32_s32:\n  case Intrinsic::nvvm_tex_unified_3d_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_grad_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_v4u32_s32:\n  case Intrinsic::nvvm_tex_unified_1d_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_v4u32_s32:\n  case Intrinsic::nvvm_tex_unified_1d_array_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_1d_array_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_v4u32_s32:\n  case Intrinsic::nvvm_tex_unified_2d_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_v4u32_s32:\n  case Intrinsic::nvvm_tex_unified_2d_array_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_2d_array_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_v4u32_s32:\n  case Intrinsic::nvvm_tex_unified_3d_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_3d_grad_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_array_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_array_level_v4s32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_level_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_array_v4u32_f32:\n  case Intrinsic::nvvm_tex_unified_cube_array_level_v4u32_f32:\n  case Intrinsic::nvvm_tld4_unified_r_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_unified_g_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_unified_b_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_unified_a_2d_v4s32_f32:\n  case Intrinsic::nvvm_tld4_unified_r_2d_v4u32_f32:\n  case Intrinsic::nvvm_tld4_unified_g_2d_v4u32_f32:\n  case Intrinsic::nvvm_tld4_unified_b_2d_v4u32_f32:\n  case Intrinsic::nvvm_tld4_unified_a_2d_v4u32_f32: {\n    Info.opc = getOpcForTextureInstr(Intrinsic);\n    Info.memVT = MVT::v4i32;\n    Info.ptrVal = nullptr;\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = 16;\n    return true;\n  }\n  case Intrinsic::nvvm_suld_1d_i8_clamp:\n  case Intrinsic::nvvm_suld_1d_v2i8_clamp:\n  case Intrinsic::nvvm_suld_1d_v4i8_clamp:\n  case Intrinsic::nvvm_suld_1d_array_i8_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v2i8_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v4i8_clamp:\n  case Intrinsic::nvvm_suld_2d_i8_clamp:\n  case Intrinsic::nvvm_suld_2d_v2i8_clamp:\n  case Intrinsic::nvvm_suld_2d_v4i8_clamp:\n  case Intrinsic::nvvm_suld_2d_array_i8_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v2i8_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v4i8_clamp:\n  case Intrinsic::nvvm_suld_3d_i8_clamp:\n  case Intrinsic::nvvm_suld_3d_v2i8_clamp:\n  case Intrinsic::nvvm_suld_3d_v4i8_clamp:\n  case Intrinsic::nvvm_suld_1d_i8_trap:\n  case Intrinsic::nvvm_suld_1d_v2i8_trap:\n  case Intrinsic::nvvm_suld_1d_v4i8_trap:\n  case Intrinsic::nvvm_suld_1d_array_i8_trap:\n  case Intrinsic::nvvm_suld_1d_array_v2i8_trap:\n  case Intrinsic::nvvm_suld_1d_array_v4i8_trap:\n  case Intrinsic::nvvm_suld_2d_i8_trap:\n  case Intrinsic::nvvm_suld_2d_v2i8_trap:\n  case Intrinsic::nvvm_suld_2d_v4i8_trap:\n  case Intrinsic::nvvm_suld_2d_array_i8_trap:\n  case Intrinsic::nvvm_suld_2d_array_v2i8_trap:\n  case Intrinsic::nvvm_suld_2d_array_v4i8_trap:\n  case Intrinsic::nvvm_suld_3d_i8_trap:\n  case Intrinsic::nvvm_suld_3d_v2i8_trap:\n  case Intrinsic::nvvm_suld_3d_v4i8_trap:\n  case Intrinsic::nvvm_suld_1d_i8_zero:\n  case Intrinsic::nvvm_suld_1d_v2i8_zero:\n  case Intrinsic::nvvm_suld_1d_v4i8_zero:\n  case Intrinsic::nvvm_suld_1d_array_i8_zero:\n  case Intrinsic::nvvm_suld_1d_array_v2i8_zero:\n  case Intrinsic::nvvm_suld_1d_array_v4i8_zero:\n  case Intrinsic::nvvm_suld_2d_i8_zero:\n  case Intrinsic::nvvm_suld_2d_v2i8_zero:\n  case Intrinsic::nvvm_suld_2d_v4i8_zero:\n  case Intrinsic::nvvm_suld_2d_array_i8_zero:\n  case Intrinsic::nvvm_suld_2d_array_v2i8_zero:\n  case Intrinsic::nvvm_suld_2d_array_v4i8_zero:\n  case Intrinsic::nvvm_suld_3d_i8_zero:\n  case Intrinsic::nvvm_suld_3d_v2i8_zero:\n  case Intrinsic::nvvm_suld_3d_v4i8_zero: {\n    Info.opc = getOpcForSurfaceInstr(Intrinsic);\n    Info.memVT = MVT::i8;\n    Info.ptrVal = nullptr;\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = 16;\n    return true;\n  }\n  case Intrinsic::nvvm_suld_1d_i16_clamp:\n  case Intrinsic::nvvm_suld_1d_v2i16_clamp:\n  case Intrinsic::nvvm_suld_1d_v4i16_clamp:\n  case Intrinsic::nvvm_suld_1d_array_i16_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v2i16_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v4i16_clamp:\n  case Intrinsic::nvvm_suld_2d_i16_clamp:\n  case Intrinsic::nvvm_suld_2d_v2i16_clamp:\n  case Intrinsic::nvvm_suld_2d_v4i16_clamp:\n  case Intrinsic::nvvm_suld_2d_array_i16_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v2i16_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v4i16_clamp:\n  case Intrinsic::nvvm_suld_3d_i16_clamp:\n  case Intrinsic::nvvm_suld_3d_v2i16_clamp:\n  case Intrinsic::nvvm_suld_3d_v4i16_clamp:\n  case Intrinsic::nvvm_suld_1d_i16_trap:\n  case Intrinsic::nvvm_suld_1d_v2i16_trap:\n  case Intrinsic::nvvm_suld_1d_v4i16_trap:\n  case Intrinsic::nvvm_suld_1d_array_i16_trap:\n  case Intrinsic::nvvm_suld_1d_array_v2i16_trap:\n  case Intrinsic::nvvm_suld_1d_array_v4i16_trap:\n  case Intrinsic::nvvm_suld_2d_i16_trap:\n  case Intrinsic::nvvm_suld_2d_v2i16_trap:\n  case Intrinsic::nvvm_suld_2d_v4i16_trap:\n  case Intrinsic::nvvm_suld_2d_array_i16_trap:\n  case Intrinsic::nvvm_suld_2d_array_v2i16_trap:\n  case Intrinsic::nvvm_suld_2d_array_v4i16_trap:\n  case Intrinsic::nvvm_suld_3d_i16_trap:\n  case Intrinsic::nvvm_suld_3d_v2i16_trap:\n  case Intrinsic::nvvm_suld_3d_v4i16_trap:\n  case Intrinsic::nvvm_suld_1d_i16_zero:\n  case Intrinsic::nvvm_suld_1d_v2i16_zero:\n  case Intrinsic::nvvm_suld_1d_v4i16_zero:\n  case Intrinsic::nvvm_suld_1d_array_i16_zero:\n  case Intrinsic::nvvm_suld_1d_array_v2i16_zero:\n  case Intrinsic::nvvm_suld_1d_array_v4i16_zero:\n  case Intrinsic::nvvm_suld_2d_i16_zero:\n  case Intrinsic::nvvm_suld_2d_v2i16_zero:\n  case Intrinsic::nvvm_suld_2d_v4i16_zero:\n  case Intrinsic::nvvm_suld_2d_array_i16_zero:\n  case Intrinsic::nvvm_suld_2d_array_v2i16_zero:\n  case Intrinsic::nvvm_suld_2d_array_v4i16_zero:\n  case Intrinsic::nvvm_suld_3d_i16_zero:\n  case Intrinsic::nvvm_suld_3d_v2i16_zero:\n  case Intrinsic::nvvm_suld_3d_v4i16_zero: {\n    Info.opc = getOpcForSurfaceInstr(Intrinsic);\n    Info.memVT = MVT::i16;\n    Info.ptrVal = nullptr;\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = 16;\n    return true;\n  }\n  case Intrinsic::nvvm_suld_1d_i32_clamp:\n  case Intrinsic::nvvm_suld_1d_v2i32_clamp:\n  case Intrinsic::nvvm_suld_1d_v4i32_clamp:\n  case Intrinsic::nvvm_suld_1d_array_i32_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v2i32_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v4i32_clamp:\n  case Intrinsic::nvvm_suld_2d_i32_clamp:\n  case Intrinsic::nvvm_suld_2d_v2i32_clamp:\n  case Intrinsic::nvvm_suld_2d_v4i32_clamp:\n  case Intrinsic::nvvm_suld_2d_array_i32_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v2i32_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v4i32_clamp:\n  case Intrinsic::nvvm_suld_3d_i32_clamp:\n  case Intrinsic::nvvm_suld_3d_v2i32_clamp:\n  case Intrinsic::nvvm_suld_3d_v4i32_clamp:\n  case Intrinsic::nvvm_suld_1d_i32_trap:\n  case Intrinsic::nvvm_suld_1d_v2i32_trap:\n  case Intrinsic::nvvm_suld_1d_v4i32_trap:\n  case Intrinsic::nvvm_suld_1d_array_i32_trap:\n  case Intrinsic::nvvm_suld_1d_array_v2i32_trap:\n  case Intrinsic::nvvm_suld_1d_array_v4i32_trap:\n  case Intrinsic::nvvm_suld_2d_i32_trap:\n  case Intrinsic::nvvm_suld_2d_v2i32_trap:\n  case Intrinsic::nvvm_suld_2d_v4i32_trap:\n  case Intrinsic::nvvm_suld_2d_array_i32_trap:\n  case Intrinsic::nvvm_suld_2d_array_v2i32_trap:\n  case Intrinsic::nvvm_suld_2d_array_v4i32_trap:\n  case Intrinsic::nvvm_suld_3d_i32_trap:\n  case Intrinsic::nvvm_suld_3d_v2i32_trap:\n  case Intrinsic::nvvm_suld_3d_v4i32_trap:\n  case Intrinsic::nvvm_suld_1d_i32_zero:\n  case Intrinsic::nvvm_suld_1d_v2i32_zero:\n  case Intrinsic::nvvm_suld_1d_v4i32_zero:\n  case Intrinsic::nvvm_suld_1d_array_i32_zero:\n  case Intrinsic::nvvm_suld_1d_array_v2i32_zero:\n  case Intrinsic::nvvm_suld_1d_array_v4i32_zero:\n  case Intrinsic::nvvm_suld_2d_i32_zero:\n  case Intrinsic::nvvm_suld_2d_v2i32_zero:\n  case Intrinsic::nvvm_suld_2d_v4i32_zero:\n  case Intrinsic::nvvm_suld_2d_array_i32_zero:\n  case Intrinsic::nvvm_suld_2d_array_v2i32_zero:\n  case Intrinsic::nvvm_suld_2d_array_v4i32_zero:\n  case Intrinsic::nvvm_suld_3d_i32_zero:\n  case Intrinsic::nvvm_suld_3d_v2i32_zero:\n  case Intrinsic::nvvm_suld_3d_v4i32_zero: {\n    Info.opc = getOpcForSurfaceInstr(Intrinsic);\n    Info.memVT = MVT::i32;\n    Info.ptrVal = nullptr;\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = 16;\n    return true;\n  }\n  case Intrinsic::nvvm_suld_1d_i64_clamp:\n  case Intrinsic::nvvm_suld_1d_v2i64_clamp:\n  case Intrinsic::nvvm_suld_1d_array_i64_clamp:\n  case Intrinsic::nvvm_suld_1d_array_v2i64_clamp:\n  case Intrinsic::nvvm_suld_2d_i64_clamp:\n  case Intrinsic::nvvm_suld_2d_v2i64_clamp:\n  case Intrinsic::nvvm_suld_2d_array_i64_clamp:\n  case Intrinsic::nvvm_suld_2d_array_v2i64_clamp:\n  case Intrinsic::nvvm_suld_3d_i64_clamp:\n  case Intrinsic::nvvm_suld_3d_v2i64_clamp:\n  case Intrinsic::nvvm_suld_1d_i64_trap:\n  case Intrinsic::nvvm_suld_1d_v2i64_trap:\n  case Intrinsic::nvvm_suld_1d_array_i64_trap:\n  case Intrinsic::nvvm_suld_1d_array_v2i64_trap:\n  case Intrinsic::nvvm_suld_2d_i64_trap:\n  case Intrinsic::nvvm_suld_2d_v2i64_trap:\n  case Intrinsic::nvvm_suld_2d_array_i64_trap:\n  case Intrinsic::nvvm_suld_2d_array_v2i64_trap:\n  case Intrinsic::nvvm_suld_3d_i64_trap:\n  case Intrinsic::nvvm_suld_3d_v2i64_trap:\n  case Intrinsic::nvvm_suld_1d_i64_zero:\n  case Intrinsic::nvvm_suld_1d_v2i64_zero:\n  case Intrinsic::nvvm_suld_1d_array_i64_zero:\n  case Intrinsic::nvvm_suld_1d_array_v2i64_zero:\n  case Intrinsic::nvvm_suld_2d_i64_zero:\n  case Intrinsic::nvvm_suld_2d_v2i64_zero:\n  case Intrinsic::nvvm_suld_2d_array_i64_zero:\n  case Intrinsic::nvvm_suld_2d_array_v2i64_zero:\n  case Intrinsic::nvvm_suld_3d_i64_zero:\n  case Intrinsic::nvvm_suld_3d_v2i64_zero: {\n    Info.opc = getOpcForSurfaceInstr(Intrinsic);\n    Info.memVT = MVT::i64;\n    Info.ptrVal = nullptr;\n    Info.offset = 0;\n    Info.vol = 0;\n    Info.readMem = true;\n    Info.writeMem = false;\n    Info.align = 16;\n    return true;\n  }\n  }\n  return false;\n}\n\n/// isLegalAddressingMode - Return true if the addressing mode represented\n/// by AM is legal for this target, for a load/store of the specified type.\n/// Used to guide target specific optimizations, like loop strength reduction\n/// (LoopStrengthReduce.cpp) and memory optimization for address mode\n/// (CodeGenPrepare.cpp)\n"},"PerformADDCombineWithOperands":{"range":[3834,3939],"code":"static SDValue PerformADDCombineWithOperands(SDNode *N, SDValue N0, SDValue N1,\n                                           TargetLowering::DAGCombinerInfo &DCI,\n                                             const NVPTXSubtarget &Subtarget,\n                                             CodeGenOpt::Level OptLevel) {\n  SelectionDAG  &DAG = DCI.DAG;\n  // Skip non-integer, non-scalar case\n  EVT VT=N0.getValueType();\n  if (VT.isVector())\n    return SDValue();\n\n  // fold (add (mul a, b), c) -> (mad a, b, c)\n  //\n  if (N0.getOpcode() == ISD::MUL) {\n    assert (VT.isInteger());\n    // For integer:\n    // Since integer multiply-add costs the same as integer multiply\n    // but is more costly than integer add, do the fusion only when\n    // the mul is only used in the add.\n    if (OptLevel==CodeGenOpt::None || VT != MVT::i32 ||\n        !N0.getNode()->hasOneUse())\n      return SDValue();\n\n    // Do the folding\n    return DAG.getNode(NVPTXISD::IMAD, SDLoc(N), VT,\n                       N0.getOperand(0), N0.getOperand(1), N1);\n  }\n  else if (N0.getOpcode() == ISD::FMUL) {\n    if (VT == MVT::f32 || VT == MVT::f64) {\n      const auto *TLI = static_cast<const NVPTXTargetLowering *>(\n          &DAG.getTargetLoweringInfo());\n      if (!TLI->allowFMA(DAG.getMachineFunction(), OptLevel))\n        return SDValue();\n\n      // For floating point:\n      // Do the fusion only when the mul has less than 5 uses and all\n      // are add.\n      // The heuristic is that if a use is not an add, then that use\n      // cannot be fused into fma, therefore mul is still needed anyway.\n      // If there are more than 4 uses, even if they are all add, fusing\n      // them will increase register pressue.\n      //\n      int numUses = 0;\n      int nonAddCount = 0;\n      for (SDNode::use_iterator UI = N0.getNode()->use_begin(),\n           UE = N0.getNode()->use_end();\n           UI != UE; ++UI) {\n        numUses++;\n        SDNode *User = *UI;\n        if (User->getOpcode() != ISD::FADD)\n          ++nonAddCount;\n      }\n      if (numUses >= 5)\n        return SDValue();\n      if (nonAddCount) {\n        int orderNo = N->getIROrder();\n        int orderNo2 = N0.getNode()->getIROrder();\n        // simple heuristics here for considering potential register\n        // pressure, the logics here is that the differnce are used\n        // to measure the distance between def and use, the longer distance\n        // more likely cause register pressure.\n        if (orderNo - orderNo2 < 500)\n          return SDValue();\n\n        // Now, check if at least one of the FMUL's operands is live beyond the node N,\n        // which guarantees that the FMA will not increase register pressure at node N.\n        bool opIsLive = false;\n        const SDNode *left = N0.getOperand(0).getNode();\n        const SDNode *right = N0.getOperand(1).getNode();\n\n        if (isa<ConstantSDNode>(left) || isa<ConstantSDNode>(right))\n          opIsLive = true;\n\n        if (!opIsLive)\n          for (SDNode::use_iterator UI = left->use_begin(), UE = left->use_end(); UI != UE; ++UI) {\n            SDNode *User = *UI;\n            int orderNo3 = User->getIROrder();\n            if (orderNo3 > orderNo) {\n              opIsLive = true;\n              break;\n            }\n          }\n\n        if (!opIsLive)\n          for (SDNode::use_iterator UI = right->use_begin(), UE = right->use_end(); UI != UE; ++UI) {\n            SDNode *User = *UI;\n            int orderNo3 = User->getIROrder();\n            if (orderNo3 > orderNo) {\n              opIsLive = true;\n              break;\n            }\n          }\n\n        if (!opIsLive)\n          return SDValue();\n      }\n\n      return DAG.getNode(ISD::FMA, SDLoc(N), VT,\n                         N0.getOperand(0), N0.getOperand(1), N1);\n    }\n  }\n\n  return SDValue();\n}\n\n/// PerformADDCombine - Target-specific dag combine xforms for ISD::ADD.\n///\n"},"PerformSELECTCombine":{"range":[4031,4100],"code":"static SDValue PerformSELECTCombine(SDNode *N,\n                                    TargetLowering::DAGCombinerInfo &DCI) {\n  // Currently this detects patterns for integer min and max and\n  // lowers them to PTX-specific intrinsics that enable hardware\n  // support.\n\n  const SDValue Cond = N->getOperand(0);\n  if (Cond.getOpcode() != ISD::SETCC) return SDValue();\n\n  const SDValue LHS = Cond.getOperand(0);\n  const SDValue RHS = Cond.getOperand(1);\n  const SDValue True = N->getOperand(1);\n  const SDValue False = N->getOperand(2);\n  if (!(LHS == True && RHS == False) && !(LHS == False && RHS == True))\n    return SDValue();\n\n  const EVT VT = N->getValueType(0);\n  if (VT != MVT::i32 && VT != MVT::i64) return SDValue();\n\n  const ISD::CondCode CC = cast<CondCodeSDNode>(Cond.getOperand(2))->get();\n  SDValue Larger;  // The larger of LHS and RHS when condition is true.\n  switch (CC) {\n    case ISD::SETULT:\n    case ISD::SETULE:\n    case ISD::SETLT:\n    case ISD::SETLE:\n      Larger = RHS;\n      break;\n\n    case ISD::SETGT:\n    case ISD::SETGE:\n    case ISD::SETUGT:\n    case ISD::SETUGE:\n      Larger = LHS;\n      break;\n\n    default:\n      return SDValue();\n  }\n  const bool IsMax = (Larger == True);\n  const bool IsSigned = ISD::isSignedIntSetCC(CC);\n\n  unsigned IntrinsicId;\n  if (VT == MVT::i32) {\n    if (IsSigned)\n      IntrinsicId = IsMax ? Intrinsic::nvvm_max_i : Intrinsic::nvvm_min_i;\n    else\n      IntrinsicId = IsMax ? Intrinsic::nvvm_max_ui : Intrinsic::nvvm_min_ui;\n  } else {\n    assert(VT == MVT::i64);\n    if (IsSigned)\n      IntrinsicId = IsMax ? Intrinsic::nvvm_max_ll : Intrinsic::nvvm_min_ll;\n    else\n      IntrinsicId = IsMax ? Intrinsic::nvvm_max_ull : Intrinsic::nvvm_min_ull;\n  }\n\n  SDLoc DL(N);\n  return DCI.DAG.getNode(ISD::INTRINSIC_WO_CHAIN, DL, VT,\n                         DCI.DAG.getConstant(IntrinsicId, DL, VT), LHS, RHS);\n}\n\nenum OperandSignedness {\n  Signed = 0,\n  Unsigned,\n  Unknown\n};\n\n/// IsMulWideOperandDemotable - Checks if the provided DAG node is an operand\n/// that can be demoted to \\p OptSize bits without loss of information. The\n/// signedness of the operand, if determinable, is placed in \\p S.\n"},"getArgumentAlignment":{"range":[1006,1053],"code":"NVPTXTargetLowering::getArgumentAlignment(SDValue Callee,\n                                          const ImmutableCallSite *CS,\n                                          Type *Ty,\n                                          unsigned Idx) const {\n  unsigned Align = 0;\n  const Value *DirectCallee = CS->getCalledFunction();\n\n  if (!DirectCallee) {\n    // We don't have a direct function symbol, but that may be because of\n    // constant cast instructions in the call.\n    const Instruction *CalleeI = CS->getInstruction();\n    assert(CalleeI && \"Call target is not a function or derived value?\");\n\n    // With bitcast'd call targets, the instruction will be the call\n    if (isa<CallInst>(CalleeI)) {\n      // Check if we have call alignment metadata\n      if (llvm::getAlign(*cast<CallInst>(CalleeI), Idx, Align))\n        return Align;\n\n      const Value *CalleeV = cast<CallInst>(CalleeI)->getCalledValue();\n      // Ignore any bitcast instructions\n      while(isa<ConstantExpr>(CalleeV)) {\n        const ConstantExpr *CE = cast<ConstantExpr>(CalleeV);\n        if (!CE->isCast())\n          break;\n        // Look through the bitcast\n        CalleeV = cast<ConstantExpr>(CalleeV)->getOperand(0);\n      }\n\n      // We have now looked past all of the bitcasts.  Do we finally have a\n      // Function?\n      if (isa<Function>(CalleeV))\n        DirectCallee = CalleeV;\n    }\n  }\n\n  // Check for function alignment information if we found that the\n  // ultimate target is a Function\n  if (DirectCallee)\n    if (llvm::getAlign(*cast<Function>(DirectCallee), Idx, Align))\n      return Align;\n\n  // Call is indirect or alignment information is not available, fall back to\n  // the ABI type alignment\n  auto &DL = CS->getCaller()->getParent()->getDataLayout();\n  return DL.getABITypeAlignment(Ty);\n}\n\n"},"LowerReturn":{"range":[2360,2531],"code":"NVPTXTargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,\n                                 bool isVarArg,\n                                 const SmallVectorImpl<ISD::OutputArg> &Outs,\n                                 const SmallVectorImpl<SDValue> &OutVals,\n                                 const SDLoc &dl, SelectionDAG &DAG) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  const Function *F = MF.getFunction();\n  Type *RetTy = F->getReturnType();\n  const DataLayout &TD = DAG.getDataLayout();\n\n  bool isABI = (STI.getSmVersion() >= 20);\n  assert(isABI && \"Non-ABI compilation is not supported\");\n  if (!isABI)\n    return Chain;\n\n  if (VectorType *VTy = dyn_cast<VectorType>(RetTy)) {\n    // If we have a vector type, the OutVals array will be the scalarized\n    // components and we have combine them into 1 or more vector stores.\n    unsigned NumElts = VTy->getNumElements();\n    assert(NumElts == Outs.size() && \"Bad scalarization of return value\");\n\n    // const_cast can be removed in later LLVM versions\n    EVT EltVT = getValueType(TD, RetTy).getVectorElementType();\n    bool NeedExtend = false;\n    if (EltVT.getSizeInBits() < 16)\n      NeedExtend = true;\n\n    // V1 store\n    if (NumElts == 1) {\n      SDValue StoreVal = OutVals[0];\n      // We only have one element, so just directly store it\n      if (NeedExtend)\n        StoreVal = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal);\n      SDValue Ops[] = { Chain, DAG.getConstant(0, dl, MVT::i32), StoreVal };\n      Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreRetval, dl,\n                                      DAG.getVTList(MVT::Other), Ops,\n                                      EltVT, MachinePointerInfo());\n\n    } else if (NumElts == 2) {\n      // V2 store\n      SDValue StoreVal0 = OutVals[0];\n      SDValue StoreVal1 = OutVals[1];\n\n      if (NeedExtend) {\n        StoreVal0 = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal0);\n        StoreVal1 = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i16, StoreVal1);\n      }\n\n      SDValue Ops[] = { Chain, DAG.getConstant(0, dl, MVT::i32), StoreVal0,\n                        StoreVal1 };\n      Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreRetvalV2, dl,\n                                      DAG.getVTList(MVT::Other), Ops,\n                                      EltVT, MachinePointerInfo());\n    } else {\n      // V4 stores\n      // We have at least 4 elements (<3 x Ty> expands to 4 elements) and the\n      // vector will be expanded to a power of 2 elements, so we know we can\n      // always round up to the next multiple of 4 when creating the vector\n      // stores.\n      // e.g.  4 elem => 1 st.v4\n      //       6 elem => 2 st.v4\n      //       8 elem => 2 st.v4\n      //      11 elem => 3 st.v4\n\n      unsigned VecSize = 4;\n      if (OutVals[0].getValueType().getSizeInBits() == 64)\n        VecSize = 2;\n\n      unsigned Offset = 0;\n\n      EVT VecVT =\n          EVT::getVectorVT(F->getContext(), EltVT, VecSize);\n      unsigned PerStoreOffset =\n          TD.getTypeAllocSize(VecVT.getTypeForEVT(F->getContext()));\n\n      for (unsigned i = 0; i < NumElts; i += VecSize) {\n        // Get values\n        SDValue StoreVal;\n        SmallVector<SDValue, 8> Ops;\n        Ops.push_back(Chain);\n        Ops.push_back(DAG.getConstant(Offset, dl, MVT::i32));\n        unsigned Opc = NVPTXISD::StoreRetvalV2;\n        EVT ExtendedVT = (NeedExtend) ? MVT::i16 : OutVals[0].getValueType();\n\n        StoreVal = OutVals[i];\n        if (NeedExtend)\n          StoreVal = DAG.getNode(ISD::ZERO_EXTEND, dl, ExtendedVT, StoreVal);\n        Ops.push_back(StoreVal);\n\n        if (i + 1 < NumElts) {\n          StoreVal = OutVals[i + 1];\n          if (NeedExtend)\n            StoreVal = DAG.getNode(ISD::ZERO_EXTEND, dl, ExtendedVT, StoreVal);\n        } else {\n          StoreVal = DAG.getUNDEF(ExtendedVT);\n        }\n        Ops.push_back(StoreVal);\n\n        if (VecSize == 4) {\n          Opc = NVPTXISD::StoreRetvalV4;\n          if (i + 2 < NumElts) {\n            StoreVal = OutVals[i + 2];\n            if (NeedExtend)\n              StoreVal =\n                  DAG.getNode(ISD::ZERO_EXTEND, dl, ExtendedVT, StoreVal);\n          } else {\n            StoreVal = DAG.getUNDEF(ExtendedVT);\n          }\n          Ops.push_back(StoreVal);\n\n          if (i + 3 < NumElts) {\n            StoreVal = OutVals[i + 3];\n            if (NeedExtend)\n              StoreVal =\n                  DAG.getNode(ISD::ZERO_EXTEND, dl, ExtendedVT, StoreVal);\n          } else {\n            StoreVal = DAG.getUNDEF(ExtendedVT);\n          }\n          Ops.push_back(StoreVal);\n        }\n\n        // Chain = DAG.getNode(Opc, dl, MVT::Other, &Ops[0], Ops.size());\n        Chain =\n            DAG.getMemIntrinsicNode(Opc, dl, DAG.getVTList(MVT::Other), Ops,\n                                    EltVT, MachinePointerInfo());\n        Offset += PerStoreOffset;\n      }\n    }\n  } else {\n    SmallVector<EVT, 16> ValVTs;\n    SmallVector<uint64_t, 16> Offsets;\n    ComputePTXValueVTs(*this, DAG.getDataLayout(), RetTy, ValVTs, &Offsets, 0);\n    assert(ValVTs.size() == OutVals.size() && \"Bad return value decomposition\");\n\n    for (unsigned i = 0, e = Outs.size(); i != e; ++i) {\n      SDValue theVal = OutVals[i];\n      EVT TheValType = theVal.getValueType();\n      unsigned numElems = 1;\n      if (TheValType.isVector())\n        numElems = TheValType.getVectorNumElements();\n      for (unsigned j = 0, je = numElems; j != je; ++j) {\n        SDValue TmpVal = theVal;\n        if (TheValType.isVector())\n          TmpVal = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl,\n                               TheValType.getVectorElementType(), TmpVal,\n                               DAG.getIntPtrConstant(j, dl));\n        EVT TheStoreType = ValVTs[i];\n        if (RetTy->isIntegerTy() && TD.getTypeAllocSizeInBits(RetTy) < 32) {\n          // The following zero-extension is for integer types only, and\n          // specifically not for aggregates.\n          TmpVal = DAG.getNode(ISD::ZERO_EXTEND, dl, MVT::i32, TmpVal);\n          TheStoreType = MVT::i32;\n        }\n        else if (TmpVal.getValueType().getSizeInBits() < 16)\n          TmpVal = DAG.getNode(ISD::ANY_EXTEND, dl, MVT::i16, TmpVal);\n\n        SDValue Ops[] = {\n          Chain,\n          DAG.getConstant(Offsets[i], dl, MVT::i32),\n          TmpVal };\n        Chain = DAG.getMemIntrinsicNode(NVPTXISD::StoreRetval, dl,\n                                        DAG.getVTList(MVT::Other), Ops,\n                                        TheStoreType,\n                                        MachinePointerInfo());\n      }\n    }\n  }\n\n  return DAG.getNode(NVPTXISD::RET_FLAG, dl, MVT::Other, Chain);\n}\n\n\n"},"PerformADDCombine":{"range":[3940,3955],"code":"static SDValue PerformADDCombine(SDNode *N,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 const NVPTXSubtarget &Subtarget,\n                                 CodeGenOpt::Level OptLevel) {\n  SDValue N0 = N->getOperand(0);\n  SDValue N1 = N->getOperand(1);\n\n  // First try with the default operand order.\n  if (SDValue Result =\n          PerformADDCombineWithOperands(N, N0, N1, DCI, Subtarget, OptLevel))\n    return Result;\n\n  // If that didn't work, try again with the operands commuted.\n  return PerformADDCombineWithOperands(N, N1, N0, DCI, Subtarget, OptLevel);\n}\n\n"},"getRegForInlineAsmConstraint":{"range":[3775,3803],"code":"NVPTXTargetLowering::getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,\n                                                  StringRef Constraint,\n                                                  MVT VT) const {\n  if (Constraint.size() == 1) {\n    switch (Constraint[0]) {\n    case 'b':\n      return std::make_pair(0U, &NVPTX::Int1RegsRegClass);\n    case 'c':\n      return std::make_pair(0U, &NVPTX::Int16RegsRegClass);\n    case 'h':\n      return std::make_pair(0U, &NVPTX::Int16RegsRegClass);\n    case 'r':\n      return std::make_pair(0U, &NVPTX::Int32RegsRegClass);\n    case 'l':\n    case 'N':\n      return std::make_pair(0U, &NVPTX::Int64RegsRegClass);\n    case 'f':\n      return std::make_pair(0U, &NVPTX::Float32RegsRegClass);\n    case 'd':\n      return std::make_pair(0U, &NVPTX::Float64RegsRegClass);\n    }\n  }\n  return TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);\n}\n\n//===----------------------------------------------------------------------===//\n//                         NVPTX DAG Combining\n//===----------------------------------------------------------------------===//\n\n"},"anchor":{"range":[4532,4557],"code":"void NVPTXSection::anchor() {}\n\nNVPTXTargetObjectFile::~NVPTXTargetObjectFile() {\n  delete static_cast<NVPTXSection *>(TextSection);\n  delete static_cast<NVPTXSection *>(DataSection);\n  delete static_cast<NVPTXSection *>(BSSSection);\n  delete static_cast<NVPTXSection *>(ReadOnlySection);\n\n  delete static_cast<NVPTXSection *>(StaticCtorSection);\n  delete static_cast<NVPTXSection *>(StaticDtorSection);\n  delete static_cast<NVPTXSection *>(LSDASection);\n  delete static_cast<NVPTXSection *>(EHFrameSection);\n  delete static_cast<NVPTXSection *>(DwarfAbbrevSection);\n  delete static_cast<NVPTXSection *>(DwarfInfoSection);\n  delete static_cast<NVPTXSection *>(DwarfLineSection);\n  delete static_cast<NVPTXSection *>(DwarfFrameSection);\n  delete static_cast<NVPTXSection *>(DwarfPubTypesSection);\n  delete static_cast<const NVPTXSection *>(DwarfDebugInlineSection);\n  delete static_cast<NVPTXSection *>(DwarfStrSection);\n  delete static_cast<NVPTXSection *>(DwarfLocSection);\n  delete static_cast<NVPTXSection *>(DwarfARangesSection);\n  delete static_cast<NVPTXSection *>(DwarfRangesSection);\n  delete static_cast<NVPTXSection *>(DwarfMacinfoSection);\n}\n\nMCSection *\n"},"PerformSHLCombine":{"range":[4245,4256],"code":"static SDValue PerformSHLCombine(SDNode *N,\n                                 TargetLowering::DAGCombinerInfo &DCI,\n                                 CodeGenOpt::Level OptLevel) {\n  if (OptLevel > 0) {\n    // Try mul.wide combining at OptLevel > 0\n    if (SDValue Ret = TryMULWIDECombine(N, DCI))\n      return Ret;\n  }\n\n  return SDValue();\n}\n\n"},"IsPTXVectorType":{"range":[57,82],"code":"static bool IsPTXVectorType(MVT VT) {\n  switch (VT.SimpleTy) {\n  default:\n    return false;\n  case MVT::v2i1:\n  case MVT::v4i1:\n  case MVT::v2i8:\n  case MVT::v4i8:\n  case MVT::v2i16:\n  case MVT::v4i16:\n  case MVT::v2i32:\n  case MVT::v4i32:\n  case MVT::v2i64:\n  case MVT::v2f32:\n  case MVT::v4f32:\n  case MVT::v2f64:\n    return true;\n  }\n}\n\n/// ComputePTXValueVTs - For the given Type \\p Ty, returns the set of primitive\n/// EVTs that compose it.  Unlike ComputeValueVTs, this will break apart vectors\n/// into their primitive components.\n/// NOTE: This is a band-aid for code that expects ComputeValueVTs to return the\n/// same number of types as the Ins/Outs arrays in LowerFormalArguments,\n/// LowerCall, and LowerReturn.\n"},"ReplaceNodeResults":{"range":[4517,4531],"code":"void NVPTXTargetLowering::ReplaceNodeResults(\n    SDNode *N, SmallVectorImpl<SDValue> &Results, SelectionDAG &DAG) const {\n  switch (N->getOpcode()) {\n  default:\n    report_fatal_error(\"Unhandled custom legalization\");\n  case ISD::LOAD:\n    ReplaceLoadVector(N, DAG, Results);\n    return;\n  case ISD::INTRINSIC_W_CHAIN:\n    ReplaceINTRINSIC_W_CHAIN(N, DAG, Results);\n    return;\n  }\n}\n\n// Pin NVPTXSection's and NVPTXTargetObjectFile's vtables to this file.\n"},"LowerSelect":{"range":[1854,1869],"code":"SDValue NVPTXTargetLowering::LowerSelect(SDValue Op, SelectionDAG &DAG) const {\n  SDValue Op0 = Op->getOperand(0);\n  SDValue Op1 = Op->getOperand(1);\n  SDValue Op2 = Op->getOperand(2);\n  SDLoc DL(Op.getNode());\n\n  assert(Op.getValueType() == MVT::i1 && \"Custom lowering enabled only for i1\");\n\n  Op1 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i32, Op1);\n  Op2 = DAG.getNode(ISD::ANY_EXTEND, DL, MVT::i32, Op2);\n  SDValue Select = DAG.getNode(ISD::SELECT, DL, MVT::i32, Op0, Op1, Op2);\n  SDValue Trunc = DAG.getNode(ISD::TRUNCATE, DL, MVT::i1, Select);\n\n  return Trunc;\n}\n\n"},"IsMulWideOperandDemotable":{"range":[4101,4127],"code":"static bool IsMulWideOperandDemotable(SDValue Op,\n                                      unsigned OptSize,\n                                      OperandSignedness &S) {\n  S = Unknown;\n\n  if (Op.getOpcode() == ISD::SIGN_EXTEND ||\n      Op.getOpcode() == ISD::SIGN_EXTEND_INREG) {\n    EVT OrigVT = Op.getOperand(0).getValueType();\n    if (OrigVT.getSizeInBits() <= OptSize) {\n      S = Signed;\n      return true;\n    }\n  } else if (Op.getOpcode() == ISD::ZERO_EXTEND) {\n    EVT OrigVT = Op.getOperand(0).getValueType();\n    if (OrigVT.getSizeInBits() <= OptSize) {\n      S = Unsigned;\n      return true;\n    }\n  }\n\n  return false;\n}\n\n/// AreMulWideOperandsDemotable - Checks if the given LHS and RHS operands can\n/// be demoted to \\p OptSize bits without loss of information. If the operands\n/// contain a constant, it should appear as the RHS operand. The signedness of\n/// the operands is placed in \\p IsSigned.\n"},"getOpcForTextureInstr":{"range":[2541,2899],"code":"static unsigned getOpcForTextureInstr(unsigned Intrinsic) {\n  switch (Intrinsic) {\n  default:\n    return 0;\n\n  case Intrinsic::nvvm_tex_1d_v4f32_s32:\n    return NVPTXISD::Tex1DFloatS32;\n  case Intrinsic::nvvm_tex_1d_v4f32_f32:\n    return NVPTXISD::Tex1DFloatFloat;\n  case Intrinsic::nvvm_tex_1d_level_v4f32_f32:\n    return NVPTXISD::Tex1DFloatFloatLevel;\n  case Intrinsic::nvvm_tex_1d_grad_v4f32_f32:\n    return NVPTXISD::Tex1DFloatFloatGrad;\n  case Intrinsic::nvvm_tex_1d_v4s32_s32:\n    return NVPTXISD::Tex1DS32S32;\n  case Intrinsic::nvvm_tex_1d_v4s32_f32:\n    return NVPTXISD::Tex1DS32Float;\n  case Intrinsic::nvvm_tex_1d_level_v4s32_f32:\n    return NVPTXISD::Tex1DS32FloatLevel;\n  case Intrinsic::nvvm_tex_1d_grad_v4s32_f32:\n    return NVPTXISD::Tex1DS32FloatGrad;\n  case Intrinsic::nvvm_tex_1d_v4u32_s32:\n    return NVPTXISD::Tex1DU32S32;\n  case Intrinsic::nvvm_tex_1d_v4u32_f32:\n    return NVPTXISD::Tex1DU32Float;\n  case Intrinsic::nvvm_tex_1d_level_v4u32_f32:\n    return NVPTXISD::Tex1DU32FloatLevel;\n  case Intrinsic::nvvm_tex_1d_grad_v4u32_f32:\n    return NVPTXISD::Tex1DU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_1d_array_v4f32_s32:\n    return NVPTXISD::Tex1DArrayFloatS32;\n  case Intrinsic::nvvm_tex_1d_array_v4f32_f32:\n    return NVPTXISD::Tex1DArrayFloatFloat;\n  case Intrinsic::nvvm_tex_1d_array_level_v4f32_f32:\n    return NVPTXISD::Tex1DArrayFloatFloatLevel;\n  case Intrinsic::nvvm_tex_1d_array_grad_v4f32_f32:\n    return NVPTXISD::Tex1DArrayFloatFloatGrad;\n  case Intrinsic::nvvm_tex_1d_array_v4s32_s32:\n    return NVPTXISD::Tex1DArrayS32S32;\n  case Intrinsic::nvvm_tex_1d_array_v4s32_f32:\n    return NVPTXISD::Tex1DArrayS32Float;\n  case Intrinsic::nvvm_tex_1d_array_level_v4s32_f32:\n    return NVPTXISD::Tex1DArrayS32FloatLevel;\n  case Intrinsic::nvvm_tex_1d_array_grad_v4s32_f32:\n    return NVPTXISD::Tex1DArrayS32FloatGrad;\n  case Intrinsic::nvvm_tex_1d_array_v4u32_s32:\n    return NVPTXISD::Tex1DArrayU32S32;\n  case Intrinsic::nvvm_tex_1d_array_v4u32_f32:\n    return NVPTXISD::Tex1DArrayU32Float;\n  case Intrinsic::nvvm_tex_1d_array_level_v4u32_f32:\n    return NVPTXISD::Tex1DArrayU32FloatLevel;\n  case Intrinsic::nvvm_tex_1d_array_grad_v4u32_f32:\n    return NVPTXISD::Tex1DArrayU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_2d_v4f32_s32:\n    return NVPTXISD::Tex2DFloatS32;\n  case Intrinsic::nvvm_tex_2d_v4f32_f32:\n    return NVPTXISD::Tex2DFloatFloat;\n  case Intrinsic::nvvm_tex_2d_level_v4f32_f32:\n    return NVPTXISD::Tex2DFloatFloatLevel;\n  case Intrinsic::nvvm_tex_2d_grad_v4f32_f32:\n    return NVPTXISD::Tex2DFloatFloatGrad;\n  case Intrinsic::nvvm_tex_2d_v4s32_s32:\n    return NVPTXISD::Tex2DS32S32;\n  case Intrinsic::nvvm_tex_2d_v4s32_f32:\n    return NVPTXISD::Tex2DS32Float;\n  case Intrinsic::nvvm_tex_2d_level_v4s32_f32:\n    return NVPTXISD::Tex2DS32FloatLevel;\n  case Intrinsic::nvvm_tex_2d_grad_v4s32_f32:\n    return NVPTXISD::Tex2DS32FloatGrad;\n  case Intrinsic::nvvm_tex_2d_v4u32_s32:\n    return NVPTXISD::Tex2DU32S32;\n  case Intrinsic::nvvm_tex_2d_v4u32_f32:\n    return NVPTXISD::Tex2DU32Float;\n  case Intrinsic::nvvm_tex_2d_level_v4u32_f32:\n    return NVPTXISD::Tex2DU32FloatLevel;\n  case Intrinsic::nvvm_tex_2d_grad_v4u32_f32:\n    return NVPTXISD::Tex2DU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_2d_array_v4f32_s32:\n    return NVPTXISD::Tex2DArrayFloatS32;\n  case Intrinsic::nvvm_tex_2d_array_v4f32_f32:\n    return NVPTXISD::Tex2DArrayFloatFloat;\n  case Intrinsic::nvvm_tex_2d_array_level_v4f32_f32:\n    return NVPTXISD::Tex2DArrayFloatFloatLevel;\n  case Intrinsic::nvvm_tex_2d_array_grad_v4f32_f32:\n    return NVPTXISD::Tex2DArrayFloatFloatGrad;\n  case Intrinsic::nvvm_tex_2d_array_v4s32_s32:\n    return NVPTXISD::Tex2DArrayS32S32;\n  case Intrinsic::nvvm_tex_2d_array_v4s32_f32:\n    return NVPTXISD::Tex2DArrayS32Float;\n  case Intrinsic::nvvm_tex_2d_array_level_v4s32_f32:\n    return NVPTXISD::Tex2DArrayS32FloatLevel;\n  case Intrinsic::nvvm_tex_2d_array_grad_v4s32_f32:\n    return NVPTXISD::Tex2DArrayS32FloatGrad;\n  case Intrinsic::nvvm_tex_2d_array_v4u32_s32:\n    return NVPTXISD::Tex2DArrayU32S32;\n  case Intrinsic::nvvm_tex_2d_array_v4u32_f32:\n    return NVPTXISD::Tex2DArrayU32Float;\n  case Intrinsic::nvvm_tex_2d_array_level_v4u32_f32:\n    return NVPTXISD::Tex2DArrayU32FloatLevel;\n  case Intrinsic::nvvm_tex_2d_array_grad_v4u32_f32:\n    return NVPTXISD::Tex2DArrayU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_3d_v4f32_s32:\n    return NVPTXISD::Tex3DFloatS32;\n  case Intrinsic::nvvm_tex_3d_v4f32_f32:\n    return NVPTXISD::Tex3DFloatFloat;\n  case Intrinsic::nvvm_tex_3d_level_v4f32_f32:\n    return NVPTXISD::Tex3DFloatFloatLevel;\n  case Intrinsic::nvvm_tex_3d_grad_v4f32_f32:\n    return NVPTXISD::Tex3DFloatFloatGrad;\n  case Intrinsic::nvvm_tex_3d_v4s32_s32:\n    return NVPTXISD::Tex3DS32S32;\n  case Intrinsic::nvvm_tex_3d_v4s32_f32:\n    return NVPTXISD::Tex3DS32Float;\n  case Intrinsic::nvvm_tex_3d_level_v4s32_f32:\n    return NVPTXISD::Tex3DS32FloatLevel;\n  case Intrinsic::nvvm_tex_3d_grad_v4s32_f32:\n    return NVPTXISD::Tex3DS32FloatGrad;\n  case Intrinsic::nvvm_tex_3d_v4u32_s32:\n    return NVPTXISD::Tex3DU32S32;\n  case Intrinsic::nvvm_tex_3d_v4u32_f32:\n    return NVPTXISD::Tex3DU32Float;\n  case Intrinsic::nvvm_tex_3d_level_v4u32_f32:\n    return NVPTXISD::Tex3DU32FloatLevel;\n  case Intrinsic::nvvm_tex_3d_grad_v4u32_f32:\n    return NVPTXISD::Tex3DU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_cube_v4f32_f32:\n    return NVPTXISD::TexCubeFloatFloat;\n  case Intrinsic::nvvm_tex_cube_level_v4f32_f32:\n    return NVPTXISD::TexCubeFloatFloatLevel;\n  case Intrinsic::nvvm_tex_cube_v4s32_f32:\n    return NVPTXISD::TexCubeS32Float;\n  case Intrinsic::nvvm_tex_cube_level_v4s32_f32:\n    return NVPTXISD::TexCubeS32FloatLevel;\n  case Intrinsic::nvvm_tex_cube_v4u32_f32:\n    return NVPTXISD::TexCubeU32Float;\n  case Intrinsic::nvvm_tex_cube_level_v4u32_f32:\n    return NVPTXISD::TexCubeU32FloatLevel;\n\n  case Intrinsic::nvvm_tex_cube_array_v4f32_f32:\n    return NVPTXISD::TexCubeArrayFloatFloat;\n  case Intrinsic::nvvm_tex_cube_array_level_v4f32_f32:\n    return NVPTXISD::TexCubeArrayFloatFloatLevel;\n  case Intrinsic::nvvm_tex_cube_array_v4s32_f32:\n    return NVPTXISD::TexCubeArrayS32Float;\n  case Intrinsic::nvvm_tex_cube_array_level_v4s32_f32:\n    return NVPTXISD::TexCubeArrayS32FloatLevel;\n  case Intrinsic::nvvm_tex_cube_array_v4u32_f32:\n    return NVPTXISD::TexCubeArrayU32Float;\n  case Intrinsic::nvvm_tex_cube_array_level_v4u32_f32:\n    return NVPTXISD::TexCubeArrayU32FloatLevel;\n\n  case Intrinsic::nvvm_tld4_r_2d_v4f32_f32:\n    return NVPTXISD::Tld4R2DFloatFloat;\n  case Intrinsic::nvvm_tld4_g_2d_v4f32_f32:\n    return NVPTXISD::Tld4G2DFloatFloat;\n  case Intrinsic::nvvm_tld4_b_2d_v4f32_f32:\n    return NVPTXISD::Tld4B2DFloatFloat;\n  case Intrinsic::nvvm_tld4_a_2d_v4f32_f32:\n    return NVPTXISD::Tld4A2DFloatFloat;\n  case Intrinsic::nvvm_tld4_r_2d_v4s32_f32:\n    return NVPTXISD::Tld4R2DS64Float;\n  case Intrinsic::nvvm_tld4_g_2d_v4s32_f32:\n    return NVPTXISD::Tld4G2DS64Float;\n  case Intrinsic::nvvm_tld4_b_2d_v4s32_f32:\n    return NVPTXISD::Tld4B2DS64Float;\n  case Intrinsic::nvvm_tld4_a_2d_v4s32_f32:\n    return NVPTXISD::Tld4A2DS64Float;\n  case Intrinsic::nvvm_tld4_r_2d_v4u32_f32:\n    return NVPTXISD::Tld4R2DU64Float;\n  case Intrinsic::nvvm_tld4_g_2d_v4u32_f32:\n    return NVPTXISD::Tld4G2DU64Float;\n  case Intrinsic::nvvm_tld4_b_2d_v4u32_f32:\n    return NVPTXISD::Tld4B2DU64Float;\n  case Intrinsic::nvvm_tld4_a_2d_v4u32_f32:\n    return NVPTXISD::Tld4A2DU64Float;\n\n  case Intrinsic::nvvm_tex_unified_1d_v4f32_s32:\n    return NVPTXISD::TexUnified1DFloatS32;\n  case Intrinsic::nvvm_tex_unified_1d_v4f32_f32:\n    return NVPTXISD::TexUnified1DFloatFloat;\n  case Intrinsic::nvvm_tex_unified_1d_level_v4f32_f32:\n    return NVPTXISD::TexUnified1DFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_1d_grad_v4f32_f32:\n    return NVPTXISD::TexUnified1DFloatFloatGrad;\n  case Intrinsic::nvvm_tex_unified_1d_v4s32_s32:\n    return NVPTXISD::TexUnified1DS32S32;\n  case Intrinsic::nvvm_tex_unified_1d_v4s32_f32:\n    return NVPTXISD::TexUnified1DS32Float;\n  case Intrinsic::nvvm_tex_unified_1d_level_v4s32_f32:\n    return NVPTXISD::TexUnified1DS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_1d_grad_v4s32_f32:\n    return NVPTXISD::TexUnified1DS32FloatGrad;\n  case Intrinsic::nvvm_tex_unified_1d_v4u32_s32:\n    return NVPTXISD::TexUnified1DU32S32;\n  case Intrinsic::nvvm_tex_unified_1d_v4u32_f32:\n    return NVPTXISD::TexUnified1DU32Float;\n  case Intrinsic::nvvm_tex_unified_1d_level_v4u32_f32:\n    return NVPTXISD::TexUnified1DU32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_1d_grad_v4u32_f32:\n    return NVPTXISD::TexUnified1DU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_unified_1d_array_v4f32_s32:\n    return NVPTXISD::TexUnified1DArrayFloatS32;\n  case Intrinsic::nvvm_tex_unified_1d_array_v4f32_f32:\n    return NVPTXISD::TexUnified1DArrayFloatFloat;\n  case Intrinsic::nvvm_tex_unified_1d_array_level_v4f32_f32:\n    return NVPTXISD::TexUnified1DArrayFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_1d_array_grad_v4f32_f32:\n    return NVPTXISD::TexUnified1DArrayFloatFloatGrad;\n  case Intrinsic::nvvm_tex_unified_1d_array_v4s32_s32:\n    return NVPTXISD::TexUnified1DArrayS32S32;\n  case Intrinsic::nvvm_tex_unified_1d_array_v4s32_f32:\n    return NVPTXISD::TexUnified1DArrayS32Float;\n  case Intrinsic::nvvm_tex_unified_1d_array_level_v4s32_f32:\n    return NVPTXISD::TexUnified1DArrayS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_1d_array_grad_v4s32_f32:\n    return NVPTXISD::TexUnified1DArrayS32FloatGrad;\n  case Intrinsic::nvvm_tex_unified_1d_array_v4u32_s32:\n    return NVPTXISD::TexUnified1DArrayU32S32;\n  case Intrinsic::nvvm_tex_unified_1d_array_v4u32_f32:\n    return NVPTXISD::TexUnified1DArrayU32Float;\n  case Intrinsic::nvvm_tex_unified_1d_array_level_v4u32_f32:\n    return NVPTXISD::TexUnified1DArrayU32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_1d_array_grad_v4u32_f32:\n    return NVPTXISD::TexUnified1DArrayU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_unified_2d_v4f32_s32:\n    return NVPTXISD::TexUnified2DFloatS32;\n  case Intrinsic::nvvm_tex_unified_2d_v4f32_f32:\n    return NVPTXISD::TexUnified2DFloatFloat;\n  case Intrinsic::nvvm_tex_unified_2d_level_v4f32_f32:\n    return NVPTXISD::TexUnified2DFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_2d_grad_v4f32_f32:\n    return NVPTXISD::TexUnified2DFloatFloatGrad;\n  case Intrinsic::nvvm_tex_unified_2d_v4s32_s32:\n    return NVPTXISD::TexUnified2DS32S32;\n  case Intrinsic::nvvm_tex_unified_2d_v4s32_f32:\n    return NVPTXISD::TexUnified2DS32Float;\n  case Intrinsic::nvvm_tex_unified_2d_level_v4s32_f32:\n    return NVPTXISD::TexUnified2DS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_2d_grad_v4s32_f32:\n    return NVPTXISD::TexUnified2DS32FloatGrad;\n  case Intrinsic::nvvm_tex_unified_2d_v4u32_s32:\n    return NVPTXISD::TexUnified2DU32S32;\n  case Intrinsic::nvvm_tex_unified_2d_v4u32_f32:\n    return NVPTXISD::TexUnified2DU32Float;\n  case Intrinsic::nvvm_tex_unified_2d_level_v4u32_f32:\n    return NVPTXISD::TexUnified2DU32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_2d_grad_v4u32_f32:\n    return NVPTXISD::TexUnified2DU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_unified_2d_array_v4f32_s32:\n    return NVPTXISD::TexUnified2DArrayFloatS32;\n  case Intrinsic::nvvm_tex_unified_2d_array_v4f32_f32:\n    return NVPTXISD::TexUnified2DArrayFloatFloat;\n  case Intrinsic::nvvm_tex_unified_2d_array_level_v4f32_f32:\n    return NVPTXISD::TexUnified2DArrayFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_2d_array_grad_v4f32_f32:\n    return NVPTXISD::TexUnified2DArrayFloatFloatGrad;\n  case Intrinsic::nvvm_tex_unified_2d_array_v4s32_s32:\n    return NVPTXISD::TexUnified2DArrayS32S32;\n  case Intrinsic::nvvm_tex_unified_2d_array_v4s32_f32:\n    return NVPTXISD::TexUnified2DArrayS32Float;\n  case Intrinsic::nvvm_tex_unified_2d_array_level_v4s32_f32:\n    return NVPTXISD::TexUnified2DArrayS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_2d_array_grad_v4s32_f32:\n    return NVPTXISD::TexUnified2DArrayS32FloatGrad;\n  case Intrinsic::nvvm_tex_unified_2d_array_v4u32_s32:\n    return NVPTXISD::TexUnified2DArrayU32S32;\n  case Intrinsic::nvvm_tex_unified_2d_array_v4u32_f32:\n    return NVPTXISD::TexUnified2DArrayU32Float;\n  case Intrinsic::nvvm_tex_unified_2d_array_level_v4u32_f32:\n    return NVPTXISD::TexUnified2DArrayU32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_2d_array_grad_v4u32_f32:\n    return NVPTXISD::TexUnified2DArrayU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_unified_3d_v4f32_s32:\n    return NVPTXISD::TexUnified3DFloatS32;\n  case Intrinsic::nvvm_tex_unified_3d_v4f32_f32:\n    return NVPTXISD::TexUnified3DFloatFloat;\n  case Intrinsic::nvvm_tex_unified_3d_level_v4f32_f32:\n    return NVPTXISD::TexUnified3DFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_3d_grad_v4f32_f32:\n    return NVPTXISD::TexUnified3DFloatFloatGrad;\n  case Intrinsic::nvvm_tex_unified_3d_v4s32_s32:\n    return NVPTXISD::TexUnified3DS32S32;\n  case Intrinsic::nvvm_tex_unified_3d_v4s32_f32:\n    return NVPTXISD::TexUnified3DS32Float;\n  case Intrinsic::nvvm_tex_unified_3d_level_v4s32_f32:\n    return NVPTXISD::TexUnified3DS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_3d_grad_v4s32_f32:\n    return NVPTXISD::TexUnified3DS32FloatGrad;\n  case Intrinsic::nvvm_tex_unified_3d_v4u32_s32:\n    return NVPTXISD::TexUnified3DU32S32;\n  case Intrinsic::nvvm_tex_unified_3d_v4u32_f32:\n    return NVPTXISD::TexUnified3DU32Float;\n  case Intrinsic::nvvm_tex_unified_3d_level_v4u32_f32:\n    return NVPTXISD::TexUnified3DU32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_3d_grad_v4u32_f32:\n    return NVPTXISD::TexUnified3DU32FloatGrad;\n\n  case Intrinsic::nvvm_tex_unified_cube_v4f32_f32:\n    return NVPTXISD::TexUnifiedCubeFloatFloat;\n  case Intrinsic::nvvm_tex_unified_cube_level_v4f32_f32:\n    return NVPTXISD::TexUnifiedCubeFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_cube_v4s32_f32:\n    return NVPTXISD::TexUnifiedCubeS32Float;\n  case Intrinsic::nvvm_tex_unified_cube_level_v4s32_f32:\n    return NVPTXISD::TexUnifiedCubeS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_cube_v4u32_f32:\n    return NVPTXISD::TexUnifiedCubeU32Float;\n  case Intrinsic::nvvm_tex_unified_cube_level_v4u32_f32:\n    return NVPTXISD::TexUnifiedCubeU32FloatLevel;\n\n  case Intrinsic::nvvm_tex_unified_cube_array_v4f32_f32:\n    return NVPTXISD::TexUnifiedCubeArrayFloatFloat;\n  case Intrinsic::nvvm_tex_unified_cube_array_level_v4f32_f32:\n    return NVPTXISD::TexUnifiedCubeArrayFloatFloatLevel;\n  case Intrinsic::nvvm_tex_unified_cube_array_v4s32_f32:\n    return NVPTXISD::TexUnifiedCubeArrayS32Float;\n  case Intrinsic::nvvm_tex_unified_cube_array_level_v4s32_f32:\n    return NVPTXISD::TexUnifiedCubeArrayS32FloatLevel;\n  case Intrinsic::nvvm_tex_unified_cube_array_v4u32_f32:\n    return NVPTXISD::TexUnifiedCubeArrayU32Float;\n  case Intrinsic::nvvm_tex_unified_cube_array_level_v4u32_f32:\n    return NVPTXISD::TexUnifiedCubeArrayU32FloatLevel;\n\n  case Intrinsic::nvvm_tld4_unified_r_2d_v4f32_f32:\n    return NVPTXISD::Tld4UnifiedR2DFloatFloat;\n  case Intrinsic::nvvm_tld4_unified_g_2d_v4f32_f32:\n    return NVPTXISD::Tld4UnifiedG2DFloatFloat;\n  case Intrinsic::nvvm_tld4_unified_b_2d_v4f32_f32:\n    return NVPTXISD::Tld4UnifiedB2DFloatFloat;\n  case Intrinsic::nvvm_tld4_unified_a_2d_v4f32_f32:\n    return NVPTXISD::Tld4UnifiedA2DFloatFloat;\n  case Intrinsic::nvvm_tld4_unified_r_2d_v4s32_f32:\n    return NVPTXISD::Tld4UnifiedR2DS64Float;\n  case Intrinsic::nvvm_tld4_unified_g_2d_v4s32_f32:\n    return NVPTXISD::Tld4UnifiedG2DS64Float;\n  case Intrinsic::nvvm_tld4_unified_b_2d_v4s32_f32:\n    return NVPTXISD::Tld4UnifiedB2DS64Float;\n  case Intrinsic::nvvm_tld4_unified_a_2d_v4s32_f32:\n    return NVPTXISD::Tld4UnifiedA2DS64Float;\n  case Intrinsic::nvvm_tld4_unified_r_2d_v4u32_f32:\n    return NVPTXISD::Tld4UnifiedR2DU64Float;\n  case Intrinsic::nvvm_tld4_unified_g_2d_v4u32_f32:\n    return NVPTXISD::Tld4UnifiedG2DU64Float;\n  case Intrinsic::nvvm_tld4_unified_b_2d_v4u32_f32:\n    return NVPTXISD::Tld4UnifiedB2DU64Float;\n  case Intrinsic::nvvm_tld4_unified_a_2d_v4u32_f32:\n    return NVPTXISD::Tld4UnifiedA2DU64Float;\n  }\n}\n\n"},"LowerFormalArguments":{"range":[2061,2359],"code":"SDValue NVPTXTargetLowering::LowerFormalArguments(\n    SDValue Chain, CallingConv::ID CallConv, bool isVarArg,\n    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &dl,\n    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals) const {\n  MachineFunction &MF = DAG.getMachineFunction();\n  const DataLayout &DL = DAG.getDataLayout();\n  auto PtrVT = getPointerTy(DAG.getDataLayout());\n\n  const Function *F = MF.getFunction();\n  const AttributeSet &PAL = F->getAttributes();\n  const TargetLowering *TLI = STI.getTargetLowering();\n\n  SDValue Root = DAG.getRoot();\n  std::vector<SDValue> OutChains;\n\n  bool isKernel = llvm::isKernelFunction(*F);\n  bool isABI = (STI.getSmVersion() >= 20);\n  assert(isABI && \"Non-ABI compilation is not supported\");\n  if (!isABI)\n    return Chain;\n\n  std::vector<Type *> argTypes;\n  std::vector<const Argument *> theArgs;\n  for (const Argument &I : F->args()) {\n    theArgs.push_back(&I);\n    argTypes.push_back(I.getType());\n  }\n  // argTypes.size() (or theArgs.size()) and Ins.size() need not match.\n  // Ins.size() will be larger\n  //   * if there is an aggregate argument with multiple fields (each field\n  //     showing up separately in Ins)\n  //   * if there is a vector argument with more than typical vector-length\n  //     elements (generally if more than 4) where each vector element is\n  //     individually present in Ins.\n  // So a different index should be used for indexing into Ins.\n  // See similar issue in LowerCall.\n  unsigned InsIdx = 0;\n\n  int idx = 0;\n  for (unsigned i = 0, e = theArgs.size(); i != e; ++i, ++idx, ++InsIdx) {\n    Type *Ty = argTypes[i];\n\n    // If the kernel argument is image*_t or sampler_t, convert it to\n    // a i32 constant holding the parameter position. This can later\n    // matched in the AsmPrinter to output the correct mangled name.\n    if (isImageOrSamplerVal(\n            theArgs[i],\n            (theArgs[i]->getParent() ? theArgs[i]->getParent()->getParent()\n                                     : nullptr))) {\n      assert(isKernel && \"Only kernels can have image/sampler params\");\n      InVals.push_back(DAG.getConstant(i + 1, dl, MVT::i32));\n      continue;\n    }\n\n    if (theArgs[i]->use_empty()) {\n      // argument is dead\n      if (Ty->isAggregateType()) {\n        SmallVector<EVT, 16> vtparts;\n\n        ComputePTXValueVTs(*this, DAG.getDataLayout(), Ty, vtparts);\n        assert(vtparts.size() > 0 && \"empty aggregate type not expected\");\n        for (unsigned parti = 0, parte = vtparts.size(); parti != parte;\n             ++parti) {\n          InVals.push_back(DAG.getNode(ISD::UNDEF, dl, Ins[InsIdx].VT));\n          ++InsIdx;\n        }\n        if (vtparts.size() > 0)\n          --InsIdx;\n        continue;\n      }\n      if (Ty->isVectorTy()) {\n        EVT ObjectVT = getValueType(DL, Ty);\n        unsigned NumRegs = TLI->getNumRegisters(F->getContext(), ObjectVT);\n        for (unsigned parti = 0; parti < NumRegs; ++parti) {\n          InVals.push_back(DAG.getNode(ISD::UNDEF, dl, Ins[InsIdx].VT));\n          ++InsIdx;\n        }\n        if (NumRegs > 0)\n          --InsIdx;\n        continue;\n      }\n      InVals.push_back(DAG.getNode(ISD::UNDEF, dl, Ins[InsIdx].VT));\n      continue;\n    }\n\n    // In the following cases, assign a node order of \"idx+1\"\n    // to newly created nodes. The SDNodes for params have to\n    // appear in the same order as their order of appearance\n    // in the original function. \"idx+1\" holds that order.\n    if (!PAL.hasAttribute(i + 1, Attribute::ByVal)) {\n      if (Ty->isAggregateType()) {\n        SmallVector<EVT, 16> vtparts;\n        SmallVector<uint64_t, 16> offsets;\n\n        // NOTE: Here, we lose the ability to issue vector loads for vectors\n        // that are a part of a struct.  This should be investigated in the\n        // future.\n        ComputePTXValueVTs(*this, DAG.getDataLayout(), Ty, vtparts, &offsets,\n                           0);\n        assert(vtparts.size() > 0 && \"empty aggregate type not expected\");\n        bool aggregateIsPacked = false;\n        if (StructType *STy = llvm::dyn_cast<StructType>(Ty))\n          aggregateIsPacked = STy->isPacked();\n\n        SDValue Arg = getParamSymbol(DAG, idx, PtrVT);\n        for (unsigned parti = 0, parte = vtparts.size(); parti != parte;\n             ++parti) {\n          EVT partVT = vtparts[parti];\n          Value *srcValue = Constant::getNullValue(\n              PointerType::get(partVT.getTypeForEVT(F->getContext()),\n                               llvm::ADDRESS_SPACE_PARAM));\n          SDValue srcAddr =\n              DAG.getNode(ISD::ADD, dl, PtrVT, Arg,\n                          DAG.getConstant(offsets[parti], dl, PtrVT));\n          unsigned partAlign = aggregateIsPacked\n                                   ? 1\n                                   : DL.getABITypeAlignment(\n                                         partVT.getTypeForEVT(F->getContext()));\n          SDValue p;\n          if (Ins[InsIdx].VT.getSizeInBits() > partVT.getSizeInBits()) {\n            ISD::LoadExtType ExtOp = Ins[InsIdx].Flags.isSExt() ? \n                                     ISD::SEXTLOAD : ISD::ZEXTLOAD;\n            p = DAG.getExtLoad(ExtOp, dl, Ins[InsIdx].VT, Root, srcAddr,\n                               MachinePointerInfo(srcValue), partVT, partAlign);\n          } else {\n            p = DAG.getLoad(partVT, dl, Root, srcAddr,\n                            MachinePointerInfo(srcValue), partAlign);\n          }\n          if (p.getNode())\n            p.getNode()->setIROrder(idx + 1);\n          InVals.push_back(p);\n          ++InsIdx;\n        }\n        if (vtparts.size() > 0)\n          --InsIdx;\n        continue;\n      }\n      if (Ty->isVectorTy()) {\n        EVT ObjectVT = getValueType(DL, Ty);\n        SDValue Arg = getParamSymbol(DAG, idx, PtrVT);\n        unsigned NumElts = ObjectVT.getVectorNumElements();\n        assert(TLI->getNumRegisters(F->getContext(), ObjectVT) == NumElts &&\n               \"Vector was not scalarized\");\n        EVT EltVT = ObjectVT.getVectorElementType();\n\n        // V1 load\n        // f32 = load ...\n        if (NumElts == 1) {\n          // We only have one element, so just directly load it\n          Value *SrcValue = Constant::getNullValue(PointerType::get(\n              EltVT.getTypeForEVT(F->getContext()), llvm::ADDRESS_SPACE_PARAM));\n          SDValue P = DAG.getLoad(\n              EltVT, dl, Root, Arg, MachinePointerInfo(SrcValue),\n              DL.getABITypeAlignment(EltVT.getTypeForEVT(F->getContext())),\n              MachineMemOperand::MOInvariant);\n          if (P.getNode())\n            P.getNode()->setIROrder(idx + 1);\n\n          if (Ins[InsIdx].VT.getSizeInBits() > EltVT.getSizeInBits())\n            P = DAG.getNode(ISD::ANY_EXTEND, dl, Ins[InsIdx].VT, P);\n          InVals.push_back(P);\n          ++InsIdx;\n        } else if (NumElts == 2) {\n          // V2 load\n          // f32,f32 = load ...\n          EVT VecVT = EVT::getVectorVT(F->getContext(), EltVT, 2);\n          Value *SrcValue = Constant::getNullValue(PointerType::get(\n              VecVT.getTypeForEVT(F->getContext()), llvm::ADDRESS_SPACE_PARAM));\n          SDValue P = DAG.getLoad(\n              VecVT, dl, Root, Arg, MachinePointerInfo(SrcValue),\n              DL.getABITypeAlignment(VecVT.getTypeForEVT(F->getContext())),\n              MachineMemOperand::MOInvariant);\n          if (P.getNode())\n            P.getNode()->setIROrder(idx + 1);\n\n          SDValue Elt0 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, EltVT, P,\n                                     DAG.getIntPtrConstant(0, dl));\n          SDValue Elt1 = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, EltVT, P,\n                                     DAG.getIntPtrConstant(1, dl));\n\n          if (Ins[InsIdx].VT.getSizeInBits() > EltVT.getSizeInBits()) {\n            Elt0 = DAG.getNode(ISD::ANY_EXTEND, dl, Ins[InsIdx].VT, Elt0);\n            Elt1 = DAG.getNode(ISD::ANY_EXTEND, dl, Ins[InsIdx].VT, Elt1);\n          }\n\n          InVals.push_back(Elt0);\n          InVals.push_back(Elt1);\n          InsIdx += 2;\n        } else {\n          // V4 loads\n          // We have at least 4 elements (<3 x Ty> expands to 4 elements) and\n          // the vector will be expanded to a power of 2 elements, so we know we\n          // can always round up to the next multiple of 4 when creating the\n          // vector loads.\n          // e.g.  4 elem => 1 ld.v4\n          //       6 elem => 2 ld.v4\n          //       8 elem => 2 ld.v4\n          //      11 elem => 3 ld.v4\n          unsigned VecSize = 4;\n          if (EltVT.getSizeInBits() == 64) {\n            VecSize = 2;\n          }\n          EVT VecVT = EVT::getVectorVT(F->getContext(), EltVT, VecSize);\n          unsigned Ofst = 0;\n          for (unsigned i = 0; i < NumElts; i += VecSize) {\n            Value *SrcValue = Constant::getNullValue(\n                PointerType::get(VecVT.getTypeForEVT(F->getContext()),\n                                 llvm::ADDRESS_SPACE_PARAM));\n            SDValue SrcAddr = DAG.getNode(ISD::ADD, dl, PtrVT, Arg,\n                                          DAG.getConstant(Ofst, dl, PtrVT));\n            SDValue P = DAG.getLoad(\n                VecVT, dl, Root, SrcAddr, MachinePointerInfo(SrcValue),\n                DL.getABITypeAlignment(VecVT.getTypeForEVT(F->getContext())),\n                MachineMemOperand::MOInvariant);\n            if (P.getNode())\n              P.getNode()->setIROrder(idx + 1);\n\n            for (unsigned j = 0; j < VecSize; ++j) {\n              if (i + j >= NumElts)\n                break;\n              SDValue Elt = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, EltVT, P,\n                                        DAG.getIntPtrConstant(j, dl));\n              if (Ins[InsIdx].VT.getSizeInBits() > EltVT.getSizeInBits())\n                Elt = DAG.getNode(ISD::ANY_EXTEND, dl, Ins[InsIdx].VT, Elt);\n              InVals.push_back(Elt);\n            }\n            Ofst += DL.getTypeAllocSize(VecVT.getTypeForEVT(F->getContext()));\n          }\n          InsIdx += NumElts;\n        }\n\n        if (NumElts > 0)\n          --InsIdx;\n        continue;\n      }\n      // A plain scalar.\n      EVT ObjectVT = getValueType(DL, Ty);\n      // If ABI, load from the param symbol\n      SDValue Arg = getParamSymbol(DAG, idx, PtrVT);\n      Value *srcValue = Constant::getNullValue(PointerType::get(\n          ObjectVT.getTypeForEVT(F->getContext()), llvm::ADDRESS_SPACE_PARAM));\n      SDValue p;\n       if (ObjectVT.getSizeInBits() < Ins[InsIdx].VT.getSizeInBits()) {\n        ISD::LoadExtType ExtOp = Ins[InsIdx].Flags.isSExt() ? \n                                       ISD::SEXTLOAD : ISD::ZEXTLOAD;\n        p = DAG.getExtLoad(\n            ExtOp, dl, Ins[InsIdx].VT, Root, Arg, MachinePointerInfo(srcValue),\n            ObjectVT,\n            DL.getABITypeAlignment(ObjectVT.getTypeForEVT(F->getContext())));\n      } else {\n        p = DAG.getLoad(\n            Ins[InsIdx].VT, dl, Root, Arg, MachinePointerInfo(srcValue),\n            DL.getABITypeAlignment(ObjectVT.getTypeForEVT(F->getContext())));\n      }\n      if (p.getNode())\n        p.getNode()->setIROrder(idx + 1);\n      InVals.push_back(p);\n      continue;\n    }\n\n    // Param has ByVal attribute\n    // Return MoveParam(param symbol).\n    // Ideally, the param symbol can be returned directly,\n    // but when SDNode builder decides to use it in a CopyToReg(),\n    // machine instruction fails because TargetExternalSymbol\n    // (not lowered) is target dependent, and CopyToReg assumes\n    // the source is lowered.\n    EVT ObjectVT = getValueType(DL, Ty);\n    assert(ObjectVT == Ins[InsIdx].VT &&\n           \"Ins type did not match function type\");\n    SDValue Arg = getParamSymbol(DAG, idx, PtrVT);\n    SDValue p = DAG.getNode(NVPTXISD::MoveParam, dl, ObjectVT, Arg);\n    if (p.getNode())\n      p.getNode()->setIROrder(idx + 1);\n    if (isKernel)\n      InVals.push_back(p);\n    else {\n      SDValue p2 = DAG.getNode(\n          ISD::INTRINSIC_WO_CHAIN, dl, ObjectVT,\n          DAG.getConstant(Intrinsic::nvvm_ptr_local_to_gen, dl, MVT::i32), p);\n      InVals.push_back(p2);\n    }\n  }\n\n  // Clang will check explicit VarArg and issue error if any. However, Clang\n  // will let code with\n  // implicit var arg like f() pass. See bug 617733.\n  // We treat this case as if the arg list is empty.\n  // if (F.isVarArg()) {\n  // assert(0 && \"VarArg not supported yet!\");\n  //}\n\n  if (!OutChains.empty())\n    DAG.setRoot(DAG.getNode(ISD::TokenFactor, dl, MVT::Other, OutChains));\n\n  return Chain;\n}\n\nSDValue\n"},"isLegalAddressingMode":{"range":[3715,3753],"code":"bool NVPTXTargetLowering::isLegalAddressingMode(const DataLayout &DL,\n                                                const AddrMode &AM, Type *Ty,\n                                                unsigned AS) const {\n\n  // AddrMode - This represents an addressing mode of:\n  //    BaseGV + BaseOffs + BaseReg + Scale*ScaleReg\n  //\n  // The legal address modes are\n  // - [avar]\n  // - [areg]\n  // - [areg+immoff]\n  // - [immAddr]\n\n  if (AM.BaseGV) {\n    return !AM.BaseOffs && !AM.HasBaseReg && !AM.Scale;\n  }\n\n  switch (AM.Scale) {\n  case 0: // \"r\", \"r+i\" or \"i\" is allowed\n    break;\n  case 1:\n    if (AM.HasBaseReg) // \"r+r+i\" or \"r+r\" is not allowed.\n      return false;\n    // Otherwise we have r+i.\n    break;\n  default:\n    // No scale > 1 is allowed\n    return false;\n  }\n  return true;\n}\n\n//===----------------------------------------------------------------------===//\n//                         NVPTX Inline Assembly Support\n//===----------------------------------------------------------------------===//\n\n/// getConstraintType - Given a constraint letter, return the type of\n/// constraint it is for this target.\nNVPTXTargetLowering::ConstraintType\n"},"LowerAsmOperandForConstraint":{"range":[2532,2540],"code":"void NVPTXTargetLowering::LowerAsmOperandForConstraint(\n    SDValue Op, std::string &Constraint, std::vector<SDValue> &Ops,\n    SelectionDAG &DAG) const {\n  if (Constraint.length() > 1)\n    return;\n  else\n    TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);\n}\n\n"}}},"NVPTXTargetTransformInfo.cpp":{"path":"NVPTXTargetTransformInfo.cpp","size":4860,"lines":131,"functions":{"readsThreadIndex":{"range":[24,33],"code":"static bool readsThreadIndex(const IntrinsicInst *II) {\n  switch (II->getIntrinsicID()) {\n    default: return false;\n    case Intrinsic::nvvm_read_ptx_sreg_tid_x:\n    case Intrinsic::nvvm_read_ptx_sreg_tid_y:\n    case Intrinsic::nvvm_read_ptx_sreg_tid_z:\n      return true;\n  }\n}\n\n"},"isSourceOfDivergence":{"range":[49,91],"code":"bool NVPTXTTIImpl::isSourceOfDivergence(const Value *V) {\n  // Without inter-procedural analysis, we conservatively assume that arguments\n  // to __device__ functions are divergent.\n  if (const Argument *Arg = dyn_cast<Argument>(V))\n    return !isKernelFunction(*Arg->getParent());\n\n  if (const Instruction *I = dyn_cast<Instruction>(V)) {\n    // Without pointer analysis, we conservatively assume values loaded from\n    // generic or local address space are divergent.\n    if (const LoadInst *LI = dyn_cast<LoadInst>(I)) {\n      unsigned AS = LI->getPointerAddressSpace();\n      return AS == ADDRESS_SPACE_GENERIC || AS == ADDRESS_SPACE_LOCAL;\n    }\n    // Atomic instructions may cause divergence. Atomic instructions are\n    // executed sequentially across all threads in a warp. Therefore, an earlier\n    // executed thread may see different memory inputs than a later executed\n    // thread. For example, suppose *a = 0 initially.\n    //\n    //   atom.global.add.s32 d, [a], 1\n    //\n    // returns 0 for the first thread that enters the critical region, and 1 for\n    // the second thread.\n    if (I->isAtomic())\n      return true;\n    if (const IntrinsicInst *II = dyn_cast<IntrinsicInst>(I)) {\n      // Instructions that read threadIdx are obviously divergent.\n      if (readsThreadIndex(II) || readsLaneId(II))\n        return true;\n      // Handle the NVPTX atomic instrinsics that cannot be represented as an\n      // atomic IR instruction.\n      if (isNVVMAtomic(II))\n        return true;\n    }\n    // Conservatively consider the return value of function calls as divergent.\n    // We could analyze callees with bodies more precisely using\n    // inter-procedural analysis.\n    if (isa<CallInst>(I))\n      return true;\n  }\n\n  return false;\n}\n\n"},"getArithmeticInstrCost":{"range":[92,120],"code":"int NVPTXTTIImpl::getArithmeticInstrCost(\n    unsigned Opcode, Type *Ty, TTI::OperandValueKind Opd1Info,\n    TTI::OperandValueKind Opd2Info, TTI::OperandValueProperties Opd1PropInfo,\n    TTI::OperandValueProperties Opd2PropInfo) {\n  // Legalize the type.\n  std::pair<int, MVT> LT = TLI->getTypeLegalizationCost(DL, Ty);\n\n  int ISD = TLI->InstructionOpcodeToISD(Opcode);\n\n  switch (ISD) {\n  default:\n    return BaseT::getArithmeticInstrCost(Opcode, Ty, Opd1Info, Opd2Info,\n                                         Opd1PropInfo, Opd2PropInfo);\n  case ISD::ADD:\n  case ISD::MUL:\n  case ISD::XOR:\n  case ISD::OR:\n  case ISD::AND:\n    // The machine code (SASS) simulates an i64 with two i32. Therefore, we\n    // estimate that arithmetic operations on i64 are twice as expensive as\n    // those on types that can fit into one machine register.\n    if (LT.second.SimpleTy == MVT::i64)\n      return 2 * LT.first;\n    // Delegate other cases to the basic TTI.\n    return BaseT::getArithmeticInstrCost(Opcode, Ty, Opd1Info, Opd2Info,\n                                         Opd1PropInfo, Opd2PropInfo);\n  }\n}\n\n"},"getUnrollingPreferences":{"range":[121,132],"code":"void NVPTXTTIImpl::getUnrollingPreferences(Loop *L,\n                                           TTI::UnrollingPreferences &UP) {\n  BaseT::getUnrollingPreferences(L, UP);\n\n  // Enable partial unrolling and runtime unrolling, but reduce the\n  // threshold.  This partially unrolls small loops which are often\n  // unrolled by the PTX to SASS compiler and unrolling earlier can be\n  // beneficial.\n  UP.Partial = UP.Runtime = true;\n  UP.PartialThreshold = UP.Threshold / 4;\n}\n"},"readsLaneId":{"range":[34,38],"code":"static bool readsLaneId(const IntrinsicInst *II) {\n  return II->getIntrinsicID() == Intrinsic::nvvm_read_ptx_sreg_laneid;\n}\n\n// Whether the given intrinsic is an atomic instruction in PTX.\n"},"isNVVMAtomic":{"range":[39,48],"code":"static bool isNVVMAtomic(const IntrinsicInst *II) {\n  switch (II->getIntrinsicID()) {\n    default: return false;\n    case Intrinsic::nvvm_atomic_load_add_f32:\n    case Intrinsic::nvvm_atomic_load_inc_32:\n    case Intrinsic::nvvm_atomic_load_dec_32:\n      return true;\n  }\n}\n\n"}}},"NVPTXAsmPrinter.cpp":{"path":"NVPTXAsmPrinter.cpp","size":74482,"lines":2373,"functions":{"setAndEmitFunctionVirtualRegisters":{"range":[1640,1706],"code":"void NVPTXAsmPrinter::setAndEmitFunctionVirtualRegisters(\n    const MachineFunction &MF) {\n  SmallString<128> Str;\n  raw_svector_ostream O(Str);\n\n  // Map the global virtual register number to a register class specific\n  // virtual register number starting from 1 with that class.\n  const TargetRegisterInfo *TRI = MF.getSubtarget().getRegisterInfo();\n  //unsigned numRegClasses = TRI->getNumRegClasses();\n\n  // Emit the Fake Stack Object\n  const MachineFrameInfo *MFI = MF.getFrameInfo();\n  int NumBytes = (int) MFI->getStackSize();\n  if (NumBytes) {\n    O << \"\\t.local .align \" << MFI->getMaxAlignment() << \" .b8 \\t\" << DEPOTNAME\n      << getFunctionNumber() << \"[\" << NumBytes << \"];\\n\";\n    if (static_cast<const NVPTXTargetMachine &>(MF.getTarget()).is64Bit()) {\n      O << \"\\t.reg .b64 \\t%SP;\\n\";\n      O << \"\\t.reg .b64 \\t%SPL;\\n\";\n    } else {\n      O << \"\\t.reg .b32 \\t%SP;\\n\";\n      O << \"\\t.reg .b32 \\t%SPL;\\n\";\n    }\n  }\n\n  // Go through all virtual registers to establish the mapping between the\n  // global virtual\n  // register number and the per class virtual register number.\n  // We use the per class virtual register number in the ptx output.\n  unsigned int numVRs = MRI->getNumVirtRegs();\n  for (unsigned i = 0; i < numVRs; i++) {\n    unsigned int vr = TRI->index2VirtReg(i);\n    const TargetRegisterClass *RC = MRI->getRegClass(vr);\n    DenseMap<unsigned, unsigned> &regmap = VRegMapping[RC];\n    int n = regmap.size();\n    regmap.insert(std::make_pair(vr, n + 1));\n  }\n\n  // Emit register declarations\n  // @TODO: Extract out the real register usage\n  // O << \"\\t.reg .pred %p<\" << NVPTXNumRegisters << \">;\\n\";\n  // O << \"\\t.reg .s16 %rc<\" << NVPTXNumRegisters << \">;\\n\";\n  // O << \"\\t.reg .s16 %rs<\" << NVPTXNumRegisters << \">;\\n\";\n  // O << \"\\t.reg .s32 %r<\" << NVPTXNumRegisters << \">;\\n\";\n  // O << \"\\t.reg .s64 %rd<\" << NVPTXNumRegisters << \">;\\n\";\n  // O << \"\\t.reg .f32 %f<\" << NVPTXNumRegisters << \">;\\n\";\n  // O << \"\\t.reg .f64 %fd<\" << NVPTXNumRegisters << \">;\\n\";\n\n  // Emit declaration of the virtual registers or 'physical' registers for\n  // each register class\n  for (unsigned i=0; i< TRI->getNumRegClasses(); i++) {\n    const TargetRegisterClass *RC = TRI->getRegClass(i);\n    DenseMap<unsigned, unsigned> &regmap = VRegMapping[RC];\n    std::string rcname = getNVPTXRegClassName(RC);\n    std::string rcStr = getNVPTXRegClassStr(RC);\n    int n = regmap.size();\n\n    // Only declare those registers that may be used.\n    if (n) {\n       O << \"\\t.reg \" << rcname << \" \\t\" << rcStr << \"<\" << (n+1)\n         << \">;\\n\";\n    }\n  }\n\n  OutStreamer->EmitRawText(O.str());\n}\n\n"},"usedInGlobalVarDef":{"range":[634,649],"code":"static bool usedInGlobalVarDef(const Constant *C) {\n  if (!C)\n    return false;\n\n  if (const GlobalVariable *GV = dyn_cast<GlobalVariable>(C)) {\n    return GV->getName() != \"llvm.used\";\n  }\n\n  for (const User *U : C->users())\n    if (const Constant *C = dyn_cast<Constant>(U))\n      if (usedInGlobalVarDef(C))\n        return true;\n\n  return false;\n}\n\n"},"printMCExpr":{"range":[2167,2238],"code":"void NVPTXAsmPrinter::printMCExpr(const MCExpr &Expr, raw_ostream &OS) {\n  switch (Expr.getKind()) {\n  case MCExpr::Target:\n    return cast<MCTargetExpr>(&Expr)->printImpl(OS, MAI);\n  case MCExpr::Constant:\n    OS << cast<MCConstantExpr>(Expr).getValue();\n    return;\n\n  case MCExpr::SymbolRef: {\n    const MCSymbolRefExpr &SRE = cast<MCSymbolRefExpr>(Expr);\n    const MCSymbol &Sym = SRE.getSymbol();\n    Sym.print(OS, MAI);\n    return;\n  }\n\n  case MCExpr::Unary: {\n    const MCUnaryExpr &UE = cast<MCUnaryExpr>(Expr);\n    switch (UE.getOpcode()) {\n    case MCUnaryExpr::LNot:  OS << '!'; break;\n    case MCUnaryExpr::Minus: OS << '-'; break;\n    case MCUnaryExpr::Not:   OS << '~'; break;\n    case MCUnaryExpr::Plus:  OS << '+'; break;\n    }\n    printMCExpr(*UE.getSubExpr(), OS);\n    return;\n  }\n\n  case MCExpr::Binary: {\n    const MCBinaryExpr &BE = cast<MCBinaryExpr>(Expr);\n\n    // Only print parens around the LHS if it is non-trivial.\n    if (isa<MCConstantExpr>(BE.getLHS()) || isa<MCSymbolRefExpr>(BE.getLHS()) ||\n        isa<NVPTXGenericMCSymbolRefExpr>(BE.getLHS())) {\n      printMCExpr(*BE.getLHS(), OS);\n    } else {\n      OS << '(';\n      printMCExpr(*BE.getLHS(), OS);\n      OS<< ')';\n    }\n\n    switch (BE.getOpcode()) {\n    case MCBinaryExpr::Add:\n      // Print \"X-42\" instead of \"X+-42\".\n      if (const MCConstantExpr *RHSC = dyn_cast<MCConstantExpr>(BE.getRHS())) {\n        if (RHSC->getValue() < 0) {\n          OS << RHSC->getValue();\n          return;\n        }\n      }\n\n      OS <<  '+';\n      break;\n    default: llvm_unreachable(\"Unhandled binary operator\");\n    }\n\n    // Only print parens around the LHS if it is non-trivial.\n    if (isa<MCConstantExpr>(BE.getRHS()) || isa<MCSymbolRefExpr>(BE.getRHS())) {\n      printMCExpr(*BE.getRHS(), OS);\n    } else {\n      OS << '(';\n      printMCExpr(*BE.getRHS(), OS);\n      OS << ')';\n    }\n    return;\n  }\n  }\n\n  llvm_unreachable(\"Invalid expression kind!\");\n}\n\n/// PrintAsmOperand - Print out an operand for an inline asm expression.\n///\n"},"EmitFunctionEntryLabel":{"range":[451,482],"code":"void NVPTXAsmPrinter::EmitFunctionEntryLabel() {\n  SmallString<128> Str;\n  raw_svector_ostream O(Str);\n\n  if (!GlobalsEmitted) {\n    emitGlobals(*MF->getFunction()->getParent());\n    GlobalsEmitted = true;\n  }\n  \n  // Set up\n  MRI = &MF->getRegInfo();\n  F = MF->getFunction();\n  emitLinkageDirective(F, O);\n  if (llvm::isKernelFunction(*F))\n    O << \".entry \";\n  else {\n    O << \".func \";\n    printReturnValStr(*MF, O);\n  }\n\n  CurrentFnSym->print(O, MAI);\n\n  emitFunctionParamList(*MF, O);\n\n  if (llvm::isKernelFunction(*F))\n    emitKernelFunctionDirectives(*F, O);\n\n  OutStreamer->EmitRawText(O.str());\n\n  prevDebugLoc = DebugLoc();\n}\n\n"},"recordAndEmitFilenames":{"range":[771,806],"code":"void NVPTXAsmPrinter::recordAndEmitFilenames(Module &M) {\n  DebugInfoFinder DbgFinder;\n  DbgFinder.processModule(M);\n\n  unsigned i = 1;\n  for (const DICompileUnit *DIUnit : DbgFinder.compile_units()) {\n    StringRef Filename = DIUnit->getFilename();\n    StringRef Dirname = DIUnit->getDirectory();\n    SmallString<128> FullPathName = Dirname;\n    if (!Dirname.empty() && !sys::path::is_absolute(Filename)) {\n      sys::path::append(FullPathName, Filename);\n      Filename = FullPathName;\n    }\n    if (filenameMap.find(Filename) != filenameMap.end())\n      continue;\n    filenameMap[Filename] = i;\n    OutStreamer->EmitDwarfFileDirective(i, \"\", Filename);\n    ++i;\n  }\n\n  for (DISubprogram *SP : DbgFinder.subprograms()) {\n    StringRef Filename = SP->getFilename();\n    StringRef Dirname = SP->getDirectory();\n    SmallString<128> FullPathName = Dirname;\n    if (!Dirname.empty() && !sys::path::is_absolute(Filename)) {\n      sys::path::append(FullPathName, Filename);\n      Filename = FullPathName;\n    }\n    if (filenameMap.find(Filename) != filenameMap.end())\n      continue;\n    filenameMap[Filename] = i;\n    OutStreamer->EmitDwarfFileDirective(i, \"\", Filename);\n    ++i;\n  }\n}\n\n"},"EmitInstruction":{"range":[158,169],"code":"void NVPTXAsmPrinter::EmitInstruction(const MachineInstr *MI) {\n  SmallString<128> Str;\n  raw_svector_ostream OS(Str);\n  if (static_cast<NVPTXTargetMachine &>(TM).getDrvInterface() == NVPTX::CUDA)\n    emitLineNumberAsDotLoc(*MI);\n\n  MCInst Inst;\n  lowerToMCInst(MI, Inst);\n  EmitToStreamer(*OutStreamer, Inst);\n}\n\n// Handle symbol backtracking for targets that do not support image handles\n"},"readLine":{"range":[2357,2369],"code":"std::string LineReader::readLine(unsigned lineNum) {\n  if (lineNum < theCurLine) {\n    theCurLine = 0;\n    fstr.seekg(0, std::ios::beg);\n  }\n  while (theCurLine < lineNum) {\n    fstr.getline(buff, 500);\n    theCurLine++;\n  }\n  return buff;\n}\n\n// Force static initialization.\n"},"emitDemotedVars":{"range":[1291,1302],"code":"void NVPTXAsmPrinter::emitDemotedVars(const Function *f, raw_ostream &O) {\n  if (localDecls.find(f) == localDecls.end())\n    return;\n\n  std::vector<const GlobalVariable *> &gvars = localDecls[f];\n\n  for (unsigned i = 0, e = gvars.size(); i != e; ++i) {\n    O << \"\\t// demoted variable\\n\\t\";\n    printModuleLevelGV(gvars[i], O, true);\n  }\n}\n\n"},"getVirtualRegisterName":{"range":[563,582],"code":"NVPTXAsmPrinter::getVirtualRegisterName(unsigned Reg) const {\n  const TargetRegisterClass *RC = MRI->getRegClass(Reg);\n\n  std::string Name;\n  raw_string_ostream NameStr(Name);\n\n  VRegRCMap::const_iterator I = VRegMapping.find(RC);\n  assert(I != VRegMapping.end() && \"Bad register class\");\n  const DenseMap<unsigned, unsigned> &RegMap = I->second;\n\n  VRegMap::const_iterator VI = RegMap.find(Reg);\n  assert(VI != RegMap.end() && \"Bad virtual register\");\n  unsigned MappedVR = VI->second;\n\n  NameStr << getNVPTXRegClassStr(RC) << MappedVR;\n\n  NameStr.flush();\n  return Name;\n}\n\n"},"emitSrcInText":{"range":[2331,2343],"code":"void NVPTXAsmPrinter::emitSrcInText(StringRef filename, unsigned line) {\n  std::stringstream temp;\n  LineReader *reader = this->getReader(filename);\n  temp << \"\\n//\";\n  temp << filename.str();\n  temp << \":\";\n  temp << line;\n  temp << \" \";\n  temp << reader->readLine(line);\n  temp << \"\\n\";\n  this->OutStreamer->EmitRawText(temp.str());\n}\n\n"},"lowerImageHandleSymbol":{"range":[220,231],"code":"void NVPTXAsmPrinter::lowerImageHandleSymbol(unsigned Index, MCOperand &MCOp) {\n  // Ewwww\n  TargetMachine &TM = const_cast<TargetMachine&>(MF->getTarget());\n  NVPTXTargetMachine &nvTM = static_cast<NVPTXTargetMachine&>(TM);\n  const NVPTXMachineFunctionInfo *MFI = MF->getInfo<NVPTXMachineFunctionInfo>();\n  const char *Sym = MFI->getImageHandleSymbol(Index);\n  std::string *SymNamePtr =\n    nvTM.getManagedStrPool()->getManagedString(Sym);\n  MCOp = GetSymbolRef(OutContext.getOrCreateSymbol(\n    StringRef(SymNamePtr->c_str())));\n}\n\n"},"DiscoverDependentGlobals":{"range":[69,84],"code":"void DiscoverDependentGlobals(const Value *V,\n                              DenseSet<const GlobalVariable *> &Globals) {\n  if (const GlobalVariable *GV = dyn_cast<GlobalVariable>(V))\n    Globals.insert(GV);\n  else {\n    if (const User *U = dyn_cast<User>(V)) {\n      for (unsigned i = 0, e = U->getNumOperands(); i != e; ++i) {\n        DiscoverDependentGlobals(U->getOperand(i), Globals);\n      }\n    }\n  }\n}\n\n/// VisitGlobalVariableForEmission - Add \\p GV to the list of GlobalVariable\n/// instances to be emitted, but only after any dependents have been added\n/// first.\n"},"emitHeader":{"range":[911,946],"code":"void NVPTXAsmPrinter::emitHeader(Module &M, raw_ostream &O,\n                                 const NVPTXSubtarget &STI) {\n  O << \"//\\n\";\n  O << \"// Generated by LLVM NVPTX Back-End\\n\";\n  O << \"//\\n\";\n  O << \"\\n\";\n\n  unsigned PTXVersion = STI.getPTXVersion();\n  O << \".version \" << (PTXVersion / 10) << \".\" << (PTXVersion % 10) << \"\\n\";\n\n  O << \".target \";\n  O << STI.getTargetName();\n\n  const NVPTXTargetMachine &NTM = static_cast<const NVPTXTargetMachine &>(TM);\n  if (NTM.getDrvInterface() == NVPTX::NVCL)\n    O << \", texmode_independent\";\n  else {\n    if (!STI.hasDouble())\n      O << \", map_f64_to_f32\";\n  }\n\n  if (MAI->doesSupportDebugInformation())\n    O << \", debug\";\n\n  O << \"\\n\";\n\n  O << \".address_size \";\n  if (NTM.is64Bit())\n    O << \"64\";\n  else\n    O << \"32\";\n  O << \"\\n\";\n\n  O << \"\\n\";\n}\n\n"},"emitDeclarations":{"range":[719,770],"code":"void NVPTXAsmPrinter::emitDeclarations(const Module &M, raw_ostream &O) {\n  llvm::DenseMap<const Function *, bool> seenMap;\n  for (Module::const_iterator FI = M.begin(), FE = M.end(); FI != FE; ++FI) {\n    const Function *F = &*FI;\n\n    if (F->isDeclaration()) {\n      if (F->use_empty())\n        continue;\n      if (F->getIntrinsicID())\n        continue;\n      emitDeclaration(F, O);\n      continue;\n    }\n    for (const User *U : F->users()) {\n      if (const Constant *C = dyn_cast<Constant>(U)) {\n        if (usedInGlobalVarDef(C)) {\n          // The use is in the initialization of a global variable\n          // that is a function pointer, so print a declaration\n          // for the original function\n          emitDeclaration(F, O);\n          break;\n        }\n        // Emit a declaration of this function if the function that\n        // uses this constant expr has already been seen.\n        if (useFuncSeen(C, seenMap)) {\n          emitDeclaration(F, O);\n          break;\n        }\n      }\n\n      if (!isa<Instruction>(U))\n        continue;\n      const Instruction *instr = cast<Instruction>(U);\n      const BasicBlock *bb = instr->getParent();\n      if (!bb)\n        continue;\n      const Function *caller = bb->getParent();\n      if (!caller)\n        continue;\n\n      // If a caller has already been seen, then the caller is\n      // appearing in the module before the callee. so print out\n      // a declaration for the callee.\n      if (seenMap.find(caller) != seenMap.end()) {\n        emitDeclaration(F, O);\n        break;\n      }\n    }\n    seenMap[F] = true;\n  }\n}\n\n"},"printFPConstant":{"range":[1707,1731],"code":"void NVPTXAsmPrinter::printFPConstant(const ConstantFP *Fp, raw_ostream &O) {\n  APFloat APF = APFloat(Fp->getValueAPF()); // make a copy\n  bool ignored;\n  unsigned int numHex;\n  const char *lead;\n\n  if (Fp->getType()->getTypeID() == Type::FloatTyID) {\n    numHex = 8;\n    lead = \"0f\";\n    APF.convert(APFloat::IEEEsingle, APFloat::rmNearestTiesToEven, &ignored);\n  } else if (Fp->getType()->getTypeID() == Type::DoubleTyID) {\n    numHex = 16;\n    lead = \"0d\";\n    APF.convert(APFloat::IEEEdouble, APFloat::rmNearestTiesToEven, &ignored);\n  } else\n    llvm_unreachable(\"unsupported fp type\");\n\n  APInt API = APF.bitcastToAPInt();\n  std::string hexstr(utohexstr(API.getZExtValue()));\n  O << lead;\n  if (hexstr.length() < numHex)\n    O << std::string(numHex - hexstr.length(), '0');\n  O << utohexstr(API.getZExtValue());\n}\n\n"},"ConvertFloatToBytes":{"range":[1792,1798],"code":"static void ConvertFloatToBytes(unsigned char *p, float val) {\n  int32_t *vp = (int32_t *)&val;\n  for (unsigned i = 0; i < sizeof(int32_t); ++i) {\n    p[i] = (unsigned char)*vp;\n    *vp >>= 8;\n  }\n}\n"},"useFuncSeen":{"range":[699,718],"code":"static bool useFuncSeen(const Constant *C,\n                        llvm::DenseMap<const Function *, bool> &seenMap) {\n  for (const User *U : C->users()) {\n    if (const Constant *cu = dyn_cast<Constant>(U)) {\n      if (useFuncSeen(cu, seenMap))\n        return true;\n    } else if (const Instruction *I = dyn_cast<Instruction>(U)) {\n      const BasicBlock *bb = I->getParent();\n      if (!bb)\n        continue;\n      const Function *caller = bb->getParent();\n      if (!caller)\n        continue;\n      if (seenMap.find(caller) != seenMap.end())\n        return true;\n    }\n  }\n  return false;\n}\n\n"},"VisitGlobalVariableForEmission":{"range":[85,113],"code":"void VisitGlobalVariableForEmission(\n    const GlobalVariable *GV, SmallVectorImpl<const GlobalVariable *> &Order,\n    DenseSet<const GlobalVariable *> &Visited,\n    DenseSet<const GlobalVariable *> &Visiting) {\n  // Have we already visited this one?\n  if (Visited.count(GV))\n    return;\n\n  // Do we have a circular dependency?\n  if (!Visiting.insert(GV).second)\n    report_fatal_error(\"Circular dependency found in global variable set\");\n\n  // Make sure we visit all dependents first\n  DenseSet<const GlobalVariable *> Others;\n  for (unsigned i = 0, e = GV->getNumOperands(); i != e; ++i)\n    DiscoverDependentGlobals(GV->getOperand(i), Others);\n\n  for (DenseSet<const GlobalVariable *>::iterator I = Others.begin(),\n                                                  E = Others.end();\n       I != E; ++I)\n    VisitGlobalVariableForEmission(*I, Order, Visited, Visiting);\n\n  // Now we can visit ourself\n  Order.push_back(GV);\n  Visited.insert(GV);\n  Visiting.erase(GV);\n}\n}\n\n"},"PrintAsmMemoryOperand":{"range":[2260,2272],"code":"bool NVPTXAsmPrinter::PrintAsmMemoryOperand(\n    const MachineInstr *MI, unsigned OpNo, unsigned AsmVariant,\n    const char *ExtraCode, raw_ostream &O) {\n  if (ExtraCode && ExtraCode[0])\n    return true; // Unknown modifier\n\n  O << '[';\n  printMemOperand(MI, OpNo, O);\n  O << ']';\n\n  return false;\n}\n\n"},"EmitFunctionBodyStart":{"range":[483,493],"code":"void NVPTXAsmPrinter::EmitFunctionBodyStart() {\n  VRegMapping.clear();\n  OutStreamer->EmitRawText(StringRef(\"{\\n\"));\n  setAndEmitFunctionVirtualRegisters(*MF);\n\n  SmallString<128> Str;\n  raw_svector_ostream O(Str);\n  emitDemotedVars(MF->getFunction(), O);\n  OutStreamer->EmitRawText(O.str());\n}\n\n"},"printReturnValStr":{"range":[409,417],"code":"void NVPTXAsmPrinter::printReturnValStr(const MachineFunction &MF,\n                                        raw_ostream &O) {\n  const Function *F = MF.getFunction();\n  printReturnValStr(F, O);\n}\n\n// Return true if MBB is the header of a loop marked with\n// llvm.loop.unroll.disable.\n// TODO: consider \"#pragma unroll 1\" which is equivalent to \"#pragma nounroll\".\n"},"lowerToMCInst":{"range":[232,257],"code":"void NVPTXAsmPrinter::lowerToMCInst(const MachineInstr *MI, MCInst &OutMI) {\n  OutMI.setOpcode(MI->getOpcode());\n  // Special: Do not mangle symbol operand of CALL_PROTOTYPE\n  if (MI->getOpcode() == NVPTX::CALL_PROTOTYPE) {\n    const MachineOperand &MO = MI->getOperand(0);\n    OutMI.addOperand(GetSymbolRef(\n      OutContext.getOrCreateSymbol(Twine(MO.getSymbolName()))));\n    return;\n  }\n\n  for (unsigned i = 0, e = MI->getNumOperands(); i != e; ++i) {\n    const MachineOperand &MO = MI->getOperand(i);\n\n    MCOperand MCOp;\n    if (!nvptxSubtarget->hasImageHandles()) {\n      if (lowerImageHandleOperand(MI, i, MCOp)) {\n        OutMI.addOperand(MCOp);\n        continue;\n      }\n    }\n\n    if (lowerOperand(MO, MCOp))\n      OutMI.addOperand(MCOp);\n  }\n}\n\n"},"emitLineNumberAsDotLoc":{"range":[114,157],"code":"void NVPTXAsmPrinter::emitLineNumberAsDotLoc(const MachineInstr &MI) {\n  if (!EmitLineNumbers)\n    return;\n  if (ignoreLoc(MI))\n    return;\n\n  const DebugLoc &curLoc = MI.getDebugLoc();\n\n  if (!prevDebugLoc && !curLoc)\n    return;\n\n  if (prevDebugLoc == curLoc)\n    return;\n\n  prevDebugLoc = curLoc;\n\n  if (!curLoc)\n    return;\n\n  auto *Scope = cast_or_null<DIScope>(curLoc.getScope());\n  if (!Scope)\n     return;\n\n  StringRef fileName(Scope->getFilename());\n  StringRef dirName(Scope->getDirectory());\n  SmallString<128> FullPathName = dirName;\n  if (!dirName.empty() && !sys::path::is_absolute(fileName)) {\n    sys::path::append(FullPathName, fileName);\n    fileName = FullPathName;\n  }\n\n  if (filenameMap.find(fileName) == filenameMap.end())\n    return;\n\n  // Emit the line from the source file.\n  if (InterleaveSrc)\n    this->emitSrcInText(fileName, curLoc.getLine());\n\n  std::stringstream temp;\n  temp << \"\\t.loc \" << filenameMap[fileName] << \" \" << curLoc.getLine()\n       << \" \" << curLoc.getCol();\n  OutStreamer->EmitRawText(temp.str());\n}\n\n"},"printVecModifiedImmediate":{"range":[588,619],"code":"void NVPTXAsmPrinter::printVecModifiedImmediate(\n    const MachineOperand &MO, const char *Modifier, raw_ostream &O) {\n  static const char vecelem[] = { '0', '1', '2', '3', '0', '1', '2', '3' };\n  int Imm = (int) MO.getImm();\n  if (0 == strcmp(Modifier, \"vecelem\"))\n    O << \"_\" << vecelem[Imm];\n  else if (0 == strcmp(Modifier, \"vecv4comm1\")) {\n    if ((Imm < 0) || (Imm > 3))\n      O << \"//\";\n  } else if (0 == strcmp(Modifier, \"vecv4comm2\")) {\n    if ((Imm < 4) || (Imm > 7))\n      O << \"//\";\n  } else if (0 == strcmp(Modifier, \"vecv4pos\")) {\n    if (Imm < 0)\n      Imm = 0;\n    O << \"_\" << vecelem[Imm % 4];\n  } else if (0 == strcmp(Modifier, \"vecv2comm1\")) {\n    if ((Imm < 0) || (Imm > 1))\n      O << \"//\";\n  } else if (0 == strcmp(Modifier, \"vecv2comm2\")) {\n    if ((Imm < 2) || (Imm > 3))\n      O << \"//\";\n  } else if (0 == strcmp(Modifier, \"vecv2pos\")) {\n    if (Imm < 0)\n      Imm = 0;\n    O << \"_\" << vecelem[Imm % 2];\n  } else\n    llvm_unreachable(\"Unknown Modifier on immediate operand\");\n}\n\n\n\n"},"lowerImageHandleOperand":{"range":[170,219],"code":"bool NVPTXAsmPrinter::lowerImageHandleOperand(const MachineInstr *MI,\n                                           unsigned OpNo, MCOperand &MCOp) {\n  const MachineOperand &MO = MI->getOperand(OpNo);\n  const MCInstrDesc &MCID = MI->getDesc();\n\n  if (MCID.TSFlags & NVPTXII::IsTexFlag) {\n    // This is a texture fetch, so operand 4 is a texref and operand 5 is\n    // a samplerref\n    if (OpNo == 4 && MO.isImm()) {\n      lowerImageHandleSymbol(MO.getImm(), MCOp);\n      return true;\n    }\n    if (OpNo == 5 && MO.isImm() && !(MCID.TSFlags & NVPTXII::IsTexModeUnifiedFlag)) {\n      lowerImageHandleSymbol(MO.getImm(), MCOp);\n      return true;\n    }\n\n    return false;\n  } else if (MCID.TSFlags & NVPTXII::IsSuldMask) {\n    unsigned VecSize =\n      1 << (((MCID.TSFlags & NVPTXII::IsSuldMask) >> NVPTXII::IsSuldShift) - 1);\n\n    // For a surface load of vector size N, the Nth operand will be the surfref\n    if (OpNo == VecSize && MO.isImm()) {\n      lowerImageHandleSymbol(MO.getImm(), MCOp);\n      return true;\n    }\n\n    return false;\n  } else if (MCID.TSFlags & NVPTXII::IsSustFlag) {\n    // This is a surface store, so operand 0 is a surfref\n    if (OpNo == 0 && MO.isImm()) {\n      lowerImageHandleSymbol(MO.getImm(), MCOp);\n      return true;\n    }\n\n    return false;\n  } else if (MCID.TSFlags & NVPTXII::IsSurfTexQueryFlag) {\n    // This is a query, so operand 1 is a surfref/texref\n    if (OpNo == 1 && MO.isImm()) {\n      lowerImageHandleSymbol(MO.getImm(), MCOp);\n      return true;\n    }\n\n    return false;\n  }\n\n  return false;\n}\n\n"},"emitFunctionParamList":{"range":[1634,1639],"code":"void NVPTXAsmPrinter::emitFunctionParamList(const MachineFunction &MF,\n                                            raw_ostream &O) {\n  const Function *F = MF.getFunction();\n  emitFunctionParamList(F, O);\n}\n\n"},"isEmptyXXStructor":{"range":[807,813],"code":"static bool isEmptyXXStructor(GlobalVariable *GV) {\n  if (!GV) return true;\n  const ConstantArray *InitList = dyn_cast<ConstantArray>(GV->getInitializer());\n  if (!InitList) return true;  // Not an array; we don't know how to parse.\n  return InitList->getNumOperands() == 0;\n}\n\n"},"getReader":{"range":[2344,2356],"code":"LineReader *NVPTXAsmPrinter::getReader(const std::string &filename) {\n  if (!reader) {\n    reader = new LineReader(filename);\n  }\n\n  if (reader->fileName() != filename) {\n    delete reader;\n    reader = new LineReader(filename);\n  }\n\n  return reader;\n}\n\n"},"doInitialization":{"range":[814,878],"code":"bool NVPTXAsmPrinter::doInitialization(Module &M) {\n  // Construct a default subtarget off of the TargetMachine defaults. The\n  // rest of NVPTX isn't friendly to change subtargets per function and\n  // so the default TargetMachine will have all of the options.\n  const Triple &TT = TM.getTargetTriple();\n  StringRef CPU = TM.getTargetCPU();\n  StringRef FS = TM.getTargetFeatureString();\n  const NVPTXTargetMachine &NTM = static_cast<const NVPTXTargetMachine &>(TM);\n  const NVPTXSubtarget STI(TT, CPU, FS, NTM);\n\n  if (M.alias_size()) {\n    report_fatal_error(\"Module has aliases, which NVPTX does not support.\");\n    return true; // error\n  }\n  if (!isEmptyXXStructor(M.getNamedGlobal(\"llvm.global_ctors\"))) {\n    report_fatal_error(\n        \"Module has a nontrivial global ctor, which NVPTX does not support.\");\n    return true;  // error\n  }\n  if (!isEmptyXXStructor(M.getNamedGlobal(\"llvm.global_dtors\"))) {\n    report_fatal_error(\n        \"Module has a nontrivial global dtor, which NVPTX does not support.\");\n    return true;  // error\n  }\n\n  SmallString<128> Str1;\n  raw_svector_ostream OS1(Str1);\n\n  MMI = getAnalysisIfAvailable<MachineModuleInfo>();\n\n  // We need to call the parent's one explicitly.\n  //bool Result = AsmPrinter::doInitialization(M);\n\n  // Initialize TargetLoweringObjectFile.\n  const_cast<TargetLoweringObjectFile &>(getObjFileLowering())\n      .Initialize(OutContext, TM);\n\n  Mang = new Mangler();\n\n  // Emit header before any dwarf directives are emitted below.\n  emitHeader(M, OS1, STI);\n  OutStreamer->EmitRawText(OS1.str());\n\n  // Already commented out\n  //bool Result = AsmPrinter::doInitialization(M);\n\n  // Emit module-level inline asm if it exists.\n  if (!M.getModuleInlineAsm().empty()) {\n    OutStreamer->AddComment(\"Start of file scope inline assembly\");\n    OutStreamer->AddBlankLine();\n    OutStreamer->EmitRawText(StringRef(M.getModuleInlineAsm()));\n    OutStreamer->AddBlankLine();\n    OutStreamer->AddComment(\"End of file scope inline assembly\");\n    OutStreamer->AddBlankLine();\n  }\n\n  // If we're not NVCL we're CUDA, go ahead and emit filenames.\n  if (TM.getTargetTriple().getOS() != Triple::NVCL)\n    recordAndEmitFilenames(M);\n\n  GlobalsEmitted = false;\n    \n  return false; // success\n}\n\n"},"emitImplicitDef":{"range":[499,510],"code":"void NVPTXAsmPrinter::emitImplicitDef(const MachineInstr *MI) const {\n  unsigned RegNo = MI->getOperand(0).getReg();\n  if (TargetRegisterInfo::isVirtualRegister(RegNo)) {\n    OutStreamer->AddComment(Twine(\"implicit-def: \") +\n                            getVirtualRegisterName(RegNo));\n  } else {\n    OutStreamer->AddComment(Twine(\"implicit-def: \") +\n                            nvptxSubtarget->getRegisterInfo()->getName(RegNo));\n  }\n  OutStreamer->AddBlankLine();\n}\n\n"},"printOperand":{"range":[2273,2314],"code":"void NVPTXAsmPrinter::printOperand(const MachineInstr *MI, int opNum,\n                                   raw_ostream &O, const char *Modifier) {\n  const MachineOperand &MO = MI->getOperand(opNum);\n  switch (MO.getType()) {\n  case MachineOperand::MO_Register:\n    if (TargetRegisterInfo::isPhysicalRegister(MO.getReg())) {\n      if (MO.getReg() == NVPTX::VRDepot)\n        O << DEPOTNAME << getFunctionNumber();\n      else\n        O << NVPTXInstPrinter::getRegisterName(MO.getReg());\n    } else {\n      emitVirtualRegister(MO.getReg(), O);\n    }\n    return;\n\n  case MachineOperand::MO_Immediate:\n    if (!Modifier)\n      O << MO.getImm();\n    else if (strstr(Modifier, \"vec\") == Modifier)\n      printVecModifiedImmediate(MO, Modifier, O);\n    else\n      llvm_unreachable(\n          \"Don't know how to handle modifier on immediate operand\");\n    return;\n\n  case MachineOperand::MO_FPImmediate:\n    printFPConstant(MO.getFPImm(), O);\n    break;\n\n  case MachineOperand::MO_GlobalAddress:\n    getSymbol(MO.getGlobal())->print(O, MAI);\n    break;\n\n  case MachineOperand::MO_MachineBasicBlock:\n    MO.getMBB()->getSymbol()->print(O, MAI);\n    return;\n\n  default:\n    llvm_unreachable(\"Operand type not supported.\");\n  }\n}\n\n"},"printModuleLevelGV":{"range":[1038,1290],"code":"void NVPTXAsmPrinter::printModuleLevelGV(const GlobalVariable *GVar,\n                                         raw_ostream &O,\n                                         bool processDemoted) {\n\n  // Skip meta data\n  if (GVar->hasSection()) {\n    if (GVar->getSection() == \"llvm.metadata\")\n      return;\n  }\n\n  // Skip LLVM intrinsic global variables\n  if (GVar->getName().startswith(\"llvm.\") ||\n      GVar->getName().startswith(\"nvvm.\"))\n    return;\n\n  const DataLayout &DL = getDataLayout();\n\n  // GlobalVariables are always constant pointers themselves.\n  PointerType *PTy = GVar->getType();\n  Type *ETy = GVar->getValueType();\n\n  if (GVar->hasExternalLinkage()) {\n    if (GVar->hasInitializer())\n      O << \".visible \";\n    else\n      O << \".extern \";\n  } else if (GVar->hasLinkOnceLinkage() || GVar->hasWeakLinkage() ||\n             GVar->hasAvailableExternallyLinkage() ||\n             GVar->hasCommonLinkage()) {\n    O << \".weak \";\n  }\n\n  if (llvm::isTexture(*GVar)) {\n    O << \".global .texref \" << llvm::getTextureName(*GVar) << \";\\n\";\n    return;\n  }\n\n  if (llvm::isSurface(*GVar)) {\n    O << \".global .surfref \" << llvm::getSurfaceName(*GVar) << \";\\n\";\n    return;\n  }\n\n  if (GVar->isDeclaration()) {\n    // (extern) declarations, no definition or initializer\n    // Currently the only known declaration is for an automatic __local\n    // (.shared) promoted to global.\n    emitPTXGlobalVariable(GVar, O);\n    O << \";\\n\";\n    return;\n  }\n\n  if (llvm::isSampler(*GVar)) {\n    O << \".global .samplerref \" << llvm::getSamplerName(*GVar);\n\n    const Constant *Initializer = nullptr;\n    if (GVar->hasInitializer())\n      Initializer = GVar->getInitializer();\n    const ConstantInt *CI = nullptr;\n    if (Initializer)\n      CI = dyn_cast<ConstantInt>(Initializer);\n    if (CI) {\n      unsigned sample = CI->getZExtValue();\n\n      O << \" = { \";\n\n      for (int i = 0,\n               addr = ((sample & __CLK_ADDRESS_MASK) >> __CLK_ADDRESS_BASE);\n           i < 3; i++) {\n        O << \"addr_mode_\" << i << \" = \";\n        switch (addr) {\n        case 0:\n          O << \"wrap\";\n          break;\n        case 1:\n          O << \"clamp_to_border\";\n          break;\n        case 2:\n          O << \"clamp_to_edge\";\n          break;\n        case 3:\n          O << \"wrap\";\n          break;\n        case 4:\n          O << \"mirror\";\n          break;\n        }\n        O << \", \";\n      }\n      O << \"filter_mode = \";\n      switch ((sample & __CLK_FILTER_MASK) >> __CLK_FILTER_BASE) {\n      case 0:\n        O << \"nearest\";\n        break;\n      case 1:\n        O << \"linear\";\n        break;\n      case 2:\n        llvm_unreachable(\"Anisotropic filtering is not supported\");\n      default:\n        O << \"nearest\";\n        break;\n      }\n      if (!((sample & __CLK_NORMALIZED_MASK) >> __CLK_NORMALIZED_BASE)) {\n        O << \", force_unnormalized_coords = 1\";\n      }\n      O << \" }\";\n    }\n\n    O << \";\\n\";\n    return;\n  }\n\n  if (GVar->hasPrivateLinkage()) {\n\n    if (!strncmp(GVar->getName().data(), \"unrollpragma\", 12))\n      return;\n\n    // FIXME - need better way (e.g. Metadata) to avoid generating this global\n    if (!strncmp(GVar->getName().data(), \"filename\", 8))\n      return;\n    if (GVar->use_empty())\n      return;\n  }\n\n  const Function *demotedFunc = nullptr;\n  if (!processDemoted && canDemoteGlobalVar(GVar, demotedFunc)) {\n    O << \"// \" << GVar->getName() << \" has been demoted\\n\";\n    if (localDecls.find(demotedFunc) != localDecls.end())\n      localDecls[demotedFunc].push_back(GVar);\n    else {\n      std::vector<const GlobalVariable *> temp;\n      temp.push_back(GVar);\n      localDecls[demotedFunc] = temp;\n    }\n    return;\n  }\n\n  O << \".\";\n  emitPTXAddressSpace(PTy->getAddressSpace(), O);\n\n  if (isManaged(*GVar)) {\n    O << \" .attribute(.managed)\";\n  }\n\n  if (GVar->getAlignment() == 0)\n    O << \" .align \" << (int)DL.getPrefTypeAlignment(ETy);\n  else\n    O << \" .align \" << GVar->getAlignment();\n\n  if (ETy->isFloatingPointTy() || ETy->isIntegerTy() || ETy->isPointerTy()) {\n    O << \" .\";\n    // Special case: ABI requires that we use .u8 for predicates\n    if (ETy->isIntegerTy(1))\n      O << \"u8\";\n    else\n      O << getPTXFundamentalTypeStr(ETy, false);\n    O << \" \";\n    getSymbol(GVar)->print(O, MAI);\n\n    // Ptx allows variable initilization only for constant and global state\n    // spaces.\n    if (GVar->hasInitializer()) {\n      if ((PTy->getAddressSpace() == llvm::ADDRESS_SPACE_GLOBAL) ||\n          (PTy->getAddressSpace() == llvm::ADDRESS_SPACE_CONST)) {\n        const Constant *Initializer = GVar->getInitializer();\n        // 'undef' is treated as there is no value specified.\n        if (!Initializer->isNullValue() && !isa<UndefValue>(Initializer)) {\n          O << \" = \";\n          printScalarConstant(Initializer, O);\n        }\n      } else {\n        // The frontend adds zero-initializer to device and constant variables\n        // that don't have an initial value, and UndefValue to shared\n        // variables, so skip warning for this case.\n        if (!GVar->getInitializer()->isNullValue() &&\n            !isa<UndefValue>(GVar->getInitializer())) {\n          report_fatal_error(\"initial value of '\" + GVar->getName() +\n                             \"' is not allowed in addrspace(\" +\n                             Twine(PTy->getAddressSpace()) + \")\");\n        }\n      }\n    }\n  } else {\n    unsigned int ElementSize = 0;\n\n    // Although PTX has direct support for struct type and array type and\n    // LLVM IR is very similar to PTX, the LLVM CodeGen does not support for\n    // targets that support these high level field accesses. Structs, arrays\n    // and vectors are lowered into arrays of bytes.\n    switch (ETy->getTypeID()) {\n    case Type::StructTyID:\n    case Type::ArrayTyID:\n    case Type::VectorTyID:\n      ElementSize = DL.getTypeStoreSize(ETy);\n      // Ptx allows variable initilization only for constant and\n      // global state spaces.\n      if (((PTy->getAddressSpace() == llvm::ADDRESS_SPACE_GLOBAL) ||\n           (PTy->getAddressSpace() == llvm::ADDRESS_SPACE_CONST)) &&\n          GVar->hasInitializer()) {\n        const Constant *Initializer = GVar->getInitializer();\n        if (!isa<UndefValue>(Initializer) && !Initializer->isNullValue()) {\n          AggBuffer aggBuffer(ElementSize, O, *this);\n          bufferAggregateConstant(Initializer, &aggBuffer);\n          if (aggBuffer.numSymbols) {\n            if (static_cast<const NVPTXTargetMachine &>(TM).is64Bit()) {\n              O << \" .u64 \";\n              getSymbol(GVar)->print(O, MAI);\n              O << \"[\";\n              O << ElementSize / 8;\n            } else {\n              O << \" .u32 \";\n              getSymbol(GVar)->print(O, MAI);\n              O << \"[\";\n              O << ElementSize / 4;\n            }\n            O << \"]\";\n          } else {\n            O << \" .b8 \";\n            getSymbol(GVar)->print(O, MAI);\n            O << \"[\";\n            O << ElementSize;\n            O << \"]\";\n          }\n          O << \" = {\";\n          aggBuffer.print();\n          O << \"}\";\n        } else {\n          O << \" .b8 \";\n          getSymbol(GVar)->print(O, MAI);\n          if (ElementSize) {\n            O << \"[\";\n            O << ElementSize;\n            O << \"]\";\n          }\n        }\n      } else {\n        O << \" .b8 \";\n        getSymbol(GVar)->print(O, MAI);\n        if (ElementSize) {\n          O << \"[\";\n          O << ElementSize;\n          O << \"]\";\n        }\n      }\n      break;\n    default:\n      llvm_unreachable(\"type not supported yet\");\n    }\n\n  }\n  O << \";\\n\";\n}\n\n"},"LLVMInitializeNVPTXAsmPrinter":{"range":[2370,2374],"code":"extern \"C\" void LLVMInitializeNVPTXAsmPrinter() {\n  RegisterAsmPrinter<NVPTXAsmPrinter> X(TheNVPTXTarget32);\n  RegisterAsmPrinter<NVPTXAsmPrinter> Y(TheNVPTXTarget64);\n}\n"},"emitGlobals":{"range":[879,910],"code":"void NVPTXAsmPrinter::emitGlobals(const Module &M) {\n  SmallString<128> Str2;\n  raw_svector_ostream OS2(Str2);\n\n  emitDeclarations(M, OS2);\n\n  // As ptxas does not support forward references of globals, we need to first\n  // sort the list of module-level globals in def-use order. We visit each\n  // global variable in order, and ensure that we emit it *after* its dependent\n  // globals. We use a little extra memory maintaining both a set and a list to\n  // have fast searches while maintaining a strict ordering.\n  SmallVector<const GlobalVariable *, 8> Globals;\n  DenseSet<const GlobalVariable *> GVVisited;\n  DenseSet<const GlobalVariable *> GVVisiting;\n\n  // Visit each global variable, in order\n  for (const GlobalVariable &I : M.globals())\n    VisitGlobalVariableForEmission(&I, Globals, GVVisited, GVVisiting);\n\n  assert(GVVisited.size() == M.getGlobalList().size() &&\n         \"Missed a global variable\");\n  assert(GVVisiting.size() == 0 && \"Did not fully process a global variable\");\n\n  // Print out module-level global variables in proper order\n  for (unsigned i = 0, e = Globals.size(); i != e; ++i)\n    printModuleLevelGV(Globals[i], OS2);\n\n  OS2 << '\\n';\n\n  OutStreamer->EmitRawText(OS2.str());\n}\n\n"},"ConvertIntToBytes":{"range":[1785,1791],"code":"template <typename T> static void ConvertIntToBytes(unsigned char *p, T val) {\n  int64_t vp = (int64_t)val;\n  for (unsigned i = 0; i < sizeof(T); ++i) {\n    p[i] = (unsigned char)vp;\n    vp >>= 8;\n  }\n}\n"},"lowerOperand":{"range":[258,298],"code":"bool NVPTXAsmPrinter::lowerOperand(const MachineOperand &MO,\n                                   MCOperand &MCOp) {\n  switch (MO.getType()) {\n  default: llvm_unreachable(\"unknown operand type\");\n  case MachineOperand::MO_Register:\n    MCOp = MCOperand::createReg(encodeVirtualRegister(MO.getReg()));\n    break;\n  case MachineOperand::MO_Immediate:\n    MCOp = MCOperand::createImm(MO.getImm());\n    break;\n  case MachineOperand::MO_MachineBasicBlock:\n    MCOp = MCOperand::createExpr(MCSymbolRefExpr::create(\n        MO.getMBB()->getSymbol(), OutContext));\n    break;\n  case MachineOperand::MO_ExternalSymbol:\n    MCOp = GetSymbolRef(GetExternalSymbolSymbol(MO.getSymbolName()));\n    break;\n  case MachineOperand::MO_GlobalAddress:\n    MCOp = GetSymbolRef(getSymbol(MO.getGlobal()));\n    break;\n  case MachineOperand::MO_FPImmediate: {\n    const ConstantFP *Cnt = MO.getFPImm();\n    const APFloat &Val = Cnt->getValueAPF();\n\n    switch (Cnt->getType()->getTypeID()) {\n    default: report_fatal_error(\"Unsupported FP type\"); break;\n    case Type::FloatTyID:\n      MCOp = MCOperand::createExpr(\n        NVPTXFloatMCExpr::createConstantFPSingle(Val, OutContext));\n      break;\n    case Type::DoubleTyID:\n      MCOp = MCOperand::createExpr(\n        NVPTXFloatMCExpr::createConstantFPDouble(Val, OutContext));\n      break;\n    }\n    break;\n  }\n  }\n  return true;\n}\n\n"},"canDemoteGlobalVar":{"range":[681,698],"code":"static bool canDemoteGlobalVar(const GlobalVariable *gv, Function const *&f) {\n  if (!gv->hasInternalLinkage())\n    return false;\n  PointerType *Pty = gv->getType();\n  if (Pty->getAddressSpace() != llvm::ADDRESS_SPACE_SHARED)\n    return false;\n\n  const Function *oneFunc = nullptr;\n\n  bool flag = usedInOneFunc(gv, oneFunc);\n  if (!flag)\n    return false;\n  if (!oneFunc)\n    return false;\n  f = oneFunc;\n  return true;\n}\n\n"},"emitLinkageDirective":{"range":[1007,1037],"code":"void NVPTXAsmPrinter::emitLinkageDirective(const GlobalValue *V,\n                                           raw_ostream &O) {\n  if (static_cast<NVPTXTargetMachine &>(TM).getDrvInterface() == NVPTX::CUDA) {\n    if (V->hasExternalLinkage()) {\n      if (isa<GlobalVariable>(V)) {\n        const GlobalVariable *GVar = cast<GlobalVariable>(V);\n        if (GVar) {\n          if (GVar->hasInitializer())\n            O << \".visible \";\n          else\n            O << \".extern \";\n        }\n      } else if (V->isDeclaration())\n        O << \".extern \";\n      else\n        O << \".visible \";\n    } else if (V->hasAppendingLinkage()) {\n      std::string msg;\n      msg.append(\"Error: \");\n      msg.append(\"Symbol \");\n      if (V->hasName())\n        msg.append(V->getName());\n      msg.append(\"has unsupported appending linkage type\");\n      llvm_unreachable(msg.c_str());\n    } else if (!V->hasInternalLinkage() &&\n               !V->hasPrivateLinkage()) {\n      O << \".weak \";\n    }\n  }\n}\n\n"},"ConvertDoubleToBytes":{"range":[1799,1806],"code":"static void ConvertDoubleToBytes(unsigned char *p, double val) {\n  int64_t *vp = (int64_t *)&val;\n  for (unsigned i = 0; i < sizeof(int64_t); ++i) {\n    p[i] = (unsigned char)*vp;\n    *vp >>= 8;\n  }\n}\n\n"},"getOpenCLAlignment":{"range":[1410,1437],"code":"static unsigned int getOpenCLAlignment(const DataLayout &DL, Type *Ty) {\n  if (Ty->isSingleValueType())\n    return DL.getPrefTypeAlignment(Ty);\n\n  auto *ATy = dyn_cast<ArrayType>(Ty);\n  if (ATy)\n    return getOpenCLAlignment(DL, ATy->getElementType());\n\n  auto *STy = dyn_cast<StructType>(Ty);\n  if (STy) {\n    unsigned int alignStruct = 1;\n    // Go through each element of the struct and find the\n    // largest alignment.\n    for (unsigned i = 0, e = STy->getNumElements(); i != e; i++) {\n      Type *ETy = STy->getElementType(i);\n      unsigned int align = getOpenCLAlignment(DL, ETy);\n      if (align > alignStruct)\n        alignStruct = align;\n    }\n    return alignStruct;\n  }\n\n  auto *FTy = dyn_cast<FunctionType>(Ty);\n  if (FTy)\n    return DL.getPointerPrefAlignment();\n  return DL.getPrefTypeAlignment(Ty);\n}\n\n"},"bufferAggregateConstant":{"range":[1930,1974],"code":"void NVPTXAsmPrinter::bufferAggregateConstant(const Constant *CPV,\n                                              AggBuffer *aggBuffer) {\n  const DataLayout &DL = getDataLayout();\n  int Bytes;\n\n  // Old constants\n  if (isa<ConstantArray>(CPV) || isa<ConstantVector>(CPV)) {\n    if (CPV->getNumOperands())\n      for (unsigned i = 0, e = CPV->getNumOperands(); i != e; ++i)\n        bufferLEByte(cast<Constant>(CPV->getOperand(i)), 0, aggBuffer);\n    return;\n  }\n\n  if (const ConstantDataSequential *CDS =\n          dyn_cast<ConstantDataSequential>(CPV)) {\n    if (CDS->getNumElements())\n      for (unsigned i = 0; i < CDS->getNumElements(); ++i)\n        bufferLEByte(cast<Constant>(CDS->getElementAsConstant(i)), 0,\n                     aggBuffer);\n    return;\n  }\n\n  if (isa<ConstantStruct>(CPV)) {\n    if (CPV->getNumOperands()) {\n      StructType *ST = cast<StructType>(CPV->getType());\n      for (unsigned i = 0, e = CPV->getNumOperands(); i != e; ++i) {\n        if (i == (e - 1))\n          Bytes = DL.getStructLayout(ST)->getElementOffset(0) +\n                  DL.getTypeAllocSize(ST) -\n                  DL.getStructLayout(ST)->getElementOffset(i);\n        else\n          Bytes = DL.getStructLayout(ST)->getElementOffset(i + 1) -\n                  DL.getStructLayout(ST)->getElementOffset(i);\n        bufferLEByte(cast<Constant>(CPV->getOperand(i)), Bytes, aggBuffer);\n      }\n    }\n    return;\n  }\n  llvm_unreachable(\"unsupported constant type in printAggregateConstant()\");\n}\n\n// buildTypeNameMap - Run through symbol table looking for type names.\n//\n\n\n"},"bufferLEByte":{"range":[1807,1929],"code":"void NVPTXAsmPrinter::bufferLEByte(const Constant *CPV, int Bytes,\n                                   AggBuffer *aggBuffer) {\n\n  const DataLayout &DL = getDataLayout();\n\n  if (isa<UndefValue>(CPV) || CPV->isNullValue()) {\n    int s = DL.getTypeAllocSize(CPV->getType());\n    if (s < Bytes)\n      s = Bytes;\n    aggBuffer->addZeros(s);\n    return;\n  }\n\n  unsigned char ptr[8];\n  switch (CPV->getType()->getTypeID()) {\n\n  case Type::IntegerTyID: {\n    Type *ETy = CPV->getType();\n    if (ETy == Type::getInt8Ty(CPV->getContext())) {\n      unsigned char c = (unsigned char)cast<ConstantInt>(CPV)->getZExtValue();\n      ConvertIntToBytes<>(ptr, c);\n      aggBuffer->addBytes(ptr, 1, Bytes);\n    } else if (ETy == Type::getInt16Ty(CPV->getContext())) {\n      short int16 = (short)cast<ConstantInt>(CPV)->getZExtValue();\n      ConvertIntToBytes<>(ptr, int16);\n      aggBuffer->addBytes(ptr, 2, Bytes);\n    } else if (ETy == Type::getInt32Ty(CPV->getContext())) {\n      if (const ConstantInt *constInt = dyn_cast<ConstantInt>(CPV)) {\n        int int32 = (int)(constInt->getZExtValue());\n        ConvertIntToBytes<>(ptr, int32);\n        aggBuffer->addBytes(ptr, 4, Bytes);\n        break;\n      } else if (const ConstantExpr *Cexpr = dyn_cast<ConstantExpr>(CPV)) {\n        if (const ConstantInt *constInt = dyn_cast<ConstantInt>(\n                ConstantFoldConstantExpression(Cexpr, DL))) {\n          int int32 = (int)(constInt->getZExtValue());\n          ConvertIntToBytes<>(ptr, int32);\n          aggBuffer->addBytes(ptr, 4, Bytes);\n          break;\n        }\n        if (Cexpr->getOpcode() == Instruction::PtrToInt) {\n          Value *v = Cexpr->getOperand(0)->stripPointerCasts();\n          aggBuffer->addSymbol(v, Cexpr->getOperand(0));\n          aggBuffer->addZeros(4);\n          break;\n        }\n      }\n      llvm_unreachable(\"unsupported integer const type\");\n    } else if (ETy == Type::getInt64Ty(CPV->getContext())) {\n      if (const ConstantInt *constInt = dyn_cast<ConstantInt>(CPV)) {\n        long long int64 = (long long)(constInt->getZExtValue());\n        ConvertIntToBytes<>(ptr, int64);\n        aggBuffer->addBytes(ptr, 8, Bytes);\n        break;\n      } else if (const ConstantExpr *Cexpr = dyn_cast<ConstantExpr>(CPV)) {\n        if (const ConstantInt *constInt = dyn_cast<ConstantInt>(\n                ConstantFoldConstantExpression(Cexpr, DL))) {\n          long long int64 = (long long)(constInt->getZExtValue());\n          ConvertIntToBytes<>(ptr, int64);\n          aggBuffer->addBytes(ptr, 8, Bytes);\n          break;\n        }\n        if (Cexpr->getOpcode() == Instruction::PtrToInt) {\n          Value *v = Cexpr->getOperand(0)->stripPointerCasts();\n          aggBuffer->addSymbol(v, Cexpr->getOperand(0));\n          aggBuffer->addZeros(8);\n          break;\n        }\n      }\n      llvm_unreachable(\"unsupported integer const type\");\n    } else\n      llvm_unreachable(\"unsupported integer const type\");\n    break;\n  }\n  case Type::FloatTyID:\n  case Type::DoubleTyID: {\n    const ConstantFP *CFP = dyn_cast<ConstantFP>(CPV);\n    Type *Ty = CFP->getType();\n    if (Ty == Type::getFloatTy(CPV->getContext())) {\n      float float32 = (float) CFP->getValueAPF().convertToFloat();\n      ConvertFloatToBytes(ptr, float32);\n      aggBuffer->addBytes(ptr, 4, Bytes);\n    } else if (Ty == Type::getDoubleTy(CPV->getContext())) {\n      double float64 = CFP->getValueAPF().convertToDouble();\n      ConvertDoubleToBytes(ptr, float64);\n      aggBuffer->addBytes(ptr, 8, Bytes);\n    } else {\n      llvm_unreachable(\"unsupported fp const type\");\n    }\n    break;\n  }\n  case Type::PointerTyID: {\n    if (const GlobalValue *GVar = dyn_cast<GlobalValue>(CPV)) {\n      aggBuffer->addSymbol(GVar, GVar);\n    } else if (const ConstantExpr *Cexpr = dyn_cast<ConstantExpr>(CPV)) {\n      const Value *v = Cexpr->stripPointerCasts();\n      aggBuffer->addSymbol(v, Cexpr);\n    }\n    unsigned int s = DL.getTypeAllocSize(CPV->getType());\n    aggBuffer->addZeros(s);\n    break;\n  }\n\n  case Type::ArrayTyID:\n  case Type::VectorTyID:\n  case Type::StructTyID: {\n    if (isa<ConstantAggregate>(CPV) || isa<ConstantDataSequential>(CPV)) {\n      int ElementSize = DL.getTypeAllocSize(CPV->getType());\n      bufferAggregateConstant(CPV, aggBuffer);\n      if (Bytes > ElementSize)\n        aggBuffer->addZeros(Bytes - ElementSize);\n    } else if (isa<ConstantAggregateZero>(CPV))\n      aggBuffer->addZeros(Bytes);\n    else\n      llvm_unreachable(\"Unexpected Constant type\");\n    break;\n  }\n\n  default:\n    llvm_unreachable(\"unsupported type\");\n  }\n}\n\n"},"usedInOneFunc":{"range":[650,680],"code":"static bool usedInOneFunc(const User *U, Function const *&oneFunc) {\n  if (const GlobalVariable *othergv = dyn_cast<GlobalVariable>(U)) {\n    if (othergv->getName() == \"llvm.used\")\n      return true;\n  }\n\n  if (const Instruction *instr = dyn_cast<Instruction>(U)) {\n    if (instr->getParent() && instr->getParent()->getParent()) {\n      const Function *curFunc = instr->getParent()->getParent();\n      if (oneFunc && (curFunc != oneFunc))\n        return false;\n      oneFunc = curFunc;\n      return true;\n    } else\n      return false;\n  }\n\n  for (const User *UU : U->users())\n    if (!usedInOneFunc(UU, oneFunc))\n      return false;\n\n  return true;\n}\n\n/* Find out if a global variable can be demoted to local scope.\n * Currently, this is valid for CUDA shared variables, which have local\n * scope and global lifetime. So the conditions to check are :\n * 1. Is the global variable in shared address space?\n * 2. Does it have internal linkage?\n * 3. Is the global variable referenced only in one function?\n */\n"},"emitPTXAddressSpace":{"range":[1303,1324],"code":"void NVPTXAsmPrinter::emitPTXAddressSpace(unsigned int AddressSpace,\n                                          raw_ostream &O) const {\n  switch (AddressSpace) {\n  case llvm::ADDRESS_SPACE_LOCAL:\n    O << \"local\";\n    break;\n  case llvm::ADDRESS_SPACE_GLOBAL:\n    O << \"global\";\n    break;\n  case llvm::ADDRESS_SPACE_CONST:\n    O << \"const\";\n    break;\n  case llvm::ADDRESS_SPACE_SHARED:\n    O << \"shared\";\n    break;\n  default:\n    report_fatal_error(\"Bad address space found while emitting PTX\");\n    break;\n  }\n}\n\nstd::string\n"},"emitKernelFunctionDirectives":{"range":[511,562],"code":"void NVPTXAsmPrinter::emitKernelFunctionDirectives(const Function &F,\n                                                   raw_ostream &O) const {\n  // If the NVVM IR has some of reqntid* specified, then output\n  // the reqntid directive, and set the unspecified ones to 1.\n  // If none of reqntid* is specified, don't output reqntid directive.\n  unsigned reqntidx, reqntidy, reqntidz;\n  bool specified = false;\n  if (!llvm::getReqNTIDx(F, reqntidx))\n    reqntidx = 1;\n  else\n    specified = true;\n  if (!llvm::getReqNTIDy(F, reqntidy))\n    reqntidy = 1;\n  else\n    specified = true;\n  if (!llvm::getReqNTIDz(F, reqntidz))\n    reqntidz = 1;\n  else\n    specified = true;\n\n  if (specified)\n    O << \".reqntid \" << reqntidx << \", \" << reqntidy << \", \" << reqntidz\n      << \"\\n\";\n\n  // If the NVVM IR has some of maxntid* specified, then output\n  // the maxntid directive, and set the unspecified ones to 1.\n  // If none of maxntid* is specified, don't output maxntid directive.\n  unsigned maxntidx, maxntidy, maxntidz;\n  specified = false;\n  if (!llvm::getMaxNTIDx(F, maxntidx))\n    maxntidx = 1;\n  else\n    specified = true;\n  if (!llvm::getMaxNTIDy(F, maxntidy))\n    maxntidy = 1;\n  else\n    specified = true;\n  if (!llvm::getMaxNTIDz(F, maxntidz))\n    maxntidz = 1;\n  else\n    specified = true;\n\n  if (specified)\n    O << \".maxntid \" << maxntidx << \", \" << maxntidy << \", \" << maxntidz\n      << \"\\n\";\n\n  unsigned mincta;\n  if (llvm::getMinCTASm(F, mincta))\n    O << \".minnctapersm \" << mincta << \"\\n\";\n}\n\nstd::string\n"},"lowerConstantForGV":{"range":[2036,2166],"code":"NVPTXAsmPrinter::lowerConstantForGV(const Constant *CV, bool ProcessingGeneric) {\n  MCContext &Ctx = OutContext;\n\n  if (CV->isNullValue() || isa<UndefValue>(CV))\n    return MCConstantExpr::create(0, Ctx);\n\n  if (const ConstantInt *CI = dyn_cast<ConstantInt>(CV))\n    return MCConstantExpr::create(CI->getZExtValue(), Ctx);\n\n  if (const GlobalValue *GV = dyn_cast<GlobalValue>(CV)) {\n    const MCSymbolRefExpr *Expr =\n      MCSymbolRefExpr::create(getSymbol(GV), Ctx);\n    if (ProcessingGeneric) {\n      return NVPTXGenericMCSymbolRefExpr::create(Expr, Ctx);\n    } else {\n      return Expr;\n    }\n  }\n\n  const ConstantExpr *CE = dyn_cast<ConstantExpr>(CV);\n  if (!CE) {\n    llvm_unreachable(\"Unknown constant value to lower!\");\n  }\n\n  switch (CE->getOpcode()) {\n  default:\n    // If the code isn't optimized, there may be outstanding folding\n    // opportunities. Attempt to fold the expression using DataLayout as a\n    // last resort before giving up.\n    if (Constant *C = ConstantFoldConstantExpression(CE, getDataLayout()))\n      if (C != CE)\n        return lowerConstantForGV(C, ProcessingGeneric);\n\n    // Otherwise report the problem to the user.\n    {\n      std::string S;\n      raw_string_ostream OS(S);\n      OS << \"Unsupported expression in static initializer: \";\n      CE->printAsOperand(OS, /*PrintType=*/false,\n                     !MF ? nullptr : MF->getFunction()->getParent());\n      report_fatal_error(OS.str());\n    }\n\n  case Instruction::AddrSpaceCast: {\n    // Strip the addrspacecast and pass along the operand\n    PointerType *DstTy = cast<PointerType>(CE->getType());\n    if (DstTy->getAddressSpace() == 0) {\n      return lowerConstantForGV(cast<const Constant>(CE->getOperand(0)), true);\n    }\n    std::string S;\n    raw_string_ostream OS(S);\n    OS << \"Unsupported expression in static initializer: \";\n    CE->printAsOperand(OS, /*PrintType=*/ false,\n                       !MF ? 0 : MF->getFunction()->getParent());\n    report_fatal_error(OS.str());\n  }\n\n  case Instruction::GetElementPtr: {\n    const DataLayout &DL = getDataLayout();\n\n    // Generate a symbolic expression for the byte address\n    APInt OffsetAI(DL.getPointerTypeSizeInBits(CE->getType()), 0);\n    cast<GEPOperator>(CE)->accumulateConstantOffset(DL, OffsetAI);\n\n    const MCExpr *Base = lowerConstantForGV(CE->getOperand(0),\n                                            ProcessingGeneric);\n    if (!OffsetAI)\n      return Base;\n\n    int64_t Offset = OffsetAI.getSExtValue();\n    return MCBinaryExpr::createAdd(Base, MCConstantExpr::create(Offset, Ctx),\n                                   Ctx);\n  }\n\n  case Instruction::Trunc:\n    // We emit the value and depend on the assembler to truncate the generated\n    // expression properly.  This is important for differences between\n    // blockaddress labels.  Since the two labels are in the same function, it\n    // is reasonable to treat their delta as a 32-bit value.\n    // FALL THROUGH.\n  case Instruction::BitCast:\n    return lowerConstantForGV(CE->getOperand(0), ProcessingGeneric);\n\n  case Instruction::IntToPtr: {\n    const DataLayout &DL = getDataLayout();\n\n    // Handle casts to pointers by changing them into casts to the appropriate\n    // integer type.  This promotes constant folding and simplifies this code.\n    Constant *Op = CE->getOperand(0);\n    Op = ConstantExpr::getIntegerCast(Op, DL.getIntPtrType(CV->getType()),\n                                      false/*ZExt*/);\n    return lowerConstantForGV(Op, ProcessingGeneric);\n  }\n\n  case Instruction::PtrToInt: {\n    const DataLayout &DL = getDataLayout();\n\n    // Support only foldable casts to/from pointers that can be eliminated by\n    // changing the pointer to the appropriately sized integer type.\n    Constant *Op = CE->getOperand(0);\n    Type *Ty = CE->getType();\n\n    const MCExpr *OpExpr = lowerConstantForGV(Op, ProcessingGeneric);\n\n    // We can emit the pointer value into this slot if the slot is an\n    // integer slot equal to the size of the pointer.\n    if (DL.getTypeAllocSize(Ty) == DL.getTypeAllocSize(Op->getType()))\n      return OpExpr;\n\n    // Otherwise the pointer is smaller than the resultant integer, mask off\n    // the high bits so we are sure to get a proper truncation if the input is\n    // a constant expr.\n    unsigned InBits = DL.getTypeAllocSizeInBits(Op->getType());\n    const MCExpr *MaskExpr = MCConstantExpr::create(~0ULL >> (64-InBits), Ctx);\n    return MCBinaryExpr::createAnd(OpExpr, MaskExpr, Ctx);\n  }\n\n  // The MC library also has a right-shift operator, but it isn't consistently\n  // signed or unsigned between different targets.\n  case Instruction::Add: {\n    const MCExpr *LHS = lowerConstantForGV(CE->getOperand(0), ProcessingGeneric);\n    const MCExpr *RHS = lowerConstantForGV(CE->getOperand(1), ProcessingGeneric);\n    switch (CE->getOpcode()) {\n    default: llvm_unreachable(\"Unknown binary operator constant cast expr\");\n    case Instruction::Add: return MCBinaryExpr::createAdd(LHS, RHS, Ctx);\n    }\n  }\n  }\n}\n\n// Copy of MCExpr::print customized for NVPTX\n"},"EmitBasicBlockStart":{"range":[445,450],"code":"void NVPTXAsmPrinter::EmitBasicBlockStart(const MachineBasicBlock &MBB) const {\n  AsmPrinter::EmitBasicBlockStart(MBB);\n  if (isLoopHeaderOfNoUnroll(MBB))\n    OutStreamer->EmitRawText(StringRef(\"\\t.pragma \\\"nounroll\\\";\\n\"));\n}\n\n"},"printMemOperand":{"range":[2315,2330],"code":"void NVPTXAsmPrinter::printMemOperand(const MachineInstr *MI, int opNum,\n                                      raw_ostream &O, const char *Modifier) {\n  printOperand(MI, opNum, O);\n\n  if (Modifier && !strcmp(Modifier, \"add\")) {\n    O << \", \";\n    printOperand(MI, opNum + 1, O);\n  } else {\n    if (MI->getOperand(opNum + 1).isImm() &&\n        MI->getOperand(opNum + 1).getImm() == 0)\n      return; // don't print ',0' or '+0'\n    O << \"+\";\n    printOperand(MI, opNum + 1, O);\n  }\n}\n\n"},"emitPTXGlobalVariable":{"range":[1362,1409],"code":"void NVPTXAsmPrinter::emitPTXGlobalVariable(const GlobalVariable *GVar,\n                                            raw_ostream &O) {\n\n  const DataLayout &DL = getDataLayout();\n\n  // GlobalVariables are always constant pointers themselves.\n  Type *ETy = GVar->getValueType();\n\n  O << \".\";\n  emitPTXAddressSpace(GVar->getType()->getAddressSpace(), O);\n  if (GVar->getAlignment() == 0)\n    O << \" .align \" << (int)DL.getPrefTypeAlignment(ETy);\n  else\n    O << \" .align \" << GVar->getAlignment();\n\n  if (ETy->isFloatingPointTy() || ETy->isIntegerTy() || ETy->isPointerTy()) {\n    O << \" .\";\n    O << getPTXFundamentalTypeStr(ETy);\n    O << \" \";\n    getSymbol(GVar)->print(O, MAI);\n    return;\n  }\n\n  int64_t ElementSize = 0;\n\n  // Although PTX has direct support for struct type and array type and LLVM IR\n  // is very similar to PTX, the LLVM CodeGen does not support for targets that\n  // support these high level field accesses. Structs and arrays are lowered\n  // into arrays of bytes.\n  switch (ETy->getTypeID()) {\n  case Type::StructTyID:\n  case Type::ArrayTyID:\n  case Type::VectorTyID:\n    ElementSize = DL.getTypeStoreSize(ETy);\n    O << \" .b8 \";\n    getSymbol(GVar)->print(O, MAI);\n    O << \"[\";\n    if (ElementSize) {\n      O << ElementSize;\n    }\n    O << \"]\";\n    break;\n  default:\n    llvm_unreachable(\"type not supported yet\");\n  }\n  return;\n}\n\n"},"doFinalization":{"range":[947,1006],"code":"bool NVPTXAsmPrinter::doFinalization(Module &M) {\n  // If we did not emit any functions, then the global declarations have not\n  // yet been emitted.\n  if (!GlobalsEmitted) {\n    emitGlobals(M);\n    GlobalsEmitted = true;\n  }\n\n  // XXX Temproarily remove global variables so that doFinalization() will not\n  // emit them again (global variables are emitted at beginning).\n\n  Module::GlobalListType &global_list = M.getGlobalList();\n  int i, n = global_list.size();\n  GlobalVariable **gv_array = new GlobalVariable *[n];\n\n  // first, back-up GlobalVariable in gv_array\n  i = 0;\n  for (Module::global_iterator I = global_list.begin(), E = global_list.end();\n       I != E; ++I)\n    gv_array[i++] = &*I;\n\n  // second, empty global_list\n  while (!global_list.empty())\n    global_list.remove(global_list.begin());\n\n  // call doFinalization\n  bool ret = AsmPrinter::doFinalization(M);\n\n  // now we restore global variables\n  for (i = 0; i < n; i++)\n    global_list.insert(global_list.end(), gv_array[i]);\n\n  clearAnnotationCache(&M);\n\n  delete[] gv_array;\n  return ret;\n\n  //bool Result = AsmPrinter::doFinalization(M);\n  // Instead of calling the parents doFinalization, we may\n  // clone parents doFinalization and customize here.\n  // Currently, we if NVISA out the EmitGlobals() in\n  // parent's doFinalization, which is too intrusive.\n  //\n  // Same for the doInitialization.\n  //return Result;\n}\n\n// This function emits appropriate linkage directives for\n// functions and global variables.\n//\n// extern function declaration            -> .extern\n// extern function definition             -> .visible\n// external global variable with init     -> .visible\n// external without init                  -> .extern\n// appending                              -> not allowed, assert.\n// for any linkage other than\n// internal, private, linker_private,\n// linker_private_weak, linker_private_weak_def_auto,\n// we emit                                -> .weak.\n\n"},"emitDeclaration":{"range":[620,633],"code":"void NVPTXAsmPrinter::emitDeclaration(const Function *F, raw_ostream &O) {\n\n  emitLinkageDirective(F, O);\n  if (llvm::isKernelFunction(*F))\n    O << \".entry \";\n  else\n    O << \".func \";\n  printReturnValStr(F, O);\n  getSymbol(F)->print(O, MAI);\n  O << \"\\n\";\n  emitFunctionParamList(F, O);\n  O << \";\\n\";\n}\n\n"},"EmitFunctionBodyEnd":{"range":[494,498],"code":"void NVPTXAsmPrinter::EmitFunctionBodyEnd() {\n  OutStreamer->EmitRawText(StringRef(\"}\\n\"));\n  VRegMapping.clear();\n}\n\n"},"ignoreLoc":{"range":[1975,2035],"code":"bool NVPTXAsmPrinter::ignoreLoc(const MachineInstr &MI) {\n  switch (MI.getOpcode()) {\n  default:\n    return false;\n  case NVPTX::CallArgBeginInst:\n  case NVPTX::CallArgEndInst0:\n  case NVPTX::CallArgEndInst1:\n  case NVPTX::CallArgF32:\n  case NVPTX::CallArgF64:\n  case NVPTX::CallArgI16:\n  case NVPTX::CallArgI32:\n  case NVPTX::CallArgI32imm:\n  case NVPTX::CallArgI64:\n  case NVPTX::CallArgParam:\n  case NVPTX::CallVoidInst:\n  case NVPTX::CallVoidInstReg:\n  case NVPTX::Callseq_End:\n  case NVPTX::CallVoidInstReg64:\n  case NVPTX::DeclareParamInst:\n  case NVPTX::DeclareRetMemInst:\n  case NVPTX::DeclareRetRegInst:\n  case NVPTX::DeclareRetScalarInst:\n  case NVPTX::DeclareScalarParamInst:\n  case NVPTX::DeclareScalarRegInst:\n  case NVPTX::StoreParamF32:\n  case NVPTX::StoreParamF64:\n  case NVPTX::StoreParamI16:\n  case NVPTX::StoreParamI32:\n  case NVPTX::StoreParamI64:\n  case NVPTX::StoreParamI8:\n  case NVPTX::StoreRetvalF32:\n  case NVPTX::StoreRetvalF64:\n  case NVPTX::StoreRetvalI16:\n  case NVPTX::StoreRetvalI32:\n  case NVPTX::StoreRetvalI64:\n  case NVPTX::StoreRetvalI8:\n  case NVPTX::LastCallArgF32:\n  case NVPTX::LastCallArgF64:\n  case NVPTX::LastCallArgI16:\n  case NVPTX::LastCallArgI32:\n  case NVPTX::LastCallArgI32imm:\n  case NVPTX::LastCallArgI64:\n  case NVPTX::LastCallArgParam:\n  case NVPTX::LoadParamMemF32:\n  case NVPTX::LoadParamMemF64:\n  case NVPTX::LoadParamMemI16:\n  case NVPTX::LoadParamMemI32:\n  case NVPTX::LoadParamMemI64:\n  case NVPTX::LoadParamMemI8:\n  case NVPTX::PrototypeInst:\n  case NVPTX::DBG_VALUE:\n    return true;\n  }\n  return false;\n}\n\n/// lowerConstantForGV - Return an MCExpr for the given Constant.  This is mostly\n/// a copy from AsmPrinter::lowerConstant, except customized to only handle\n/// expressions that are representable in PTX and create\n/// NVPTXGenericMCSymbolRefExpr nodes for addrspacecast instructions.\nconst MCExpr *\n"},"PrintAsmOperand":{"range":[2239,2259],"code":"bool NVPTXAsmPrinter::PrintAsmOperand(const MachineInstr *MI, unsigned OpNo,\n                                      unsigned AsmVariant,\n                                      const char *ExtraCode, raw_ostream &O) {\n  if (ExtraCode && ExtraCode[0]) {\n    if (ExtraCode[1] != 0)\n      return true; // Unknown modifier.\n\n    switch (ExtraCode[0]) {\n    default:\n      // See if this is a generic print operand\n      return AsmPrinter::PrintAsmOperand(MI, OpNo, AsmVariant, ExtraCode, O);\n    case 'r':\n      break;\n    }\n  }\n\n  printOperand(MI, OpNo, O);\n\n  return false;\n}\n\n"},"GetSymbolRef":{"range":[335,341],"code":"MCOperand NVPTXAsmPrinter::GetSymbolRef(const MCSymbol *Symbol) {\n  const MCExpr *Expr;\n  Expr = MCSymbolRefExpr::create(Symbol, MCSymbolRefExpr::VK_None,\n                                 OutContext);\n  return MCOperand::createExpr(Expr);\n}\n\n"},"printScalarConstant":{"range":[1732,1784],"code":"void NVPTXAsmPrinter::printScalarConstant(const Constant *CPV, raw_ostream &O) {\n  if (const ConstantInt *CI = dyn_cast<ConstantInt>(CPV)) {\n    O << CI->getValue();\n    return;\n  }\n  if (const ConstantFP *CFP = dyn_cast<ConstantFP>(CPV)) {\n    printFPConstant(CFP, O);\n    return;\n  }\n  if (isa<ConstantPointerNull>(CPV)) {\n    O << \"0\";\n    return;\n  }\n  if (const GlobalValue *GVar = dyn_cast<GlobalValue>(CPV)) {\n    bool IsNonGenericPointer = false;\n    if (GVar->getType()->getAddressSpace() != 0) {\n      IsNonGenericPointer = true;\n    }\n    if (EmitGeneric && !isa<Function>(CPV) && !IsNonGenericPointer) {\n      O << \"generic(\";\n      getSymbol(GVar)->print(O, MAI);\n      O << \")\";\n    } else {\n      getSymbol(GVar)->print(O, MAI);\n    }\n    return;\n  }\n  if (const ConstantExpr *Cexpr = dyn_cast<ConstantExpr>(CPV)) {\n    const Value *v = Cexpr->stripPointerCasts();\n    PointerType *PTy = dyn_cast<PointerType>(Cexpr->getType());\n    bool IsNonGenericPointer = false;\n    if (PTy && PTy->getAddressSpace() != 0) {\n      IsNonGenericPointer = true;\n    }\n    if (const GlobalValue *GVar = dyn_cast<GlobalValue>(v)) {\n      if (EmitGeneric && !isa<Function>(v) && !IsNonGenericPointer) {\n        O << \"generic(\";\n        getSymbol(GVar)->print(O, MAI);\n        O << \")\";\n      } else {\n        getSymbol(GVar)->print(O, MAI);\n      }\n      return;\n    } else {\n      lowerConstant(CPV)->print(O, MAI);\n      return;\n    }\n  }\n  llvm_unreachable(\"Not scalar type found in printScalarConstant()\");\n}\n\n// These utility functions assure we get the right sequence of bytes for a given\n// type even for big-endian machines\n"},"printParamName":{"range":[1438,1443],"code":"void NVPTXAsmPrinter::printParamName(Function::const_arg_iterator I,\n                                     int paramIndex, raw_ostream &O) {\n  getSymbol(I->getParent())->print(O, MAI);\n  O << \"_param_\" << paramIndex;\n}\n\n"},"emitVirtualRegister":{"range":[583,587],"code":"void NVPTXAsmPrinter::emitVirtualRegister(unsigned int vr,\n                                          raw_ostream &O) {\n  O << getVirtualRegisterName(vr);\n}\n\n"},"encodeVirtualRegister":{"range":[299,334],"code":"unsigned NVPTXAsmPrinter::encodeVirtualRegister(unsigned Reg) {\n  if (TargetRegisterInfo::isVirtualRegister(Reg)) {\n    const TargetRegisterClass *RC = MRI->getRegClass(Reg);\n\n    DenseMap<unsigned, unsigned> &RegMap = VRegMapping[RC];\n    unsigned RegNum = RegMap[Reg];\n\n    // Encode the register class in the upper 4 bits\n    // Must be kept in sync with NVPTXInstPrinter::printRegName\n    unsigned Ret = 0;\n    if (RC == &NVPTX::Int1RegsRegClass) {\n      Ret = (1 << 28);\n    } else if (RC == &NVPTX::Int16RegsRegClass) {\n      Ret = (2 << 28);\n    } else if (RC == &NVPTX::Int32RegsRegClass) {\n      Ret = (3 << 28);\n    } else if (RC == &NVPTX::Int64RegsRegClass) {\n      Ret = (4 << 28);\n    } else if (RC == &NVPTX::Float32RegsRegClass) {\n      Ret = (5 << 28);\n    } else if (RC == &NVPTX::Float64RegsRegClass) {\n      Ret = (6 << 28);\n    } else {\n      report_fatal_error(\"Bad register class\");\n    }\n\n    // Insert the vreg number\n    Ret |= (RegNum & 0x0FFFFFFF);\n    return Ret;\n  } else {\n    // Some special-use registers are actually physical registers.\n    // Encode this as the register class ID of 0 and the real register ID.\n    return Reg & 0x0FFFFFFF;\n  }\n}\n\n"},"isLoopHeaderOfNoUnroll":{"range":[418,444],"code":"bool NVPTXAsmPrinter::isLoopHeaderOfNoUnroll(\n    const MachineBasicBlock &MBB) const {\n  MachineLoopInfo &LI = getAnalysis<MachineLoopInfo>();\n  // We insert .pragma \"nounroll\" only to the loop header.\n  if (!LI.isLoopHeader(&MBB))\n    return false;\n\n  // llvm.loop.unroll.disable is marked on the back edges of a loop. Therefore,\n  // we iterate through each back edge of the loop with header MBB, and check\n  // whether its metadata contains llvm.loop.unroll.disable.\n  for (auto I = MBB.pred_begin(); I != MBB.pred_end(); ++I) {\n    const MachineBasicBlock *PMBB = *I;\n    if (LI.getLoopFor(PMBB) != LI.getLoopFor(&MBB)) {\n      // Edges from other loops to MBB are not back edges.\n      continue;\n    }\n    if (const BasicBlock *PBB = PMBB->getBasicBlock()) {\n      if (MDNode *LoopID =\n              PBB->getTerminator()->getMetadata(LLVMContext::MD_loop)) {\n        if (GetUnrollMetadata(LoopID, \"llvm.loop.unroll.disable\"))\n          return true;\n      }\n    }\n  }\n  return false;\n}\n\n"},"getPTXFundamentalTypeStr":{"range":[1325,1361],"code":"NVPTXAsmPrinter::getPTXFundamentalTypeStr(Type *Ty, bool useB4PTR) const {\n  switch (Ty->getTypeID()) {\n  default:\n    llvm_unreachable(\"unexpected type\");\n    break;\n  case Type::IntegerTyID: {\n    unsigned NumBits = cast<IntegerType>(Ty)->getBitWidth();\n    if (NumBits == 1)\n      return \"pred\";\n    else if (NumBits <= 64) {\n      std::string name = \"u\";\n      return name + utostr(NumBits);\n    } else {\n      llvm_unreachable(\"Integer too large\");\n      break;\n    }\n    break;\n  }\n  case Type::FloatTyID:\n    return \"f32\";\n  case Type::DoubleTyID:\n    return \"f64\";\n  case Type::PointerTyID:\n    if (static_cast<const NVPTXTargetMachine &>(TM).is64Bit())\n      if (useB4PTR)\n        return \"b64\";\n      else\n        return \"u64\";\n    else if (useB4PTR)\n      return \"b32\";\n    else\n      return \"u32\";\n  }\n  llvm_unreachable(\"unexpected type\");\n  return nullptr;\n}\n\n"}}},"NVPTXReplaceImageHandles.cpp":{"path":"NVPTXReplaceImageHandles.cpp","size":6352,"lines":191,"functions":{"createNVPTXReplaceImageHandlesPass":{"range":[189,192],"code":"MachineFunctionPass *llvm::createNVPTXReplaceImageHandlesPass() {\n  return new NVPTXReplaceImageHandles();\n}\n"},"runOnMachineFunction":{"range":[55,78],"code":"bool NVPTXReplaceImageHandles::runOnMachineFunction(MachineFunction &MF) {\n  bool Changed = false;\n  InstrsToRemove.clear();\n\n  for (MachineFunction::iterator BI = MF.begin(), BE = MF.end(); BI != BE;\n       ++BI) {\n    for (MachineBasicBlock::iterator I = (*BI).begin(), E = (*BI).end();\n         I != E; ++I) {\n      MachineInstr &MI = *I;\n      Changed |= processInstr(MI);\n    }\n  }\n\n  // Now clean up any handle-access instructions\n  // This is needed in debug mode when code cleanup passes are not executed,\n  // but we need the handle access to be eliminated because they are not\n  // valid instructions when image handles are disabled.\n  for (DenseSet<MachineInstr *>::iterator I = InstrsToRemove.begin(),\n       E = InstrsToRemove.end(); I != E; ++I) {\n    (*I)->eraseFromParent();\n  }\n  return Changed;\n}\n\n"},"replaceImageHandle":{"range":[125,132],"code":"replaceImageHandle(MachineOperand &Op, MachineFunction &MF) {\n  unsigned Idx;\n  if (findIndexForHandle(Op, MF, Idx)) {\n    Op.ChangeToImmediate(Idx);\n  }\n}\n\nbool NVPTXReplaceImageHandles::\n"},"NVPTXReplaceImageHandles":{"range":[52,54],"code":"NVPTXReplaceImageHandles::NVPTXReplaceImageHandles()\n  : MachineFunctionPass(ID) {}\n\n"},"findIndexForHandle":{"range":[133,188],"code":"findIndexForHandle(MachineOperand &Op, MachineFunction &MF, unsigned &Idx) {\n  const MachineRegisterInfo &MRI = MF.getRegInfo();\n  NVPTXMachineFunctionInfo *MFI = MF.getInfo<NVPTXMachineFunctionInfo>();\n\n  assert(Op.isReg() && \"Handle is not in a reg?\");\n\n  // Which instruction defines the handle?\n  MachineInstr &TexHandleDef = *MRI.getVRegDef(Op.getReg());\n\n  switch (TexHandleDef.getOpcode()) {\n  case NVPTX::LD_i64_avar: {\n    // The handle is a parameter value being loaded, replace with the\n    // parameter symbol\n    const NVPTXTargetMachine &TM =\n        static_cast<const NVPTXTargetMachine &>(MF.getTarget());\n    if (TM.getDrvInterface() == NVPTX::CUDA) {\n      // For CUDA, we preserve the param loads coming from function arguments\n      return false;\n    }\n\n    assert(TexHandleDef.getOperand(6).isSymbol() && \"Load is not a symbol!\");\n    StringRef Sym = TexHandleDef.getOperand(6).getSymbolName();\n    std::string ParamBaseName = MF.getName();\n    ParamBaseName += \"_param_\";\n    assert(Sym.startswith(ParamBaseName) && \"Invalid symbol reference\");\n    unsigned Param = atoi(Sym.data()+ParamBaseName.size());\n    std::string NewSym;\n    raw_string_ostream NewSymStr(NewSym);\n    NewSymStr << MF.getFunction()->getName() << \"_param_\" << Param;\n\n    InstrsToRemove.insert(&TexHandleDef);\n    Idx = MFI->getImageHandleSymbolIndex(NewSymStr.str().c_str());\n    return true;\n  }\n  case NVPTX::texsurf_handles: {\n    // The handle is a global variable, replace with the global variable name\n    assert(TexHandleDef.getOperand(1).isGlobal() && \"Load is not a global!\");\n    const GlobalValue *GV = TexHandleDef.getOperand(1).getGlobal();\n    assert(GV->hasName() && \"Global sampler must be named!\");\n    InstrsToRemove.insert(&TexHandleDef);\n    Idx = MFI->getImageHandleSymbolIndex(GV->getName().data());\n    return true;\n  }\n  case NVPTX::nvvm_move_i64:\n  case TargetOpcode::COPY: {\n    bool Res = findIndexForHandle(TexHandleDef.getOperand(1), MF, Idx);\n    if (Res) {\n      InstrsToRemove.insert(&TexHandleDef);\n    }\n    return Res;\n  }\n  default:\n    llvm_unreachable(\"Unknown instruction operating on handle\");\n  }\n}\n\n"},"processInstr":{"range":[79,124],"code":"bool NVPTXReplaceImageHandles::processInstr(MachineInstr &MI) {\n  MachineFunction &MF = *MI.getParent()->getParent();\n  const MCInstrDesc &MCID = MI.getDesc();\n\n  if (MCID.TSFlags & NVPTXII::IsTexFlag) {\n    // This is a texture fetch, so operand 4 is a texref and operand 5 is\n    // a samplerref\n    MachineOperand &TexHandle = MI.getOperand(4);\n    replaceImageHandle(TexHandle, MF);\n\n    if (!(MCID.TSFlags & NVPTXII::IsTexModeUnifiedFlag)) {\n      MachineOperand &SampHandle = MI.getOperand(5);\n      replaceImageHandle(SampHandle, MF);\n    }\n\n    return true;\n  } else if (MCID.TSFlags & NVPTXII::IsSuldMask) {\n    unsigned VecSize =\n      1 << (((MCID.TSFlags & NVPTXII::IsSuldMask) >> NVPTXII::IsSuldShift) - 1);\n\n    // For a surface load of vector size N, the Nth operand will be the surfref\n    MachineOperand &SurfHandle = MI.getOperand(VecSize);\n\n    replaceImageHandle(SurfHandle, MF);\n\n    return true;\n  } else if (MCID.TSFlags & NVPTXII::IsSustFlag) {\n    // This is a surface store, so operand 0 is a surfref\n    MachineOperand &SurfHandle = MI.getOperand(0);\n\n    replaceImageHandle(SurfHandle, MF);\n\n    return true;\n  } else if (MCID.TSFlags & NVPTXII::IsSurfTexQueryFlag) {\n    // This is a query, so operand 1 is a surfref/texref\n    MachineOperand &Handle = MI.getOperand(1);\n\n    replaceImageHandle(Handle, MF);\n\n    return true;\n  }\n\n  return false;\n}\n\nvoid NVPTXReplaceImageHandles::\n"}}},"NVPTXPeephole.cpp":{"path":"NVPTXPeephole.cpp","size":5064,"lines":157,"functions":{"createNVPTXPeephole":{"range":[157,0],"code":"MachineFunctionPass *llvm::createNVPTXPeephole() { return new NVPTXPeephole(); }\n"},"runOnMachineFunction":{"range":[127,156],"code":"bool NVPTXPeephole::runOnMachineFunction(MachineFunction &MF) {\n  if (skipFunction(*MF.getFunction()))\n    return false;\n\n  bool Changed = false;\n  // Loop over all of the basic blocks.\n  for (auto &MBB : MF) {\n    // Traverse the basic block.\n    auto BlockIter = MBB.begin();\n\n    while (BlockIter != MBB.end()) {\n      auto &MI = *BlockIter++;\n      if (isCVTAToLocalCombinationCandidate(MI)) {\n        CombineCVTAToLocal(MI);\n        Changed = true;\n      }\n    }  // Instruction\n  }    // Basic Block\n\n  // Remove unnecessary %VRFrame = cvta.local %VRFrameLocal\n  const auto &MRI = MF.getRegInfo();\n  if (MRI.use_empty(NVPTX::VRFrame)) {\n    if (auto MI = MRI.getUniqueVRegDef(NVPTX::VRFrame)) {\n      MI->eraseFromParentAndMarkDBGValuesForRemoval();\n    }\n  }\n\n  return Changed;\n}\n\n"},"NVPTXPeephole":{"range":[54,73],"code":"  NVPTXPeephole() : MachineFunctionPass(ID) {\n    initializeNVPTXPeepholePass(*PassRegistry::getPassRegistry());\n  }\n\n  bool runOnMachineFunction(MachineFunction &MF) override;\n\n  const char *getPassName() const override {\n    return \"NVPTX optimize redundant cvta.to.local instruction\";\n  }\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override {\n    MachineFunctionPass::getAnalysisUsage(AU);\n  }\n};\n}\n\nchar NVPTXPeephole::ID = 0;\n\nINITIALIZE_PASS(NVPTXPeephole, \"nvptx-peephole\", \"NVPTX Peephole\", false, false)\n\n"},"CombineCVTAToLocal":{"range":[105,126],"code":"static void CombineCVTAToLocal(MachineInstr &Root) {\n  auto &MBB = *Root.getParent();\n  auto &MF = *MBB.getParent();\n  const auto &MRI = MF.getRegInfo();\n  const TargetInstrInfo *TII = MF.getSubtarget().getInstrInfo();\n  auto &Prev = *MRI.getUniqueVRegDef(Root.getOperand(1).getReg());\n\n  MachineInstrBuilder MIB =\n      BuildMI(MF, Root.getDebugLoc(), TII->get(Prev.getOpcode()),\n              Root.getOperand(0).getReg())\n          .addReg(NVPTX::VRFrameLocal)\n          .addOperand(Prev.getOperand(2));\n\n  MBB.insert((MachineBasicBlock::iterator)&Root, MIB);\n\n  // Check if MRI has only one non dbg use, which is Root\n  if (MRI.hasOneNonDBGUse(Prev.getOperand(0).getReg())) {\n    Prev.eraseFromParentAndMarkDBGValuesForRemoval();\n  }\n  Root.eraseFromParentAndMarkDBGValuesForRemoval();\n}\n\n"},"isCVTAToLocalCombinationCandidate":{"range":[74,104],"code":"static bool isCVTAToLocalCombinationCandidate(MachineInstr &Root) {\n  auto &MBB = *Root.getParent();\n  auto &MF = *MBB.getParent();\n  // Check current instruction is cvta.to.local\n  if (Root.getOpcode() != NVPTX::cvta_to_local_yes_64 &&\n      Root.getOpcode() != NVPTX::cvta_to_local_yes)\n    return false;\n\n  auto &Op = Root.getOperand(1);\n  const auto &MRI = MF.getRegInfo();\n  MachineInstr *GenericAddrDef = nullptr;\n  if (Op.isReg() && TargetRegisterInfo::isVirtualRegister(Op.getReg())) {\n    GenericAddrDef = MRI.getUniqueVRegDef(Op.getReg());\n  }\n\n  // Check the register operand is uniquely defined by LEA_ADDRi instruction\n  if (!GenericAddrDef || GenericAddrDef->getParent() != &MBB ||\n      (GenericAddrDef->getOpcode() != NVPTX::LEA_ADDRi64 &&\n       GenericAddrDef->getOpcode() != NVPTX::LEA_ADDRi)) {\n    return false;\n  }\n\n  // Check the LEA_ADDRi operand is Frame index\n  auto &BaseAddrOp = GenericAddrDef->getOperand(1);\n  if (BaseAddrOp.isReg() && BaseAddrOp.getReg() == NVPTX::VRFrame) {\n    return true;\n  }\n\n  return false;\n}\n\n"}}},"NVPTXImageOptimizer.cpp":{"path":"NVPTXImageOptimizer.cpp","size":5808,"lines":181,"functions":{"cleanupValue":{"range":[172,178],"code":"Value *NVPTXImageOptimizer::cleanupValue(Value *V) {\n  if (ExtractValueInst *EVI = dyn_cast<ExtractValueInst>(V)) {\n    return cleanupValue(EVI->getAggregateOperand());\n  }\n  return V;\n}\n\n"},"NVPTXImageOptimizer":{"range":[49,51],"code":"NVPTXImageOptimizer::NVPTXImageOptimizer()\n  : FunctionPass(ID) {}\n\n"},"runOnFunction":{"range":[52,92],"code":"bool NVPTXImageOptimizer::runOnFunction(Function &F) {\n  if (skipFunction(F))\n    return false;\n\n  bool Changed = false;\n  InstrToDelete.clear();\n\n  // Look for call instructions in the function\n  for (Function::iterator BI = F.begin(), BE = F.end(); BI != BE;\n       ++BI) {\n    for (BasicBlock::iterator I = (*BI).begin(), E = (*BI).end();\n         I != E; ++I) {\n      Instruction &Instr = *I;\n      if (CallInst *CI = dyn_cast<CallInst>(I)) {\n        Function *CalledF = CI->getCalledFunction();\n        if (CalledF && CalledF->isIntrinsic()) {\n          // This is an intrinsic function call, check if its an istypep\n          switch (CalledF->getIntrinsicID()) {\n          default: break;\n          case Intrinsic::nvvm_istypep_sampler:\n            Changed |= replaceIsTypePSampler(Instr);\n            break;\n          case Intrinsic::nvvm_istypep_surface:\n            Changed |= replaceIsTypePSurface(Instr);\n            break;\n          case Intrinsic::nvvm_istypep_texture:\n            Changed |= replaceIsTypePTexture(Instr);\n            break;\n          }\n        }\n      }\n    }\n  }\n\n  // Delete any istypep instances we replaced in the IR\n  for (unsigned i = 0, e = InstrToDelete.size(); i != e; ++i)\n    InstrToDelete[i]->eraseFromParent();\n\n  return Changed;\n}\n\n"},"replaceIsTypePTexture":{"range":[130,148],"code":"bool NVPTXImageOptimizer::replaceIsTypePTexture(Instruction &I) {\n  Value *TexHandle = cleanupValue(I.getOperand(0));\n  if (isImageReadOnly(*TexHandle)) {\n    // This is an OpenCL read-only image, so it must be a texref\n    replaceWith(&I, ConstantInt::getTrue(I.getContext()));\n    return true;\n  } else if (isImageWriteOnly(*TexHandle) ||\n             isImageReadWrite(*TexHandle) ||\n             isSampler(*TexHandle)) {\n    // This is an OpenCL read-write/write-only image or a sampler, so it\n    // cannot be a texref\n    replaceWith(&I, ConstantInt::getFalse(I.getContext()));\n    return true;\n  } else {\n    // The image type is unknown, so we cannot eliminate the intrinsic\n    return false;\n  }\n}\n\n"},"replaceIsTypePSurface":{"range":[111,129],"code":"bool NVPTXImageOptimizer::replaceIsTypePSurface(Instruction &I) {\n  Value *TexHandle = cleanupValue(I.getOperand(0));\n  if (isImageReadWrite(*TexHandle) ||\n      isImageWriteOnly(*TexHandle)) {\n    // This is an OpenCL read-only/read-write image, so it must be a surfref\n    replaceWith(&I, ConstantInt::getTrue(I.getContext()));\n    return true;\n  } else if (isImageReadOnly(*TexHandle) ||\n             isSampler(*TexHandle)) {\n    // This is an OpenCL read-only/ imageor sampler, so it cannot be\n    // a surfref\n    replaceWith(&I, ConstantInt::getFalse(I.getContext()));\n    return true;\n  } else {\n    // The image type is unknown, so we cannot eliminate the intrinsic\n    return false;\n  }\n}\n\n"},"replaceIsTypePSampler":{"range":[93,110],"code":"bool NVPTXImageOptimizer::replaceIsTypePSampler(Instruction &I) {\n  Value *TexHandle = cleanupValue(I.getOperand(0));\n  if (isSampler(*TexHandle)) {\n    // This is an OpenCL sampler, so it must be a samplerref\n    replaceWith(&I, ConstantInt::getTrue(I.getContext()));\n    return true;\n  } else if (isImageWriteOnly(*TexHandle) ||\n             isImageReadWrite(*TexHandle) ||\n             isImageReadOnly(*TexHandle)) {\n    // This is an OpenCL image, so it cannot be a samplerref\n    replaceWith(&I, ConstantInt::getFalse(I.getContext()));\n    return true;\n  } else {\n    // The image type is unknown, so we cannot eliminate the intrinsic\n    return false;\n  }\n}\n\n"},"replaceWith":{"range":[149,171],"code":"void NVPTXImageOptimizer::replaceWith(Instruction *From, ConstantInt *To) {\n  // We implement \"poor man's DCE\" here to make sure any code that is no longer\n  // live is actually unreachable and can be trivially eliminated by the\n  // unreachable block elimination pass.\n  for (CallInst::use_iterator UI = From->use_begin(), UE = From->use_end();\n       UI != UE; ++UI) {\n    if (BranchInst *BI = dyn_cast<BranchInst>(*UI)) {\n      if (BI->isUnconditional()) continue;\n      BasicBlock *Dest;\n      if (To->isZero())\n        // Get false block\n        Dest = BI->getSuccessor(1);\n      else\n        // Get true block\n        Dest = BI->getSuccessor(0);\n      BranchInst::Create(Dest, BI);\n      InstrToDelete.push_back(BI);\n    }\n  }\n  From->replaceAllUsesWith(To);\n  InstrToDelete.push_back(From);\n}\n\n"},"createNVPTXImageOptimizerPass":{"range":[179,182],"code":"FunctionPass *llvm::createNVPTXImageOptimizerPass() {\n  return new NVPTXImageOptimizer();\n}\n"}}},"NVPTXAllocaHoisting.cpp":{"path":"NVPTXAllocaHoisting.cpp","size":2244,"lines":72,"functions":{"runOnFunction":{"range":[43,71],"code":"bool NVPTXAllocaHoisting::runOnFunction(Function &function) {\n  bool functionModified = false;\n  Function::iterator I = function.begin();\n  TerminatorInst *firstTerminatorInst = (I++)->getTerminator();\n\n  for (Function::iterator E = function.end(); I != E; ++I) {\n    for (BasicBlock::iterator BI = I->begin(), BE = I->end(); BI != BE;) {\n      AllocaInst *allocaInst = dyn_cast<AllocaInst>(BI++);\n      if (allocaInst && isa<ConstantInt>(allocaInst->getArraySize())) {\n        allocaInst->moveBefore(firstTerminatorInst);\n        functionModified = true;\n      }\n    }\n  }\n\n  return functionModified;\n}\n\nchar NVPTXAllocaHoisting::ID = 0;\n\nnamespace llvm {\nvoid initializeNVPTXAllocaHoistingPass(PassRegistry &);\n}\n\nINITIALIZE_PASS(\n    NVPTXAllocaHoisting, \"alloca-hoisting\",\n    \"Hoisting alloca instructions in non-entry blocks to the entry block\",\n    false, false)\n\n"},"createAllocaHoisting":{"range":[72,0],"code":"FunctionPass *llvm::createAllocaHoisting() { return new NVPTXAllocaHoisting; }\n"},"NVPTXAllocaHoisting":{"range":[28,42],"code":"  NVPTXAllocaHoisting() : FunctionPass(ID) {}\n\n  void getAnalysisUsage(AnalysisUsage &AU) const override {\n    AU.addPreserved<MachineFunctionAnalysis>();\n    AU.addPreserved<StackProtector>();\n  }\n\n  const char *getPassName() const override {\n    return \"NVPTX specific alloca hoisting\";\n  }\n\n  bool runOnFunction(Function &function) override;\n};\n} // namespace\n\n"}}},"NVPTXMCExpr.cpp":{"path":"NVPTXMCExpr.cpp","size":1840,"lines":60,"functions":{"create":{"range":[50,54],"code":"NVPTXGenericMCSymbolRefExpr::create(const MCSymbolRefExpr *SymExpr,\n                                    MCContext &Ctx) {\n  return new (Ctx) NVPTXGenericMCSymbolRefExpr(SymExpr);\n}\n\n"},"printImpl":{"range":[55,61],"code":"void NVPTXGenericMCSymbolRefExpr::printImpl(raw_ostream &OS,\n                                            const MCAsmInfo *MAI) const {\n  OS << \"generic(\";\n  SymExpr->print(OS, MAI);\n  OS << \")\";\n}\n"}}}},"directories":{}}}}